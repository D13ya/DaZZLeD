{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6732215f",
   "metadata": {},
   "source": [
    "# ü¶ñ DaZZLeD: Recursive Hasher Training Notebook\n",
    "\n",
    "**Goal:** Train a Tiny Recursive Model (TRM) using DINOv3 distillation for adversarially-robust perceptual hashing.\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run Cell 1** - Mount Google Drive\n",
    "2. **Run Cell 2** - Download training datasets (one-time, ~30 min)\n",
    "3. **Run Cell 3** - Clone repo & install dependencies\n",
    "4. **Run Cell 4** - Build manifest & train the model\n",
    "5. **Run remaining cells** - Test and export the trained model\n",
    "\n",
    "‚ö†Ô∏è **Before training:** Runtime ‚Üí Change runtime type ‚Üí **GPU (T4 free or A100)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd8ce8",
   "metadata": {},
   "source": [
    "## 0. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef02f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (required for data storage)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
    "DATA_ROOT = DRIVE_ROOT / \"data\"\n",
    "OUTPUT_ROOT = DRIVE_ROOT / \"outputs\"\n",
    "\n",
    "# Create all needed directories\n",
    "for d in [DATA_ROOT / \"ffhq\", DATA_ROOT / \"openimages\", DATA_ROOT / \"text\",\n",
    "          OUTPUT_ROOT / \"checkpoints\", OUTPUT_ROOT / \"models\",\n",
    "          DRIVE_ROOT / \"manifests\"]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Project root: {DRIVE_ROOT}\")\n",
    "print(f\"‚úì Data root: {DATA_ROOT}\")\n",
    "print(f\"‚úì Output root: {OUTPUT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaee7b4",
   "metadata": {},
   "source": [
    "## 0.1 Download Training Datasets (Direct to Drive)\n",
    "\n",
    "Run this cell **once** to download datasets directly to Google Drive. Takes ~30 minutes.\n",
    "\n",
    "**Datasets used (non-sensitive proxies):**\n",
    "- **FFHQ Thumbnails** (10k faces) - Human-centric feature extraction\n",
    "- **OpenImages Subset** (30k images) - Diverse real-world images  \n",
    "- **COCO Subset** (5k images) - Additional diversity\n",
    "\n",
    "‚ö†Ô∏è Skip this cell if you've already downloaded the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOWNLOAD DATASETS DIRECTLY TO GOOGLE DRIVE\n",
    "# Run this cell ONCE - it will download ~5GB of training data\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "DATA_ROOT = Path(\"/content/drive/MyDrive/dazzled/data\")\n",
    "\n",
    "def download_with_progress(url, dest):\n",
    "    \"\"\"Download file with progress indicator.\"\"\"\n",
    "    def reporthook(count, block_size, total_size):\n",
    "        percent = min(100, int(count * block_size * 100 / total_size))\n",
    "        print(f\"\\r  Downloading: {percent}%\", end=\"\", flush=True)\n",
    "    \n",
    "    urllib.request.urlretrieve(url, dest, reporthook)\n",
    "    print(\" ‚úì\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. FFHQ Thumbnails (128x128, 10k images) - ~95MB\n",
    "# -----------------------------------------------------------------------------\n",
    "ffhq_dir = DATA_ROOT / \"ffhq\"\n",
    "if len(list(ffhq_dir.glob(\"*.png\"))) < 1000:\n",
    "    print(\"üì• Downloading FFHQ thumbnails...\")\n",
    "    \n",
    "    # FFHQ thumbnails from official Google Drive\n",
    "    # Using a smaller 10k subset for training\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    \n",
    "    # FFHQ thumbnails 128x128 (official)\n",
    "    ffhq_url = \"https://drive.google.com/uc?id=1SbZKLwDxBV2k4aFa5LS9lEE3cNMV5IEU\"\n",
    "    ffhq_zip = \"/content/ffhq_thumbnails.zip\"\n",
    "    \n",
    "    gdown.download(ffhq_url, ffhq_zip, quiet=False)\n",
    "    \n",
    "    print(\"  Extracting FFHQ...\")\n",
    "    with zipfile.ZipFile(ffhq_zip, 'r') as z:\n",
    "        z.extractall(\"/content/ffhq_temp\")\n",
    "    \n",
    "    # Move images to Drive and resize to 224x224\n",
    "    from PIL import Image\n",
    "    from torchvision import transforms\n",
    "    \n",
    "    resize_transform = transforms.Resize((224, 224))\n",
    "    \n",
    "    temp_images = list(Path(\"/content/ffhq_temp\").rglob(\"*.png\"))[:10000]\n",
    "    for i, img_path in enumerate(temp_images):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"  Processing FFHQ: {i}/{len(temp_images)}\")\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = resize_transform(img)\n",
    "            img.save(ffhq_dir / f\"ffhq_{i:05d}.jpg\", quality=95)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(ffhq_zip)\n",
    "    shutil.rmtree(\"/content/ffhq_temp\", ignore_errors=True)\n",
    "    print(f\"‚úì FFHQ: {len(list(ffhq_dir.glob('*.jpg')))} images\")\n",
    "else:\n",
    "    print(f\"‚úì FFHQ already exists: {len(list(ffhq_dir.glob('*')))} images\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. COCO 2017 Validation Set (5k images) - ~1GB\n",
    "# -----------------------------------------------------------------------------\n",
    "coco_dir = DATA_ROOT / \"openimages\"  # Using for general images\n",
    "if len(list(coco_dir.glob(\"*.jpg\"))) < 1000:\n",
    "    print(\"\\nüì• Downloading COCO 2017 validation set...\")\n",
    "    \n",
    "    coco_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "    coco_zip = \"/content/coco_val2017.zip\"\n",
    "    \n",
    "    download_with_progress(coco_url, coco_zip)\n",
    "    \n",
    "    print(\"  Extracting COCO...\")\n",
    "    with zipfile.ZipFile(coco_zip, 'r') as z:\n",
    "        z.extractall(\"/content/coco_temp\")\n",
    "    \n",
    "    # Resize and move to Drive\n",
    "    from PIL import Image\n",
    "    from torchvision import transforms\n",
    "    \n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "    ])\n",
    "    \n",
    "    temp_images = list(Path(\"/content/coco_temp/val2017\").glob(\"*.jpg\"))\n",
    "    for i, img_path in enumerate(temp_images):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"  Processing COCO: {i}/{len(temp_images)}\")\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = resize_transform(img)\n",
    "            img.save(coco_dir / img_path.name, quality=95)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(coco_zip)\n",
    "    shutil.rmtree(\"/content/coco_temp\", ignore_errors=True)\n",
    "    print(f\"‚úì COCO: {len(list(coco_dir.glob('*.jpg')))} images\")\n",
    "else:\n",
    "    print(f\"‚úì COCO already exists: {len(list(coco_dir.glob('*')))} images\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Text/Document Images (synthetic screenshots)\n",
    "# -----------------------------------------------------------------------------\n",
    "text_dir = DATA_ROOT / \"text\"\n",
    "if len(list(text_dir.glob(\"*.jpg\"))) < 100:\n",
    "    print(\"\\nüì• Generating synthetic text images...\")\n",
    "    \n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    import random\n",
    "    import string\n",
    "    \n",
    "    # Generate simple text images as edge cases\n",
    "    for i in range(500):\n",
    "        img = Image.new('RGB', (224, 224), color=(255, 255, 255))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Random text\n",
    "        text = ''.join(random.choices(string.ascii_letters + string.digits + ' ', k=random.randint(20, 100)))\n",
    "        \n",
    "        # Wrap text\n",
    "        lines = [text[j:j+20] for j in range(0, len(text), 20)]\n",
    "        y = 20\n",
    "        for line in lines[:8]:\n",
    "            draw.text((10, y), line, fill=(0, 0, 0))\n",
    "            y += 25\n",
    "        \n",
    "        img.save(text_dir / f\"text_{i:04d}.jpg\", quality=95)\n",
    "    \n",
    "    print(f\"‚úì Text images: {len(list(text_dir.glob('*.jpg')))} images\")\n",
    "else:\n",
    "    print(f\"‚úì Text images already exist: {len(list(text_dir.glob('*')))} images\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Summary\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä DATASET SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "total = 0\n",
    "for name, path in [(\"FFHQ (faces)\", DATA_ROOT / \"ffhq\"), \n",
    "                   (\"OpenImages/COCO\", DATA_ROOT / \"openimages\"),\n",
    "                   (\"Text/Docs\", DATA_ROOT / \"text\")]:\n",
    "    count = len(list(path.glob(\"*\")))\n",
    "    total += count\n",
    "    print(f\"  {name}: {count:,} images\")\n",
    "print(f\"  {'‚îÄ'*30}\")\n",
    "print(f\"  TOTAL: {total:,} images\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c8053",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo (only needed in Colab)\n",
    "import os\n",
    "if not os.path.exists('DaZZLeD'):\n",
    "    !git clone https://github.com/D13ya/DaZZLeD.git\n",
    "    %cd DaZZLeD/ml-core\n",
    "else:\n",
    "    %cd DaZZLeD/ml-core\n",
    "\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f363c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from models.recursive_student import RecursiveHasher\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d9a99",
   "metadata": {},
   "source": [
    "## 1.1 Build Manifest & Train Model\n",
    "\n",
    "‚ö†Ô∏è **Switch to GPU first:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or A100)\n",
    "\n",
    "This cell:\n",
    "1. Builds a manifest of all training images\n",
    "2. Trains the RecursiveHasher using DINOv3 distillation\n",
    "3. Saves checkpoints to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILD MANIFEST FROM DOWNLOADED DATA\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
    "DATA_ROOT = DRIVE_ROOT / \"data\"\n",
    "\n",
    "# Find all training images\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "paths = []\n",
    "\n",
    "for data_dir in [DATA_ROOT / \"ffhq\", DATA_ROOT / \"openimages\", DATA_ROOT / \"text\"]:\n",
    "    if data_dir.exists():\n",
    "        paths.extend([str(p) for p in data_dir.rglob(\"*\") if p.suffix.lower() in exts])\n",
    "\n",
    "print(f\"Found {len(paths):,} training images\")\n",
    "\n",
    "if len(paths) < 100:\n",
    "    print(\"‚ö†Ô∏è  Not enough images! Run the download cell first.\")\n",
    "else:\n",
    "    # Write manifest\n",
    "    manifest = DRIVE_ROOT / \"manifests/train.txt\"\n",
    "    manifest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    manifest.write_text(\"\\n\".join(paths))\n",
    "    print(f\"‚úì Manifest written: {manifest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN THE MODEL (Choose one option)\n",
    "# =============================================================================\n",
    "\n",
    "# üîß TRAINING OPTIONS - Uncomment ONE of the following:\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTION A: Quick Test (5-10 min on T4) - Just to verify everything works\n",
    "# -----------------------------------------------------------------------------\n",
    "!python training/train.py \\\n",
    "    --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
    "    --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
    "    --epochs 1 \\\n",
    "    --batch-size 16 \\\n",
    "    --recursion-steps 8 \\\n",
    "    --max-steps 100 \\\n",
    "    --amp \\\n",
    "    --log-interval 10 \\\n",
    "    --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTION B: Full Training (2-4 hours on A100) - Production quality\n",
    "# -----------------------------------------------------------------------------\n",
    "# !python training/train.py \\\n",
    "#     --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
    "#     --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
    "#     --epochs 5 \\\n",
    "#     --batch-size 64 \\\n",
    "#     --recursion-steps 16 \\\n",
    "#     --grad-accum 2 \\\n",
    "#     --lr 1e-4 \\\n",
    "#     --amp \\\n",
    "#     --allow-tf32 \\\n",
    "#     --channels-last \\\n",
    "#     --cudnn-benchmark \\\n",
    "#     --workers 4 \\\n",
    "#     --log-interval 50 \\\n",
    "#     --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints \\\n",
    "#     --checkpoint-every 500\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTION C: Resume from Checkpoint (if session disconnected)\n",
    "# -----------------------------------------------------------------------------\n",
    "# !python training/train.py \\\n",
    "#     --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
    "#     --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
    "#     --resume /content/drive/MyDrive/dazzled/outputs/checkpoints/student_epoch_3.safetensors \\\n",
    "#     --epochs 5 \\\n",
    "#     --batch-size 64 \\\n",
    "#     --recursion-steps 16 \\\n",
    "#     --amp \\\n",
    "#     --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIST CHECKPOINTS & LOAD TRAINED WEIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import safetensors.torch\n",
    "\n",
    "CKPT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
    "\n",
    "# List available checkpoints\n",
    "checkpoints = sorted(CKPT_DIR.glob(\"*.safetensors\"))\n",
    "print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "for ckpt in checkpoints:\n",
    "    size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {ckpt.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# Load the latest checkpoint into model\n",
    "if checkpoints:\n",
    "    latest_ckpt = checkpoints[-1]\n",
    "    print(f\"\\nüì• Loading: {latest_ckpt.name}\")\n",
    "    \n",
    "    # Model should already be defined from earlier cell\n",
    "    # If not, uncomment:\n",
    "    # from models.recursive_student import RecursiveHasher\n",
    "    # model = RecursiveHasher(state_dim=128, hash_dim=96)\n",
    "    \n",
    "    safetensors.torch.load_model(model, str(latest_ckpt))\n",
    "    model.eval()\n",
    "    print(\"‚úì Trained weights loaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No checkpoints found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbba1d4",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with default params\n",
    "STATE_DIM = 128\n",
    "HASH_DIM = 96\n",
    "RECURSION_STEPS = 16\n",
    "\n",
    "model = RecursiveHasher(state_dim=STATE_DIM, hash_dim=HASH_DIM)\n",
    "model.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with dummy input\n",
    "batch_size = 4\n",
    "image_size = 224\n",
    "\n",
    "dummy_img = torch.randn(batch_size, 3, image_size, image_size)\n",
    "dummy_state = torch.zeros(batch_size, STATE_DIM)\n",
    "\n",
    "with torch.no_grad():\n",
    "    next_state, hash_out = model(dummy_img, dummy_state)\n",
    "\n",
    "print(f\"Input image shape: {dummy_img.shape}\")\n",
    "print(f\"Input state shape: {dummy_state.shape}\")\n",
    "print(f\"Output state shape: {next_state.shape}\")\n",
    "print(f\"Output hash shape: {hash_out.shape}\")\n",
    "print(f\"Hash L2 norm (should be ~1.0): {torch.norm(hash_out, dim=1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b0ff3",
   "metadata": {},
   "source": [
    "## 3. Recursive Inference Test\n",
    "\n",
    "The key innovation is running the model recursively 16 times, refining the hash at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc68871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_inference(model, image, steps=16):\n",
    "    \"\"\"Run recursive inference for the specified number of steps.\"\"\"\n",
    "    batch_size = image.size(0)\n",
    "    state = torch.zeros(batch_size, STATE_DIM, device=image.device)\n",
    "    \n",
    "    hashes = []\n",
    "    with torch.no_grad():\n",
    "        for step in range(steps):\n",
    "            state, hash_out = model(image, state)\n",
    "            hashes.append(hash_out.clone())\n",
    "    \n",
    "    return hashes\n",
    "\n",
    "# Run recursive inference\n",
    "hashes = recursive_inference(model, dummy_img, steps=RECURSION_STEPS)\n",
    "\n",
    "print(f\"Generated {len(hashes)} hash vectors\")\n",
    "print(f\"Final hash shape: {hashes[-1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7063c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hash stability across recursive steps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute cosine similarity between consecutive steps\n",
    "similarities = []\n",
    "for i in range(1, len(hashes)):\n",
    "    sim = F.cosine_similarity(hashes[i], hashes[i-1], dim=1).mean().item()\n",
    "    similarities.append(sim)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(hashes)), similarities, 'b-o')\n",
    "plt.xlabel('Recursion Step')\n",
    "plt.ylabel('Cosine Similarity with Previous')\n",
    "plt.title('Hash Convergence Over Recursion')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True)\n",
    "\n",
    "# Compute similarity to final hash\n",
    "final_hash = hashes[-1]\n",
    "similarities_to_final = []\n",
    "for h in hashes:\n",
    "    sim = F.cosine_similarity(h, final_hash, dim=1).mean().item()\n",
    "    similarities_to_final.append(sim)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(hashes)), similarities_to_final, 'r-o')\n",
    "plt.xlabel('Recursion Step')\n",
    "plt.ylabel('Cosine Similarity to Final Hash')\n",
    "plt.title('Convergence to Final Hash')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87e5f8",
   "metadata": {},
   "source": [
    "## 4. Adversarial Robustness Test\n",
    "\n",
    "Test if small perturbations to input cause large changes in hash (they shouldn't after recursive refinement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa469be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perturbation_robustness(model, image, epsilon_range, steps=16):\n",
    "    \"\"\"Test hash stability under input perturbations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get baseline hash\n",
    "    baseline_hashes = recursive_inference(model, image, steps)\n",
    "    baseline_final = baseline_hashes[-1]\n",
    "    \n",
    "    for epsilon in epsilon_range:\n",
    "        # Add random noise\n",
    "        noise = torch.randn_like(image) * epsilon\n",
    "        perturbed = image + noise\n",
    "        \n",
    "        # Get perturbed hash\n",
    "        perturbed_hashes = recursive_inference(model, perturbed, steps)\n",
    "        perturbed_final = perturbed_hashes[-1]\n",
    "        \n",
    "        # Compute similarity\n",
    "        sim = F.cosine_similarity(baseline_final, perturbed_final, dim=1).mean().item()\n",
    "        results.append((epsilon, sim))\n",
    "        print(f\"Epsilon={epsilon:.4f}: Cosine Similarity={sim:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with various noise levels\n",
    "epsilons = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "test_img = torch.randn(1, 3, 224, 224)\n",
    "results = test_perturbation_robustness(model, test_img, epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot robustness results\n",
    "epsilons, sims = zip(*results)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epsilons, sims, 'g-o', linewidth=2, markersize=8)\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='0.9 threshold')\n",
    "plt.xlabel('Noise Level (epsilon)', fontsize=12)\n",
    "plt.ylabel('Cosine Similarity to Original', fontsize=12)\n",
    "plt.title('Hash Robustness to Input Perturbations', fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e33729",
   "metadata": {},
   "source": [
    "## 5. ONNX Export Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe78324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"test_model.onnx\"\n",
    "\n",
    "model.eval()\n",
    "dummy_img = torch.randn(1, 3, 224, 224)\n",
    "dummy_state = torch.zeros(1, STATE_DIM)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_img, dummy_state),\n",
    "    onnx_path,\n",
    "    input_names=[\"image\", \"prev_state\"],\n",
    "    output_names=[\"next_state\", \"hash\"],\n",
    "    opset_version=14,\n",
    "    dynamic_axes={\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"prev_state\": {0: \"batch\"},\n",
    "        \"next_state\": {0: \"batch\"},\n",
    "        \"hash\": {0: \"batch\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Exported ONNX model to {onnx_path}\")\n",
    "print(f\"File size: {os.path.getsize(onnx_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe910628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate ONNX model\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model validation passed!\")\n",
    "\n",
    "# Test ONNX runtime inference\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Run inference\n",
    "test_img = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
    "test_state = np.zeros((1, STATE_DIM), dtype=np.float32)\n",
    "\n",
    "outputs = session.run(None, {\"image\": test_img, \"prev_state\": test_state})\n",
    "next_state_onnx, hash_onnx = outputs\n",
    "\n",
    "print(f\"ONNX output state shape: {next_state_onnx.shape}\")\n",
    "print(f\"ONNX output hash shape: {hash_onnx.shape}\")\n",
    "print(f\"ONNX hash L2 norm: {np.linalg.norm(hash_onnx, axis=1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ac143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PyTorch vs ONNX outputs\n",
    "with torch.no_grad():\n",
    "    pt_state, pt_hash = model(torch.from_numpy(test_img), torch.from_numpy(test_state))\n",
    "\n",
    "pt_hash_np = pt_hash.numpy()\n",
    "pt_state_np = pt_state.numpy()\n",
    "\n",
    "hash_diff = np.abs(pt_hash_np - hash_onnx).max()\n",
    "state_diff = np.abs(pt_state_np - next_state_onnx).max()\n",
    "\n",
    "print(f\"Max hash difference (PyTorch vs ONNX): {hash_diff:.8f}\")\n",
    "print(f\"Max state difference (PyTorch vs ONNX): {state_diff:.8f}\")\n",
    "\n",
    "if hash_diff < 1e-5 and state_diff < 1e-5:\n",
    "    print(\"‚úÖ ONNX export matches PyTorch output!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Numerical differences detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934775c",
   "metadata": {},
   "source": [
    "## 6. Latency Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f27eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_inference(model, device, num_runs=100, warmup=10):\n",
    "    \"\"\"Benchmark recursive inference latency.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_img = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = recursive_inference(model, test_img, steps=16)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        _ = recursive_inference(model, test_img, steps=16)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Benchmark on CPU\n",
    "cpu_times = benchmark_inference(model, torch.device('cpu'), num_runs=50)\n",
    "print(f\"CPU Latency: {np.mean(cpu_times)*1000:.2f} ¬± {np.std(cpu_times)*1000:.2f} ms\")\n",
    "\n",
    "# Benchmark on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    gpu_times = benchmark_inference(model, torch.device('cuda'), num_runs=100)\n",
    "    print(f\"GPU Latency: {np.mean(gpu_times)*1000:.2f} ¬± {np.std(gpu_times)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5027b0",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### ‚úÖ This notebook provides a complete workflow:\n",
    "\n",
    "| Step | Cell | Description |\n",
    "|------|------|-------------|\n",
    "| 0 | Mount Drive | Connect Google Drive for data/checkpoints |\n",
    "| 0.1 | Download Data | Download FFHQ, COCO, text images directly to Drive |\n",
    "| 1 | Setup | Clone repo & install dependencies |\n",
    "| 1.1 | Train | Build manifest & train with DINOv3 distillation |\n",
    "| 2-3 | Architecture | Verify model structure & forward pass |\n",
    "| 4 | Recursion | Test recursive inference convergence |\n",
    "| 5 | Robustness | Perturbation stability testing |\n",
    "| 6 | ONNX | Export & validate ONNX model |\n",
    "| 7 | Save | Copy ONNX to Drive for Go runtime |\n",
    "\n",
    "### üì¶ Output artifacts (on Google Drive):\n",
    "\n",
    "```\n",
    "/content/drive/MyDrive/dazzled/outputs/\n",
    "‚îú‚îÄ‚îÄ checkpoints/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ student_epoch_5.safetensors   # Trained weights\n",
    "‚îî‚îÄ‚îÄ models/\n",
    "    ‚îî‚îÄ‚îÄ recursive_hasher.onnx          # ONNX for Go runtime\n",
    "```\n",
    "\n",
    "### üîó References:\n",
    "- **DINOv3:** [arXiv:2508.10104](https://arxiv.org/abs/2508.10104)\n",
    "- **TRM:** [arXiv:2510.04871](https://arxiv.org/abs/2510.04871)\n",
    "- **Split Accumulation:** [ePrint 2020/1618](https://eprint.iacr.org/2020/1618)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a5805",
   "metadata": {},
   "source": [
    "## 7. Export ONNX to Google Drive\n",
    "\n",
    "Save the trained model to Drive for use in the Go application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT TRAINED ONNX MODEL TO GOOGLE DRIVE\n",
    "# =============================================================================\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Source: the ONNX file exported earlier in this notebook\n",
    "source_onnx = Path(\"test_model.onnx\")\n",
    "\n",
    "# Destination on Drive\n",
    "dest_dir = Path(\"/content/drive/MyDrive/dazzled/outputs/models\")\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "dest_onnx = dest_dir / \"recursive_hasher.onnx\"\n",
    "\n",
    "if source_onnx.exists():\n",
    "    shutil.copy(source_onnx, dest_onnx)\n",
    "    size_mb = dest_onnx.stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úì ONNX model saved to Drive: {dest_onnx}\")\n",
    "    print(f\"  Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Also save the latest checkpoint\n",
    "    ckpt_dir = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
    "    checkpoints = sorted(ckpt_dir.glob(\"*.safetensors\"))\n",
    "    if checkpoints:\n",
    "        print(f\"\\nüì¶ Available artifacts on Drive:\")\n",
    "        print(f\"  Model: {dest_onnx}\")\n",
    "        print(f\"  Checkpoint: {checkpoints[-1]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ONNX file not found. Run the ONNX export cell first (Cell 23-25).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
