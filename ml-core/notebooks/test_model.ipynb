{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6732215f",
   "metadata": {
    "id": "6732215f"
   },
   "source": [
    "# ü¶ñ DaZZLeD: Recursive Hasher Training Notebook\n",
    "\n",
    "**Goal:** Train a Tiny Recursive Model (TRM) using DINOv3 distillation for adversarially-robust perceptual hashing.\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "‚ö†Ô∏è **FIRST:** Runtime ‚Üí Change runtime type ‚Üí **GPU (T4 free or A100)**\n",
    "\n",
    "1. **Run Cell 1** - Mount Google Drive *(required after every runtime change!)*\n",
    "2. **Run Cell 2** - Download training datasets (first run ~30 min, cached runs ~2 min)\n",
    "3. **Run Cell 3** - Clone repo & install dependencies\n",
    "4. **Run Cell 4** - Build manifest & train the model\n",
    "5. **Run remaining cells** - Test and export the trained model\n",
    "\n",
    "> üí° **Tip:** If you change runtime mid-session, re-run Cell 1 (Mount Drive) first. The data zip on Drive persists - it just needs to be extracted to local storage again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd8ce8",
   "metadata": {
    "id": "7ccd8ce8"
   },
   "source": [
    "## 0. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef02f76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ef02f76",
    "outputId": "316d357e-8921-48c7-f0c9-7300d0e00965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "‚úì Project root: /content/drive/MyDrive/dazzled\n",
      "‚úì Data root: /content/drive/MyDrive/dazzled/data\n",
      "‚úì Output root: /content/drive/MyDrive/dazzled/outputs\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (required for data storage)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
    "DATA_ROOT = DRIVE_ROOT / \"data\"\n",
    "OUTPUT_ROOT = DRIVE_ROOT / \"outputs\"\n",
    "\n",
    "# Create all needed directories\n",
    "for d in [DATA_ROOT / \"ffhq\", DATA_ROOT / \"openimages\", DATA_ROOT / \"text\",\n",
    "          OUTPUT_ROOT / \"checkpoints\", OUTPUT_ROOT / \"models\",\n",
    "          DRIVE_ROOT / \"manifests\"]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Project root: {DRIVE_ROOT}\")\n",
    "print(f\"‚úì Data root: {DATA_ROOT}\")\n",
    "print(f\"‚úì Output root: {OUTPUT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaee7b4",
   "metadata": {
    "id": "dfaee7b4"
   },
   "source": [
    "## 1.1 Build Manifest & Train TRM Model\n",
    "\n",
    "‚ö†Ô∏è **Switch to GPU first:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or A100)\n",
    "\n",
    "This cell:\n",
    "1. Builds a manifest of all training images\n",
    "2. Trains the **TRM Hasher** using proper TRM deep supervision (from [arXiv:2510.04871](https://arxiv.org/abs/2510.04871))\n",
    "3. Distills DINOv3 teacher into tiny 2-layer recursive network\n",
    "4. Saves checkpoints to Google Drive\n",
    "\n",
    "**Key TRM features:**\n",
    "- Deep supervision: N_sup=16 steps per sample, loss at each step\n",
    "- Two features: y (hash) and z (latent reasoning)\n",
    "- Latent recursion: z = net(x, y, z) √ó 6, then y = net(y, z)\n",
    "- EMA for stability (decay=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfac76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13dfac76",
    "outputId": "cfd039fa-27c2-4793-f7b0-edf5e418a912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "üöÄ DOWNLOADING DATASETS\n",
      "=================================================================\n",
      "\n",
      "‚úì [1/3] FFHQ already exists (52,001 images). Skipping download.\n",
      "\n",
      "üì• [2/3] OpenImages via FiftyOne\n",
      "   Method: Official Google download (handles AWS S3 shards)\n",
      "   Target: 15k diverse real-world images\n",
      "   Downloading from AWS S3...\n",
      "Downloading split 'validation' to '/root/fiftyone/open-images-v7/validation' if necessary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/open-images-v7/validation' if necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 15000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.utils.openimages:Downloading 15000 images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [4.9m elapsed, 0s remaining, 50.6 files/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [4.9m elapsed, 0s remaining, 50.6 files/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info written to '/root/fiftyone/open-images-v7/info.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/open-images-v7/info.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'open-images-v7' split 'validation'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.zoo.datasets:Loading 'open-images-v7' split 'validation'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  94% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-| 14173/15000 [11.4m elapsed, 40.2s remaining, 20.6 samples/s]   "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üì¶ DOWNLOAD & PREPARE DATASETS\n",
    "# =============================================================================\n",
    "# This cell downloads and prepares the training data.\n",
    "# Total: ~45k images from 3 sources:\n",
    "#   - FFHQ: Kaggle (face images for identity)\n",
    "#   - OpenImages: FiftyOne (diverse real-world objects)\n",
    "#   - MobileViews: HuggingFace parquet\n",
    "#\n",
    "# WHY THIS MIX?\n",
    "#   - FFHQ 40k: Faces require precise hashing (identity preservation)\n",
    "#   - OpenImages 2.5k: Broad category coverage (animals, vehicles, food)\n",
    "#   - MobileViews 2k: Edge cases for text/UI (600k available, 2k is enough)\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Config\n",
    "DATA_ROOT = Path(\"/content/data\")\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
    "DRIVE_ARCHIVE = DRIVE_ROOT / \"data-cache/training-images.zip\"\n",
    "\n",
    "# =============================================================================\n",
    "# ‚ö†Ô∏è CHECK: Is Google Drive mounted?\n",
    "# =============================================================================\n",
    "if not Path(\"/content/drive/MyDrive\").exists():\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå Google Drive is NOT mounted!\\n\\n\"\n",
    "        \"This happens after runtime changes (CPU‚ÜíGPU, session timeout, etc.)\\n\\n\"\n",
    "        \"FIX: Run the 'Mount Google Drive' cell above first (Cell 1),\\n\"\n",
    "        \"     then re-run this cell. Your cached data zip is safe on Drive.\"\n",
    "    )\n",
    "\n",
    "EXPECTED_COUNTS = {\n",
    "    \"ffhq\": 40000,          # Full dataset\n",
    "    \"openimages\": 2500,     # Subset\n",
    "    \"mobileviews\": 1500,    # Target: 2k\n",
    "}\n",
    "\n",
    "def validate_dataset(data_root: Path, expected: dict) -> tuple[bool, dict]:\n",
    "    \"\"\"Validate all datasets have enough images.\"\"\"\n",
    "    results = {}\n",
    "    for name, exp_count in expected.items():\n",
    "        path = data_root / name\n",
    "        actual = len(list(path.glob(\"*.jpg\"))) if path.exists() else 0\n",
    "        results[name] = {\"count\": actual, \"expected\": exp_count, \"valid\": actual >= exp_count * 0.95}\n",
    "    return all(r[\"valid\"] for r in results.values()), results\n",
    "\n",
    "# Check what we already have\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize download flags\n",
    "need_download = {\"ffhq\": False, \"openimages\": False, \"mobileviews\": False}\n",
    "skip_downloads = False\n",
    "\n",
    "# Option 1: Restore from Drive cache (fastest)\n",
    "if DRIVE_ARCHIVE.exists():\n",
    "    print(\"üîÑ Found cached dataset on Drive!\")\n",
    "    print(f\"   üìÅ {DRIVE_ARCHIVE}\")\n",
    "    shutil.unpack_archive(DRIVE_ARCHIVE, DATA_ROOT)\n",
    "    all_valid, validation = validate_dataset(DATA_ROOT, EXPECTED_COUNTS)\n",
    "    if all_valid:\n",
    "        print(\"\\n‚úÖ All datasets restored from cache successfully!\")\n",
    "        for name, info in validation.items():\n",
    "            print(f\"   ‚úì {name}: {info['count']:,} images\")\n",
    "        print(\"\\nüéâ Skipping downloads - data is ready!\")\n",
    "        skip_downloads = True\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Cache incomplete, will download missing data:\")\n",
    "        for name, info in validation.items():\n",
    "            if not info[\"valid\"]:\n",
    "                need_download[name] = True\n",
    "                print(f\"   ‚úó {name}: {info['count']:,}/{info['expected']:,} (need more)\")\n",
    "            else:\n",
    "                print(f\"   ‚úì {name}: {info['count']:,} images (OK)\")\n",
    "else:\n",
    "    print(\"üì• No cache found on Drive, downloading all datasets...\")\n",
    "    need_download = {\"ffhq\": True, \"openimages\": True, \"mobileviews\": True}\n",
    "\n",
    "# Option 2: Download fresh data (only if cache was missing or incomplete)\n",
    "if not skip_downloads and any(need_download.values()):\n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "    print(\"DOWNLOADING DATASETS\")\n",
    "    print(\"=\"*65)\n",
    "\n",
    "    import torchvision.transforms as transforms\n",
    "    from PIL import Image\n",
    "    import io\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. FFHQ via Kaggle\n",
    "    # -------------------------------------------------------------------------\n",
    "    if need_download[\"ffhq\"]:\n",
    "        ffhq_dir = DATA_ROOT / \"ffhq\"\n",
    "        ffhq_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\"\\nüì• [1/3] FFHQ via Kaggle\")\n",
    "        print(\"   Target: 40k high-quality face images\")\n",
    "\n",
    "        # Setup Kaggle credentials from Colab secrets\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            import os\n",
    "            os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
    "            os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
    "            print(\"   ‚úì Kaggle credentials loaded from Colab secrets\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Could not load Kaggle secrets: {e}\")\n",
    "            print(\"   Add KAGGLE_USERNAME and KAGGLE_KEY to Colab secrets\")\n",
    "\n",
    "        # Download FFHQ from Kaggle\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"download\", \"-d\", \"arnaud58/flickrfaceshq-dataset-ffhq\",\n",
    "            \"-p\", str(ffhq_dir), \"--unzip\"\n",
    "        ], check=True)\n",
    "\n",
    "        # Flatten directory structure (Kaggle downloads into nested folders)\n",
    "        for nested_dir in ffhq_dir.rglob(\"*\"):\n",
    "            if nested_dir.is_file() and nested_dir.suffix.lower() in {\".jpg\", \".png\"}:\n",
    "                target = ffhq_dir / nested_dir.name\n",
    "                if not target.exists():\n",
    "                    shutil.move(str(nested_dir), str(target))\n",
    "\n",
    "        # Clean empty directories\n",
    "        for d in ffhq_dir.iterdir():\n",
    "            if d.is_dir():\n",
    "                shutil.rmtree(d)\n",
    "\n",
    "        count = len(list(ffhq_dir.glob(\"*.jpg\"))) + len(list(ffhq_dir.glob(\"*.png\")))\n",
    "        print(f\"   ‚úì FFHQ: {count:,} images\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. OpenImages via FiftyOne\n",
    "    # -------------------------------------------------------------------------\n",
    "    if need_download[\"openimages\"]:\n",
    "        oi_dir = DATA_ROOT / \"openimages\"\n",
    "        oi_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\"\\nüì• [2/3] OpenImages via FiftyOne\")\n",
    "        print(\"   Target: 2.5k diverse real-world images\")\n",
    "\n",
    "        try:\n",
    "            import fiftyone as fo\n",
    "            import fiftyone.zoo as foz\n",
    "\n",
    "            # Download a subset of OpenImages validation split\n",
    "            dataset = foz.load_zoo_dataset(\n",
    "                \"open-images-v7\",\n",
    "                split=\"validation\",\n",
    "                max_samples=2500,\n",
    "                shuffle=True,\n",
    "                seed=42,\n",
    "            )\n",
    "\n",
    "            # Copy images to our directory\n",
    "            for sample in dataset:\n",
    "                src = Path(sample.filepath)\n",
    "                dst = oi_dir / src.name\n",
    "                if src.exists() and not dst.exists():\n",
    "                    shutil.copy2(src, dst)\n",
    "\n",
    "            count = len(list(oi_dir.glob(\"*\")))\n",
    "            print(f\"   ‚úì OpenImages: {count:,} images\")\n",
    "\n",
    "            # Cleanup FiftyOne dataset\n",
    "            fo.delete_dataset(dataset.name)\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"   ‚ö†Ô∏è FiftyOne not installed. Installing...\")\n",
    "            subprocess.run([\"pip\", \"install\", \"fiftyone\"], check=True)\n",
    "            print(\"   Please re-run this cell after installation\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. MobileViews via HuggingFace\n",
    "    # -------------------------------------------------------------------------\n",
    "    if need_download[\"mobileviews\"]:\n",
    "        mv_dir = DATA_ROOT / \"mobileviews\"\n",
    "        mv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\"\\nüì• [3/3] MobileViews via HuggingFace\")\n",
    "        print(\"   Target: 2k mobile UI screenshots\")\n",
    "\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "\n",
    "            # Load subset of MobileViews dataset\n",
    "            ds = load_dataset(\n",
    "                \"mllmTeam/MobileViews\",\n",
    "                split=\"train\",\n",
    "                streaming=True,\n",
    "            )\n",
    "\n",
    "            # Take first 2000 samples\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.Lambda(lambda x: x.convert(\"RGB\")),\n",
    "            ])\n",
    "\n",
    "            count = 0\n",
    "            target = 2000\n",
    "            for i, sample in enumerate(ds):\n",
    "                if count >= target:\n",
    "                    break\n",
    "                try:\n",
    "                    img = sample.get(\"image\")\n",
    "                    if img is not None:\n",
    "                        img = transform(img)\n",
    "                        img.save(mv_dir / f\"mv_{count:05d}.jpg\", \"JPEG\", quality=85)\n",
    "                        count += 1\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            print(f\"   ‚úì MobileViews: {count:,} images\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"   ‚ö†Ô∏è datasets not installed. Installing...\")\n",
    "            subprocess.run([\"pip\", \"install\", \"datasets\"], check=True)\n",
    "            print(\"   Please re-run this cell after installation\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # üíæ SAVE TO DRIVE CACHE (for future sessions)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "    print(\"SAVING CACHE TO DRIVE\")\n",
    "    print(\"=\"*65)\n",
    "\n",
    "    DRIVE_ARCHIVE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create archive\n",
    "    print(f\"üì¶ Creating archive: {DRIVE_ARCHIVE}\")\n",
    "    shutil.make_archive(\n",
    "        str(DRIVE_ARCHIVE.with_suffix(\"\")),  # Remove .zip for make_archive\n",
    "        \"zip\",\n",
    "        DATA_ROOT,\n",
    "    )\n",
    "\n",
    "    archive_size = DRIVE_ARCHIVE.stat().st_size / (1024 ** 3)\n",
    "    print(f\"‚úì Cache saved ({archive_size:.2f} GB)\")\n",
    "    print(\"  Next session will restore from cache instantly!\")\n",
    "\n",
    "# =========================================================================\n",
    "# üìä FINAL VALIDATION\n",
    "# =========================================================================\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"FINAL DATASET VALIDATION\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "all_valid, validation = validate_dataset(DATA_ROOT, EXPECTED_COUNTS)\n",
    "total_images = sum(info[\"count\"] for info in validation.values())\n",
    "\n",
    "for name, info in validation.items():\n",
    "    status = \"‚úì\" if info[\"valid\"] else \"‚úó\"\n",
    "    print(f\"{status} {name}: {info['count']:,} / {info['expected']:,} images\")\n",
    "\n",
    "print(f\"\\nüìä Total: {total_images:,} images\")\n",
    "\n",
    "if all_valid:\n",
    "    print(\"‚úÖ All datasets ready for training!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some datasets are incomplete. Re-run this cell to download more.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c8053",
   "metadata": {
    "id": "1c4c8053"
   },
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade7e62",
   "metadata": {
    "id": "9ade7e62"
   },
   "outputs": [],
   "source": [
    "# Clone the repo (only needed in Colab)\n",
    "import os\n",
    "if not os.path.exists('DaZZLeD'):\n",
    "    !git clone https://github.com/D13ya/DaZZLeD.git\n",
    "    %cd DaZZLeD/ml-core\n",
    "else:\n",
    "    %cd DaZZLeD/ml-core\n",
    "\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f363c2",
   "metadata": {
    "id": "99f363c2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from models.trm_hasher import TRMHasher\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d9a99",
   "metadata": {
    "id": "4b3d9a99"
   },
   "source": [
    "## 1.1 Build Manifest & Train Model\n",
    "\n",
    "‚ö†Ô∏è **Switch to GPU first:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or A100)\n",
    "\n",
    "This cell:\n",
    "1. Builds a manifest of all training images\n",
    "2. Trains the RecursiveHasher using DINOv3 distillation\n",
    "3. Saves checkpoints to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674d4df",
   "metadata": {
    "id": "e674d4df"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILD MANIFEST FROM LOCAL DATA\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Point to local fast storage\n",
    "DATA_ROOT = Path(\"/content/data\")\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
    "\n",
    "# Find all training images\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "paths = [str(p) for p in DATA_ROOT.rglob(\"*\") if p.suffix.lower() in exts]\n",
    "\n",
    "print(f\"Found {len(paths):,} training images in {DATA_ROOT}\")\n",
    "\n",
    "if len(paths) < 100:\n",
    "    print(\"‚ö†Ô∏è  Not enough images! Run the download cell first.\")\n",
    "else:\n",
    "    # Write manifest to Drive so it persists, but content points to /content/data\n",
    "    manifest_path = DRIVE_ROOT / \"manifests/train.txt\"\n",
    "    manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(manifest_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(paths))\n",
    "\n",
    "    print(f\"‚úì Manifest written: {manifest_path}\")\n",
    "    print(f\"  (Points to {len(paths)} local files for high-speed training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291d049",
   "metadata": {
    "id": "7291d049"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN THE MODEL WITH TRM ARCHITECTURE (Deep Supervision)\n",
    "# =============================================================================\n",
    "# Using the proper TRM algorithm from https://arxiv.org/abs/2510.04871\n",
    "# Key differences from standard distillation:\n",
    "#   1. Deep supervision: N_sup=16 steps, loss at EACH step\n",
    "#   2. (y, z) features: y=hash, z=latent reasoning state\n",
    "#   3. Carry (y, z) across supervision steps (detached)\n",
    "#   4. EMA for stability (decay=0.999)\n",
    "\n",
    "# üîë AUTHENTICATION (Required for DINOv3)\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    print(\"üîê Logging in to Hugging Face...\")\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    if hf_token:\n",
    "        login(hf_token)\n",
    "        print(\"‚úì Logged in!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è HF_TOKEN not found in Secrets! You may encounter 401 Unauthorized errors.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Login failed: {e}\")\n",
    "\n",
    "# üîß TRM TRAINING OPTIONS - Uncomment ONE of the following:\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTION A: Quick Test (5-10 min on T4) - Just to verify everything works\n",
    "# -----------------------------------------------------------------------------\n",
    "# !python training/train_trm.py \\\n",
    "#     --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
    "#     --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
    "#     --epochs 1 \\\n",
    "#     --batch-size 64 \\\n",
    "#     --n-sup 8 \\\n",
    "#     --max-steps 100 \\\n",
    "#     --amp \\\n",
    "#     --log-interval 10 \\\n",
    "#     --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTION B: A100 80GB - TRM + SimCLR (Option A)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Loss function (mathematically clean):\n",
    "#   L = (1/N_sup) * Œ£_{k=1}^{N_sup} L_align(y_k, teacher) \n",
    "#       + Œª * L_SimCLR(y_N^1, y_N^2) / log(2B)\n",
    "#\n",
    "# Key properties:\n",
    "# - Distillation at EVERY step k (core TRM deep supervision)\n",
    "# - SimCLR ONLY at final step N (on most refined hash)\n",
    "# - Contrast loss normalized by log(2B) for stable Œª tuning\n",
    "# - Œª warmup: first epoch is pure TRM, then SimCLR kicks in\n",
    "#\n",
    "# Memory: batch 192 (two views) + DINOv3 + TRM ‚âà 60GB VRAM\n",
    "#\n",
    "# TRM architecture (from arXiv:2510.04871):\n",
    "#   --embed-dim 256, --hash-dim 128, --n-layers 2, --n-latent 6, --t 3, --n-sup 16\n",
    "#\n",
    "# SimCLR (true two-view NT-Xent, applied at final step only):\n",
    "#   --contrast-weight 0.3: Œª (normalized, 0.1-0.5 range)\n",
    "#   --nce-temperature 0.07: œÑ (lower = sharper)\n",
    "#   --contrast-warmup-epochs 1: first epoch is pure distillation\n",
    "#\n",
    "# If OOM: reduce --batch-size to 128\n",
    "# -----------------------------------------------------------------------------\n",
    "!python training/train_trm.py \\\n",
    "    --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
    "    --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
    "    --epochs 5 \\\n",
    "    --batch-size 192 \\\n",
    "    --embed-dim 256 \\\n",
    "    --hash-dim 128 \\\n",
    "    --n-layers 2 \\\n",
    "    --n-latent 6 \\\n",
    "    --t 3 \\\n",
    "    --n-sup 16 \\\n",
    "    --lr 1e-4 \\\n",
    "    --warmup-steps 2000 \\\n",
    "    --contrast-weight 0.3 \\\n",
    "    --nce-temperature 0.07 \\\n",
    "    --contrast-warmup-epochs 1 \\\n",
    "    --use-ema \\\n",
    "    --amp \\\n",
    "    --allow-tf32 \\\n",
    "    --channels-last \\\n",
    "    --cudnn-benchmark \\\n",
    "    --cache-ram \\\n",
    "    --workers 2 \\\n",
    "    --prefetch-factor 2 \\\n",
    "    --pin-memory \\\n",
    "    --log-interval 25 \\\n",
    "    --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints \\\n",
    "    --checkpoint-every 500\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTION C: Resume from Checkpoint (if session disconnected)\n",
    "# -----------------------------------------------------------------------------\n",
    "# !python training/train_trm.py \\\n",
    "#     --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
    "#     --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
    "#     --resume /content/drive/MyDrive/dazzled/outputs/checkpoints/student_epoch_1.safetensors \\\n",
    "#     --epochs 5 \\\n",
    "#     --batch-size 192 \\\n",
    "#     --embed-dim 256 \\\n",
    "#     --hash-dim 128 \\\n",
    "#     --n-layers 2 \\\n",
    "#     --n-latent 6 \\\n",
    "#     --t 3 \\\n",
    "#     --n-sup 16 \\\n",
    "#     --contrast-weight 0.3 \\\n",
    "#     --nce-temperature 0.07 \\\n",
    "#     --contrast-warmup-epochs 0 \\\n",
    "#     --use-ema \\\n",
    "#     --amp \\\n",
    "#     --allow-tf32 \\\n",
    "#     --channels-last \\\n",
    "#     --cudnn-benchmark \\\n",
    "#     --cache-ram \\\n",
    "#     --workers 2 \\\n",
    "#     --prefetch-factor 2 \\\n",
    "#     --pin-memory \\\n",
    "#     --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints \\\n",
    "#     --checkpoint-every 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102d066",
   "metadata": {
    "id": "5102d066"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIST CHECKPOINTS & LOAD TRAINED TRM WEIGHTS\n",
    "# =============================================================================\n",
    "# This cell is SELF-CONTAINED - run after runtime restart to reload model\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Add project path\n",
    "sys.path.insert(0, \"/content/DaZZLeD/ml-core\")\n",
    "\n",
    "# TRM Architecture parameters (MUST match training)\n",
    "EMBED_DIM = 256\n",
    "HASH_DIM = 96\n",
    "N_LAYERS = 2\n",
    "N_LATENT = 6\n",
    "T = 3  # Note: TRMHasher uses lowercase 't' parameter\n",
    "N_SUP = 16  # Supervision steps for inference\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Checkpoint directory on Drive\n",
    "CKPT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
    "ONNX_PATH = Path(\"/content/drive/MyDrive/dazzled/outputs/models/trm_hasher.onnx\")\n",
    "\n",
    "# Check if Drive is mounted\n",
    "if not Path(\"/content/drive/MyDrive\").exists():\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå Google Drive is NOT mounted!\\n\\n\"\n",
    "        \"FIX: Run the 'Mount Google Drive' cell first (Cell 2),\\n\"\n",
    "        \"     then re-run this cell.\"\n",
    "    )\n",
    "\n",
    "# List available checkpoints\n",
    "checkpoints = sorted(CKPT_DIR.glob(\"*.safetensors\"))\n",
    "print(f\"Found {len(checkpoints)} checkpoints in {CKPT_DIR}:\")\n",
    "\n",
    "# Separate epoch and step checkpoints\n",
    "epoch_ckpts = [c for c in checkpoints if \"epoch\" in c.name]\n",
    "step_ckpts = [c for c in checkpoints if \"step\" in c.name]\n",
    "\n",
    "if epoch_ckpts:\n",
    "    print(\"\\nüìÅ Epoch checkpoints:\")\n",
    "    for ckpt in epoch_ckpts:\n",
    "        size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {ckpt.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "if step_ckpts:\n",
    "    print(f\"\\nüìÅ Step checkpoints: {len(step_ckpts)} files\")\n",
    "    # Show first and last few\n",
    "    for ckpt in step_ckpts[:2] + step_ckpts[-2:]:\n",
    "        size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {ckpt.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# Load the latest checkpoint\n",
    "if epoch_ckpts:\n",
    "    latest = epoch_ckpts[-1]\n",
    "elif step_ckpts:\n",
    "    latest = step_ckpts[-1]\n",
    "elif checkpoints:\n",
    "    latest = checkpoints[-1]\n",
    "else:\n",
    "    latest = None\n",
    "    raise FileNotFoundError(f\"‚ö†Ô∏è No checkpoints found in {CKPT_DIR}!\")\n",
    "\n",
    "print(f\"\\nüîÑ Loading: {latest.name}\")\n",
    "\n",
    "# Import and create TRM model\n",
    "from models.trm_hasher import TRMHasher\n",
    "import safetensors.torch\n",
    "\n",
    "student = TRMHasher(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hash_dim=HASH_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_latent=N_LATENT,\n",
    "    t=T  # lowercase 't' to match TRMHasher signature\n",
    ").to(device)\n",
    "\n",
    "safetensors.torch.load_model(student, str(latest))\n",
    "student.eval()\n",
    "\n",
    "# Also set 'model' as alias for compatibility with other cells\n",
    "model = student\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in student.parameters())\n",
    "print(f\"\\n‚úì Loaded TRM model from {latest.name}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "print(f\"\\n‚úÖ Ready for validation milestones!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbba1d4",
   "metadata": {
    "id": "acbba1d4"
   },
   "source": [
    "## 2. Model Architecture Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3c710",
   "metadata": {
    "id": "30f3c710"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD TRM HASHER MODEL\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.insert(0, \"/content/DaZZLeD/ml-core\")\n",
    "from models.trm_hasher import TRMHasher\n",
    "\n",
    "# TRM Architecture parameters (must match training)\n",
    "EMBED_DIM = 256\n",
    "HASH_DIM = 96\n",
    "N_LAYERS = 2\n",
    "N_LATENT = 6\n",
    "T = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Only create a new model if one doesn't already exist\n",
    "if 'model' not in dir() or model is None:\n",
    "    print(\"‚ö†Ô∏è  Creating UNTRAINED model for architecture testing\")\n",
    "    model = TRMHasher(\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hash_dim=HASH_DIM,\n",
    "        n_layers=N_LAYERS,\n",
    "        n_latent=N_LATENT,\n",
    "        T=T\n",
    "    ).to(device)\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"‚úì Using existing model (trained weights loaded)\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6d21e",
   "metadata": {
    "id": "49f6d21e"
   },
   "outputs": [],
   "source": [
    "# Test forward pass with dummy input (TRM deep recursion)\n",
    "batch_size = 4\n",
    "image_size = 224\n",
    "\n",
    "dummy_img = torch.randn(batch_size, 3, image_size, image_size).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Use inference mode with N_sup supervision steps\n",
    "    hash_out = model.inference(dummy_img, n_sup=16)\n",
    "\n",
    "print(f\"Input image shape: {dummy_img.shape}\")\n",
    "print(f\"Output hash shape: {hash_out.shape}\")\n",
    "print(f\"Hash L2 norm (should be ~1.0): {torch.norm(hash_out, dim=1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b0ff3",
   "metadata": {
    "id": "871b0ff3"
   },
   "source": [
    "## 3. Recursive Inference Test\n",
    "\n",
    "The key innovation is running the model recursively 16 times, refining the hash at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc68871",
   "metadata": {
    "id": "dbc68871"
   },
   "outputs": [],
   "source": [
    "def recursive_inference_trm(model, image, n_sup=16):\n",
    "    \"\"\"\n",
    "    Run TRM recursive inference for the specified number of supervision steps.\n",
    "    \n",
    "    This implements the deep_recursion from the TRM paper:\n",
    "    - For each supervision step, run latent_recursion\n",
    "    - Return hash at each step to observe convergence\n",
    "    \"\"\"\n",
    "    batch_size = image.size(0)\n",
    "    device = image.device\n",
    "    \n",
    "    # Initialize y and z\n",
    "    y = model.y_init.expand(batch_size, -1).to(device)\n",
    "    z = model.z_init.expand(batch_size, -1).to(device)\n",
    "    \n",
    "    # Encode image once\n",
    "    with torch.no_grad():\n",
    "        x = model.image_encoder(image)\n",
    "    \n",
    "    hashes = []\n",
    "    with torch.no_grad():\n",
    "        for step in range(n_sup):\n",
    "            # Deep recursion: T-1 without grad, 1 with grad (all no-grad in inference)\n",
    "            for _ in range(model.t):  # lowercase 't' to match TRMHasher attribute\n",
    "                y, z = model.latent_recursion(x, y, z)\n",
    "            \n",
    "            # Compute hash at this step\n",
    "            hash_out = F.normalize(model.output_head(y), p=2, dim=-1)\n",
    "            hashes.append(hash_out.clone())\n",
    "            \n",
    "            # Carry forward (already detached since no_grad)\n",
    "    \n",
    "    return hashes\n",
    "\n",
    "# Run TRM recursive inference\n",
    "hashes = recursive_inference_trm(model, dummy_img.to(device), n_sup=16)\n",
    "\n",
    "print(f\"Generated {len(hashes)} hash vectors (one per supervision step)\")\n",
    "print(f\"Final hash shape: {hashes[-1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7063c3",
   "metadata": {
    "id": "7a7063c3"
   },
   "outputs": [],
   "source": [
    "# Analyze TRM hash convergence across supervision steps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute cosine similarity between consecutive steps\n",
    "similarities = []\n",
    "for i in range(1, len(hashes)):\n",
    "    sim = F.cosine_similarity(hashes[i], hashes[i-1], dim=1).mean().item()\n",
    "    similarities.append(sim)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(hashes)), similarities, 'b-o')\n",
    "plt.xlabel('Supervision Step')\n",
    "plt.ylabel('Cosine Similarity with Previous')\n",
    "plt.title('TRM Hash Convergence')\n",
    "plt.ylim([0.9, 1.0])\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot hash norm evolution\n",
    "norms = [torch.norm(h, dim=1).mean().item() for h in hashes]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(hashes)), norms, 'g-o')\n",
    "plt.xlabel('Supervision Step')\n",
    "plt.ylabel('L2 Norm')\n",
    "plt.title('Hash Norm Stability')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convergence check: similarity should increase and stabilize\n",
    "if len(similarities) > 5:\n",
    "    early_sim = np.mean(similarities[:3])\n",
    "    late_sim = np.mean(similarities[-3:])\n",
    "    print(f\"\\nConvergence Analysis:\")\n",
    "    print(f\"  Early steps avg similarity: {early_sim:.4f}\")\n",
    "    print(f\"  Late steps avg similarity:  {late_sim:.4f}\")\n",
    "    print(f\"  Improvement: {(late_sim - early_sim):.4f}\")\n",
    "    if late_sim > 0.995:\n",
    "        print(\"  ‚úì Model has converged (high stability)\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Model may need more training or supervision steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87e5f8",
   "metadata": {
    "id": "3f87e5f8"
   },
   "source": [
    "## 4. Adversarial Robustness Test\n",
    "\n",
    "Test if small perturbations to input cause large changes in hash (they shouldn't after recursive refinement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa469be7",
   "metadata": {
    "id": "aa469be7"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ROBUSTNESS TEST WITH PROPER SANITY CHECKS\n",
    "# =============================================================================\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Load normalization from model_config.json (saved during training)\n",
    "# This ensures eval uses the SAME mean/std as training (AutoImageProcessor values)\n",
    "CHECKPOINT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
    "config_path = CHECKPOINT_DIR / \"model_config.json\"\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path) as f:\n",
    "        model_config = json.load(f)\n",
    "    NORM_MEAN = model_config.get(\"norm_mean\", [0.485, 0.456, 0.406])\n",
    "    NORM_STD = model_config.get(\"norm_std\", [0.229, 0.224, 0.225])\n",
    "    print(f\"‚úì Loaded normalization from {config_path}\")\n",
    "    print(f\"  Mean: {NORM_MEAN}\")\n",
    "    print(f\"  Std:  {NORM_STD}\")\n",
    "else:\n",
    "    # Fallback to ImageNet defaults (may cause slight eval mismatch)\n",
    "    print(f\"‚ö†Ô∏è {config_path} not found, using ImageNet defaults\")\n",
    "    print(\"   (This may cause slight mismatch with training normalization)\")\n",
    "    NORM_MEAN = [0.485, 0.456, 0.406]\n",
    "    NORM_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "])\n",
    "\n",
    "# Load real test images\n",
    "DATA_ROOT = Path(\"/content/data\")\n",
    "test_image_paths = list(DATA_ROOT.rglob(\"*.jpg\"))[:10]\n",
    "\n",
    "if len(test_image_paths) < 2:\n",
    "    print(\"‚ö†Ô∏è Not enough test images, using synthetic data\")\n",
    "    test_image_paths = None\n",
    "\n",
    "# =============================================================================\n",
    "# SANITY CHECK 1: Embedding Collapse Detection\n",
    "# =============================================================================\n",
    "print(\"=\"*65)\n",
    "print(\"SANITY CHECK 1: Embedding Collapse Detection\")\n",
    "print(\"=\"*65)\n",
    "print(\"Comparing hashes of different images (should have LOW similarity)\\n\")\n",
    "\n",
    "if test_image_paths and len(test_image_paths) >= 5:\n",
    "    hashes = []\n",
    "    for img_path in test_image_paths[:5]:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            h = model.inference(x, n_sup=16)\n",
    "            hashes.append(F.normalize(h, dim=1).clone())  # .clone() to avoid aliasing!\n",
    "    \n",
    "    # Compute pairwise similarities\n",
    "    print(f\"{'Image Pair':<30} {'Cosine Sim':>12}\")\n",
    "    print(\"-\"*45)\n",
    "    collapse_detected = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c2f8d",
   "metadata": {
    "id": "1c7c2f8d"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE ROBUSTNESS ANALYSIS\n",
    "# =============================================================================\n",
    "# NOTE: Pillow must be upgraded BEFORE any PIL imports (see first cell).\n",
    "#       If you get JPEG errors, restart runtime and re-run from cell 1.\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"COMPREHENSIVE ROBUSTNESS ANALYSIS\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. HISTOGRAM OF PAIRWISE SIMILARITIES (1000+ pairs)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìä 1. Pairwise Similarity Distribution\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Get more images for statistical analysis\n",
    "all_test_images = list(DATA_ROOT.rglob(\"*.jpg\"))[:200]\n",
    "random.shuffle(all_test_images)\n",
    "\n",
    "# Compute hashes for many images\n",
    "all_hashes = []\n",
    "print(f\"Computing hashes for {min(100, len(all_test_images))} images...\")\n",
    "for img_path in all_test_images[:100]:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            h = model.inference(x, n_sup=16)\n",
    "            all_hashes.append(F.normalize(h, dim=1).clone().cpu())\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(f\"Computed {len(all_hashes)} hashes\")\n",
    "\n",
    "# Compute pairwise similarities\n",
    "pairwise_sims = []\n",
    "for i in range(len(all_hashes)):\n",
    "    for j in range(i+1, len(all_hashes)):\n",
    "        sim = F.cosine_similarity(all_hashes[i], all_hashes[j]).item()\n",
    "        pairwise_sims.append(sim)\n",
    "\n",
    "pairwise_sims = np.array(pairwise_sims)\n",
    "print(f\"Computed {len(pairwise_sims)} pairwise similarities\")\n",
    "print(f\"   Mean: {pairwise_sims.mean():.4f}\")\n",
    "print(f\"   Std:  {pairwise_sims.std():.4f}\")\n",
    "print(f\"   Min:  {pairwise_sims.min():.4f}\")\n",
    "print(f\"   Max:  {pairwise_sims.max():.4f}\")\n",
    "print(f\"   % > 0.95: {100 * (pairwise_sims > 0.95).mean():.1f}%\")\n",
    "print(f\"   % > 0.99: {100 * (pairwise_sims > 0.99).mean():.1f}%\")\n",
    "\n",
    "# Plot histogram\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(pairwise_sims, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0.9, color='r', linestyle='--', label='0.9 threshold')\n",
    "axes[0].axvline(x=pairwise_sims.mean(), color='g', linestyle='-', label=f'Mean={pairwise_sims.mean():.3f}')\n",
    "axes[0].set_xlabel('Cosine Similarity')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Pairwise Similarity Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. PIXEL-SPACE NOISE (before normalization)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìä 2. Pixel-Space Noise (Pre-Normalization)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Raw transform without normalization\n",
    "raw_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),  # Just [0,1] range, no normalization\n",
    "])\n",
    "\n",
    "normalize = transforms.Normalize(mean=NORM_MEAN, std=NORM_STD)\n",
    "\n",
    "# Use a real image\n",
    "test_img = Image.open(all_test_images[0]).convert(\"RGB\")\n",
    "raw_tensor = raw_transform(test_img).unsqueeze(0).to(device)  # [0,1] range\n",
    "\n",
    "# Get baseline (apply noise BEFORE normalization)\n",
    "baseline_normalized = normalize(raw_tensor.squeeze(0)).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    baseline_hash = model.inference(baseline_normalized, n_sup=16)\n",
    "    baseline_hash = F.normalize(baseline_hash, dim=1).clone()\n",
    "\n",
    "pixel_noise_results = []\n",
    "pixel_epsilons = [0.01, 0.05, 0.1, 0.2, 0.3]  # In [0,1] pixel range\n",
    "\n",
    "for eps in pixel_epsilons:\n",
    "    # Add noise in pixel space [0,1]\n",
    "    noise = torch.randn_like(raw_tensor) * eps\n",
    "    noisy_pixels = torch.clamp(raw_tensor + noise, 0, 1)\n",
    "    \n",
    "    # Then normalize\n",
    "    noisy_normalized = normalize(noisy_pixels.squeeze(0)).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        noisy_hash = model.inference(noisy_normalized, n_sup=16)\n",
    "        noisy_hash = F.normalize(noisy_hash, dim=1).clone()\n",
    "    \n",
    "    sim = F.cosine_similarity(baseline_hash, noisy_hash).item()\n",
    "    pixel_noise_results.append((eps, sim))\n",
    "    print(f\"Pixel noise œÉ={eps:.2f}: Cosine={sim:.6f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. HARD TRANSFORMS (JPEG, Blur, Crop, Resize)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìä 3. Hard Transform Invariance\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "hard_transform_results = []\n",
    "\n",
    "# JPEG compression using torchvision (bypasses PIL's buggy JPEG encoder)\n",
    "import torchvision.io as tvio\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "\n",
    "def jpeg_compress(pil_img, quality):\n",
    "    \"\"\"Compress image via JPEG using torchvision (avoids PIL JPEG bugs).\"\"\"\n",
    "    t = (to_tensor(pil_img) * 255).clamp(0, 255).to(torch.uint8)\n",
    "    enc = tvio.encode_jpeg(t, quality=quality)\n",
    "    dec = tvio.decode_jpeg(enc)\n",
    "    return to_pil_image(dec)\n",
    "\n",
    "for quality in [95, 75, 50, 25, 10]:\n",
    "    try:\n",
    "        jpeg_img = jpeg_compress(test_img, quality)\n",
    "        x = transform(jpeg_img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            h = model.inference(x, n_sup=16)\n",
    "            h = F.normalize(h, dim=1).clone()\n",
    "        sim = F.cosine_similarity(baseline_hash.to(device), h).item()\n",
    "        hard_transform_results.append((f\"JPEG q={quality}\", sim))\n",
    "        print(f\"JPEG quality={quality:2d}: Cosine={sim:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"JPEG quality={quality:2d}: FAILED - {e}\")\n",
    "        print(\"   Try: Runtime ‚Üí Restart runtime, then re-run cells\")\n",
    "\n",
    "# Gaussian blur\n",
    "for radius in [1, 2, 3, 5]:\n",
    "    blurred = test_img.filter(ImageFilter.GaussianBlur(radius=radius))\n",
    "    x = transform(blurred).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        h = model.inference(x, n_sup=16)\n",
    "        h = F.normalize(h, dim=1).clone()\n",
    "    sim = F.cosine_similarity(baseline_hash.to(device), h).item()\n",
    "    hard_transform_results.append((f\"Blur r={radius}\", sim))\n",
    "    print(f\"Blur radius={radius}: Cosine={sim:.6f}\")\n",
    "\n",
    "# Center crop + resize back\n",
    "for crop_pct in [0.9, 0.8, 0.7, 0.6]:\n",
    "    w, h_img = test_img.size\n",
    "    crop_w, crop_h = int(w * crop_pct), int(h_img * crop_pct)\n",
    "    left = (w - crop_w) // 2\n",
    "    top = (h_img - crop_h) // 2\n",
    "    cropped = test_img.crop((left, top, left + crop_w, top + crop_h))\n",
    "    cropped = cropped.resize((w, h_img), Image.BICUBIC)\n",
    "    x = transform(cropped).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        hash_out = model.inference(x, n_sup=16)\n",
    "        hash_out = F.normalize(hash_out, dim=1).clone()\n",
    "    sim = F.cosine_similarity(baseline_hash.to(device), hash_out).item()\n",
    "    hard_transform_results.append((f\"Crop {int(crop_pct*100)}%\", sim))\n",
    "    print(f\"Center crop {int(crop_pct*100)}%: Cosine={sim:.6f}\")\n",
    "\n",
    "# Plot hard transforms\n",
    "transform_names = [r[0] for r in hard_transform_results]\n",
    "transform_sims = [r[1] for r in hard_transform_results]\n",
    "\n",
    "axes[1].barh(transform_names, transform_sims, color='steelblue', edgecolor='black')\n",
    "axes[1].axvline(x=0.9, color='r', linestyle='--', label='0.9 threshold')\n",
    "axes[1].set_xlabel('Cosine Similarity')\n",
    "axes[1].set_title('Hard Transform Robustness')\n",
    "axes[1].set_xlim(0, 1.05)\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot pixel noise curve\n",
    "pixel_eps = [r[0] for r in pixel_noise_results]\n",
    "pixel_sims = [r[1] for r in pixel_noise_results]\n",
    "axes[2].plot(pixel_eps, pixel_sims, 'go-', linewidth=2, markersize=8)\n",
    "axes[2].axhline(y=0.9, color='r', linestyle='--', label='0.9 threshold')\n",
    "axes[2].set_xlabel('Pixel Noise œÉ (in [0,1] range)')\n",
    "axes[2].set_ylabel('Cosine Similarity')\n",
    "axes[2].set_title('Pixel-Space Noise Robustness')\n",
    "axes[2].set_ylim(0.5, 1.05)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"ROBUSTNESS SUMMARY\")\n",
    "print(\"=\"*65)\n",
    "print(f\"‚úì Pairwise similarity: mean={pairwise_sims.mean():.3f}, std={pairwise_sims.std():.3f}\")\n",
    "print(f\"  ‚Üí {100*(pairwise_sims > 0.95).mean():.1f}% pairs > 0.95 (potential near-duplicates)\")\n",
    "\n",
    "# Safe extraction of results\n",
    "jpeg_25 = [r[1] for r in hard_transform_results if 'q=25' in r[0]]\n",
    "blur_3 = [r[1] for r in hard_transform_results if 'r=3' in r[0]]\n",
    "crop_70 = [r[1] for r in hard_transform_results if '70%' in r[0]]\n",
    "\n",
    "if jpeg_25: print(f\"‚úì JPEG q=25: {jpeg_25[0]:.4f}\")\n",
    "if blur_3: print(f\"‚úì Blur r=3:  {blur_3[0]:.4f}\")\n",
    "if crop_70: print(f\"‚úì Crop 70%:  {crop_70[0]:.4f}\")\n",
    "\n",
    "# Pass/fail assessment\n",
    "jpeg_results = [r[1] for r in hard_transform_results if 'JPEG' in r[0]]\n",
    "blur_results = [r[1] for r in hard_transform_results if 'Blur' in r[0]]\n",
    "\n",
    "jpeg_robust = all(s > 0.9 for s in jpeg_results) if jpeg_results else False\n",
    "blur_robust = all(s > 0.85 for s in blur_results) if blur_results else False\n",
    "no_collapse = pairwise_sims.mean() < 0.9\n",
    "\n",
    "print(f\"\\n{'‚úì' if jpeg_robust else '‚úó'} JPEG robustness (all > 0.9)\")\n",
    "print(f\"{'‚úì' if blur_robust else '‚úó'} Blur robustness (all > 0.85)\")\n",
    "print(f\"{'‚úì' if no_collapse else '‚úó'} No collapse (mean pairwise < 0.9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f18b35c",
   "metadata": {},
   "source": [
    "## Deterministic Eval & Threshold Calibration\n",
    "\n",
    "The previous robustness analysis used random augmentations which inflate similarity.\n",
    "This cell uses **deterministic transforms** (Resize ‚Üí CenterCrop ‚Üí Normalize) for accurate measurement.\n",
    "\n",
    "Then we calibrate the threshold using ROC/PR curves:\n",
    "- **Same image (augmented)**: Should match (positives)\n",
    "- **Different images**: Should NOT match (negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b836a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETERMINISTIC EVALUATION WITH THRESHOLD CALIBRATION\n",
    "# =============================================================================\n",
    "# Uses eval transforms (no random augmentation) for unbiased measurement\n",
    "# Calibrates threshold using ROC/PR curves\n",
    "# IMPORTANT: Uses the SAME normalization as training for consistency!\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import json\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"DETERMINISTIC EVALUATION & THRESHOLD CALIBRATION\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOAD NORMALIZATION CONFIG FROM TRAINING (for consistency)\n",
    "# -----------------------------------------------------------------------------\n",
    "CKPT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
    "config_path = CKPT_DIR / \"model_config.json\"\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path) as f:\n",
    "        model_config = json.load(f)\n",
    "    NORM_MEAN = model_config.get(\"norm_mean\", [0.485, 0.456, 0.406])\n",
    "    NORM_STD = model_config.get(\"norm_std\", [0.229, 0.224, 0.225])\n",
    "    IMAGE_SIZE = model_config.get(\"image_size\", 224)\n",
    "    print(f\"‚úì Loaded normalization from training config:\")\n",
    "    print(f\"   mean={NORM_MEAN}\")\n",
    "    print(f\"   std={NORM_STD}\")\n",
    "else:\n",
    "    # Fallback to ImageNet defaults\n",
    "    NORM_MEAN = [0.485, 0.456, 0.406]\n",
    "    NORM_STD = [0.229, 0.224, 0.225]\n",
    "    IMAGE_SIZE = 224\n",
    "    print(\"‚ö†Ô∏è  No model_config.json found, using ImageNet defaults\")\n",
    "    print(\"   (This may cause eval/train distribution mismatch)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DETERMINISTIC EVAL TRANSFORM (no randomness!)\n",
    "# -----------------------------------------------------------------------------\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "])\n",
    "\n",
    "print(f\"‚úì Using deterministic eval transform (Resize‚ÜíCenterCrop({IMAGE_SIZE})‚ÜíNormalize)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# GATHER TEST IMAGES\n",
    "# -----------------------------------------------------------------------------\n",
    "all_test_images = list(DATA_ROOT.rglob(\"*.jpg\"))\n",
    "random.seed(42)  # Reproducible\n",
    "random.shuffle(all_test_images)\n",
    "test_images = all_test_images[:200]  # 200 images for calibration\n",
    "print(f\"‚úì Selected {len(test_images)} test images\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# COMPUTE HASHES WITH DETERMINISTIC TRANSFORM\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìä Computing hashes with deterministic transform...\")\n",
    "hashes = []\n",
    "for img_path in test_images:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = eval_transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            h = model.inference(x, n_sup=16)\n",
    "            hashes.append(F.normalize(h, dim=1).cpu())\n",
    "    except Exception:\n",
    "        hashes.append(None)\n",
    "\n",
    "valid_indices = [i for i, h in enumerate(hashes) if h is not None]\n",
    "valid_hashes = [hashes[i] for i in valid_indices]\n",
    "valid_images = [test_images[i] for i in valid_indices]\n",
    "print(f\"‚úì Computed {len(valid_hashes)} valid hashes\")\n",
    "\n",
    "# Stack for batch operations\n",
    "hash_tensor = torch.cat(valid_hashes, dim=0)  # [N, hash_dim]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# COMPUTE SIMILARITY DISTRIBUTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìä Computing similarity distributions...\")\n",
    "\n",
    "# 1. DIFFERENT IMAGES (negatives)\n",
    "neg_sims = []\n",
    "n = len(valid_hashes)\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        sim = F.cosine_similarity(valid_hashes[i], valid_hashes[j]).item()\n",
    "        neg_sims.append(sim)\n",
    "neg_sims = np.array(neg_sims)\n",
    "print(f\"  Different-image pairs: {len(neg_sims)}\")\n",
    "print(f\"    Mean: {neg_sims.mean():.4f}, Std: {neg_sims.std():.4f}\")\n",
    "print(f\"    [Min, Max]: [{neg_sims.min():.4f}, {neg_sims.max():.4f}]\")\n",
    "\n",
    "# 2. SAME IMAGE WITH AUGMENTATIONS (positives)\n",
    "# Apply realistic augmentations to create \"same image\" pairs\n",
    "augmentations = [\n",
    "    (\"JPEG q=50\", lambda img: jpeg_compress_eval(img, 50)),\n",
    "    (\"JPEG q=25\", lambda img: jpeg_compress_eval(img, 25)),\n",
    "    (\"Blur r=2\", lambda img: img.filter(ImageFilter.GaussianBlur(radius=2))),\n",
    "    (\"Blur r=4\", lambda img: img.filter(ImageFilter.GaussianBlur(radius=4))),\n",
    "    (\"Crop 80%\", lambda img: center_crop_resize(img, 0.8)),\n",
    "    (\"Crop 70%\", lambda img: center_crop_resize(img, 0.7)),\n",
    "]\n",
    "\n",
    "def jpeg_compress_eval(pil_img, quality):\n",
    "    import io\n",
    "    buffer = io.BytesIO()\n",
    "    pil_img.save(buffer, format='JPEG', quality=quality)\n",
    "    buffer.seek(0)\n",
    "    return Image.open(buffer).convert(\"RGB\")\n",
    "\n",
    "def center_crop_resize(pil_img, crop_pct):\n",
    "    w, h = pil_img.size\n",
    "    crop_w, crop_h = int(w * crop_pct), int(h * crop_pct)\n",
    "    left = (w - crop_w) // 2\n",
    "    top = (h - crop_h) // 2\n",
    "    cropped = pil_img.crop((left, top, left + crop_w, top + crop_h))\n",
    "    return cropped.resize((w, h), Image.BICUBIC)\n",
    "\n",
    "pos_sims = []\n",
    "sample_images = valid_images[:50]  # Use 50 images for augmentation pairs\n",
    "\n",
    "for img_path in sample_images:\n",
    "    try:\n",
    "        original = Image.open(img_path).convert(\"RGB\")\n",
    "        x_orig = eval_transform(original).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            h_orig = model.inference(x_orig, n_sup=16)\n",
    "            h_orig = F.normalize(h_orig, dim=1)\n",
    "        \n",
    "        for aug_name, aug_fn in augmentations:\n",
    "            augmented = aug_fn(original)\n",
    "            x_aug = eval_transform(augmented).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                h_aug = model.inference(x_aug, n_sup=16)\n",
    "                h_aug = F.normalize(h_aug, dim=1)\n",
    "            \n",
    "            sim = F.cosine_similarity(h_orig, h_aug).item()\n",
    "            pos_sims.append(sim)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "pos_sims = np.array(pos_sims)\n",
    "print(f\"  Same-image (augmented) pairs: {len(pos_sims)}\")\n",
    "print(f\"    Mean: {pos_sims.mean():.4f}, Std: {pos_sims.std():.4f}\")\n",
    "print(f\"    [Min, Max]: [{pos_sims.min():.4f}, {pos_sims.max():.4f}]\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ROC & PR CURVE ANALYSIS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìä Computing ROC & PR curves...\")\n",
    "\n",
    "# Labels: 1 = same image (should match), 0 = different image (should not match)\n",
    "y_true = np.concatenate([np.ones(len(pos_sims)), np.zeros(len(neg_sims))])\n",
    "y_scores = np.concatenate([pos_sims, neg_sims])\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# PR curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_true, y_scores)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Find optimal thresholds\n",
    "# 1. Youden's J statistic (maximize TPR - FPR)\n",
    "j_scores = tpr - fpr\n",
    "best_j_idx = np.argmax(j_scores)\n",
    "threshold_youden = roc_thresholds[best_j_idx]\n",
    "\n",
    "# 2. F1-optimal threshold\n",
    "f1_scores = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-8)\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "threshold_f1 = pr_thresholds[best_f1_idx]\n",
    "\n",
    "# 3. High-precision threshold (precision > 0.95)\n",
    "high_prec_mask = precision[:-1] > 0.95\n",
    "if high_prec_mask.any():\n",
    "    high_prec_idx = np.where(high_prec_mask)[0][0]\n",
    "    threshold_high_prec = pr_thresholds[high_prec_idx]\n",
    "else:\n",
    "    threshold_high_prec = 0.99\n",
    "\n",
    "print(f\"\\nüìè Optimal Thresholds:\")\n",
    "print(f\"  Youden (max TPR-FPR):     {threshold_youden:.4f}\")\n",
    "print(f\"  F1-optimal:               {threshold_f1:.4f}\")\n",
    "print(f\"  High-precision (>95%):    {threshold_high_prec:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# VISUALIZATION\n",
    "# -----------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Similarity histograms\n",
    "axes[0].hist(neg_sims, bins=50, alpha=0.7, label=f'Different (n={len(neg_sims)})', color='red')\n",
    "axes[0].hist(pos_sims, bins=50, alpha=0.7, label=f'Same (n={len(pos_sims)})', color='green')\n",
    "axes[0].axvline(x=threshold_f1, color='blue', linestyle='--', label=f'F1 thresh={threshold_f1:.3f}')\n",
    "axes[0].axvline(x=threshold_youden, color='orange', linestyle=':', label=f'Youden={threshold_youden:.3f}')\n",
    "axes[0].set_xlabel('Cosine Similarity')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Similarity Distributions')\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# 2. ROC curve\n",
    "axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC={roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[1].scatter([fpr[best_j_idx]], [tpr[best_j_idx]], color='orange', s=100, zorder=5, label=f'Youden @ {threshold_youden:.3f}')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. PR curve\n",
    "axes[2].plot(recall, precision, 'g-', linewidth=2, label=f'PR (AUC={pr_auc:.3f})')\n",
    "axes[2].scatter([recall[best_f1_idx]], [precision[best_f1_idx]], color='blue', s=100, zorder=5, label=f'F1-opt @ {threshold_f1:.3f}')\n",
    "axes[2].set_xlabel('Recall')\n",
    "axes[2].set_ylabel('Precision')\n",
    "axes[2].set_title('Precision-Recall Curve')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SUMMARY & DIAGNOSIS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"CALIBRATION SUMMARY\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "separation = pos_sims.mean() - neg_sims.mean()\n",
    "overlap = (neg_sims > pos_sims.min()).sum() / len(neg_sims)\n",
    "\n",
    "print(f\"  Same-image mean:      {pos_sims.mean():.4f}\")\n",
    "print(f\"  Different-image mean: {neg_sims.mean():.4f}\")\n",
    "print(f\"  Separation:           {separation:.4f}\")\n",
    "print(f\"  Overlap (neg > min_pos): {100*overlap:.1f}%\")\n",
    "print(f\"\\n  ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"  PR AUC:  {pr_auc:.4f}\")\n",
    "\n",
    "# Diagnosis\n",
    "if separation < 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è  COLLAPSE DETECTED: Separation < 0.1\")\n",
    "    print(\"   ‚Üí Model is not discriminating between different images\")\n",
    "    print(\"   ‚Üí RETRAIN with --contrast-weight 0.5 (InfoNCE loss)\")\n",
    "elif separation < 0.3:\n",
    "    print(\"\\n‚ö†Ô∏è  WEAK DISCRIMINATION: Separation < 0.3\")\n",
    "    print(\"   ‚Üí Consider increasing hash_dim (128 or 192)\")\n",
    "    print(\"   ‚Üí Consider adding --contrast-weight 0.3\")\n",
    "else:\n",
    "    print(\"\\n‚úì Good discrimination (separation > 0.3)\")\n",
    "\n",
    "if roc_auc < 0.9:\n",
    "    print(f\"\\n‚ö†Ô∏è  ROC AUC = {roc_auc:.3f} (should be > 0.95 for production)\")\n",
    "else:\n",
    "    print(f\"\\n‚úì ROC AUC = {roc_auc:.3f} (good)\")\n",
    "\n",
    "print(f\"\\nüìù RECOMMENDED THRESHOLD: {threshold_f1:.4f}\")\n",
    "print(f\"   (Use this in your Go runtime for matching)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e33729",
   "metadata": {
    "id": "36e33729"
   },
   "source": [
    "## 5. ONNX Export Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe78324",
   "metadata": {
    "id": "dbe78324"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ONNX EXPORT FOR TRM HASHER\n",
    "# =============================================================================\n",
    "# The TRM model uses recursive inference, which ONNX doesn't handle well.\n",
    "# We export a single-step model that the Go runtime calls repeatedly.\n",
    "\n",
    "import os\n",
    "\n",
    "# Install onnxscript (required for torch.onnx.export in PyTorch 2.x)\n",
    "!pip install -q onnxscript\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Import ONNX wrapper\n",
    "from models.trm_hasher import TRMHasherONNX\n",
    "\n",
    "# Create ONNX-exportable wrapper from trained model\n",
    "onnx_model = TRMHasherONNX(student)  # `student` is the trained TRMHasher\n",
    "onnx_model.eval()\n",
    "onnx_model = onnx_model.to(\"cpu\")\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"trm_hasher.onnx\"\n",
    "\n",
    "# Dummy inputs for ONNX export\n",
    "batch_size = 1\n",
    "dummy_img = torch.randn(batch_size, 3, 224, 224)\n",
    "dummy_y = torch.randn(batch_size, EMBED_DIM)\n",
    "dummy_z = torch.randn(batch_size, EMBED_DIM)\n",
    "dummy_x_cached = torch.zeros(batch_size, EMBED_DIM)  # Zeros = need to encode\n",
    "\n",
    "torch.onnx.export(\n",
    "    onnx_model,\n",
    "    (dummy_img, dummy_y, dummy_z, dummy_x_cached),\n",
    "    onnx_path,\n",
    "    input_names=[\"image\", \"y\", \"z\", \"x_cached\"],\n",
    "    output_names=[\"x\", \"y_new\", \"z_new\", \"hash\"],\n",
    "    opset_version=18,\n",
    "    dynamic_axes={\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"y\": {0: \"batch\"},\n",
    "        \"z\": {0: \"batch\"},\n",
    "        \"x_cached\": {0: \"batch\"},\n",
    "        \"x\": {0: \"batch\"},\n",
    "        \"y_new\": {0: \"batch\"},\n",
    "        \"z_new\": {0: \"batch\"},\n",
    "        \"hash\": {0: \"batch\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"‚úì Exported TRM ONNX model to {onnx_path}\")\n",
    "print(f\"  File size: {os.path.getsize(onnx_path) / 1024:.2f} KB\")\n",
    "print(f\"\\n  Inputs: image[B,3,224,224], y[B,{EMBED_DIM}], z[B,{EMBED_DIM}], x_cached[B,{EMBED_DIM}]\")\n",
    "print(f\"  Outputs: x[B,{EMBED_DIM}], y_new[B,{EMBED_DIM}], z_new[B,{EMBED_DIM}], hash[B,{HASH_DIM}]\")\n",
    "print(f\"\\n  Usage: Call N_sup=16 times, passing (y_new, z_new, x) back as (y, z, x_cached)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe910628",
   "metadata": {
    "id": "fe910628"
   },
   "outputs": [],
   "source": [
    "# Validate ONNX model and test recursive inference\n",
    "onnx_model_loaded = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model_loaded)\n",
    "print(\"‚úì ONNX model validation passed!\")\n",
    "\n",
    "# Create ONNX Runtime session\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Test recursive ONNX inference (simulating Go runtime)\n",
    "N_SUP_TEST = 16\n",
    "rng = np.random.default_rng(42)\n",
    "test_img = rng.standard_normal((1, 3, 224, 224)).astype(np.float32)\n",
    "\n",
    "# Initialize y and z (from model's learned init)\n",
    "y = onnx_model.y_init.detach().cpu().numpy()\n",
    "z = onnx_model.z_init.detach().cpu().numpy()\n",
    "x_cached = np.zeros((1, EMBED_DIM), dtype=np.float32)\n",
    "\n",
    "print(f\"\\nRunning {N_SUP_TEST} supervision steps in ONNX Runtime...\")\n",
    "for step in range(N_SUP_TEST):\n",
    "    outputs = session.run(None, {\n",
    "        \"image\": test_img,\n",
    "        \"y\": y,\n",
    "        \"z\": z,\n",
    "        \"x_cached\": x_cached,\n",
    "    })\n",
    "    x_new, y_new, z_new, hash_onnx = outputs\n",
    "    \n",
    "    # For next iteration, use cached x (don't re-encode image)\n",
    "    x_cached = x_new\n",
    "    y = y_new\n",
    "    z = z_new\n",
    "    \n",
    "    if step in [0, 7, 15]:\n",
    "        print(f\"  Step {step+1}: hash L2 norm = {np.linalg.norm(hash_onnx, axis=1).mean():.4f}\")\n",
    "\n",
    "print(f\"\\n‚úì ONNX inference complete!\")\n",
    "print(f\"  Final hash shape: {hash_onnx.shape}\")\n",
    "print(f\"  Final hash L2 norm: {np.linalg.norm(hash_onnx, axis=1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ac143",
   "metadata": {
    "id": "fa9ac143"
   },
   "outputs": [],
   "source": [
    "# Compare PyTorch vs ONNX outputs\n",
    "print(\"Comparing PyTorch TRM vs ONNX outputs...\")\n",
    "\n",
    "# Run PyTorch inference with same input\n",
    "test_img_torch = torch.from_numpy(test_img)\n",
    "with torch.no_grad():\n",
    "    pt_hash = student.inference(test_img_torch.to(\"cpu\"), n_sup=N_SUP_TEST)\n",
    "    pt_hash_np = pt_hash.cpu().numpy()\n",
    "\n",
    "# Compare with ONNX output\n",
    "hash_diff = np.abs(pt_hash_np - hash_onnx).max()\n",
    "cosine_sim = np.dot(pt_hash_np.flatten(), hash_onnx.flatten()) / (\n",
    "    np.linalg.norm(pt_hash_np) * np.linalg.norm(hash_onnx)\n",
    ")\n",
    "\n",
    "print(f\"\\nMax absolute difference: {hash_diff:.8f}\")\n",
    "print(f\"Cosine similarity: {cosine_sim:.6f}\")\n",
    "\n",
    "if cosine_sim > 0.999:\n",
    "    print(\"‚úÖ ONNX export matches PyTorch output!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Some differences detected (may be due to floating point)\")\n",
    "    print(\"   This is expected for complex recursive models. Check cosine similarity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934775c",
   "metadata": {
    "id": "8934775c"
   },
   "source": [
    "## 6. Latency Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f27eef",
   "metadata": {
    "id": "48f27eef"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_trm_inference(model, device, n_sup=16, num_runs=100, warmup=10):\n",
    "    \"\"\"Benchmark TRM inference latency.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_img = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        with torch.no_grad():\n",
    "            _ = model.inference(test_img, n_sup=n_sup)\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model.inference(test_img, n_sup=n_sup)\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - start)\n",
    "\n",
    "    return times\n",
    "\n",
    "# Benchmark on CPU\n",
    "print(f\"Benchmarking TRM inference (N_sup={N_SUP})...\")\n",
    "cpu_times = benchmark_trm_inference(student, torch.device('cpu'), n_sup=N_SUP, num_runs=50)\n",
    "print(f\"CPU Latency: {np.mean(cpu_times)*1000:.2f} ¬± {np.std(cpu_times)*1000:.2f} ms\")\n",
    "\n",
    "# Benchmark on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    gpu_times = benchmark_trm_inference(student.to('cuda'), torch.device('cuda'), n_sup=N_SUP, num_runs=100)\n",
    "    print(f\"GPU Latency: {np.mean(gpu_times)*1000:.2f} ¬± {np.std(gpu_times)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5027b0",
   "metadata": {
    "id": "bd5027b0"
   },
   "source": [
    "## 8. Summary\n",
    "\n",
    "### ‚úÖ This notebook provides a complete TRM training workflow:\n",
    "\n",
    "| Step | Cell | Description |\n",
    "|------|------|-------------|\n",
    "| 0 | Mount Drive | Connect Google Drive for data/checkpoints |\n",
    "| 0.1 | Download Data | Download FFHQ, OpenImages, MobileViews (~45k images) |\n",
    "| 1 | Setup | Clone repo & install dependencies |\n",
    "| 1.1 | Train TRM | Build manifest & train with **TRM deep supervision** |\n",
    "| 2-3 | Architecture | Verify TRM structure & forward pass |\n",
    "| 4 | Recursion | Test N_sup supervision step convergence |\n",
    "| 5 | Robustness | Perturbation stability testing |\n",
    "| 6 | ONNX | Export & validate ONNX model |\n",
    "| 7 | Save | Copy ONNX to Drive for Go runtime |\n",
    "\n",
    "### üì¶ Output artifacts (on Google Drive):\n",
    "\n",
    "```\n",
    "/content/drive/MyDrive/dazzled/outputs/\n",
    "‚îú‚îÄ‚îÄ checkpoints/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ student_epoch_5.safetensors   # Trained TRM weights\n",
    "‚îî‚îÄ‚îÄ models/\n",
    "    ‚îî‚îÄ‚îÄ trm_hasher.onnx               # ONNX for Go runtime\n",
    "```\n",
    "\n",
    "### üîë TRM Architecture (from paper):\n",
    "- **2-layer network** with RMSNorm + SwiGLU\n",
    "- **Two features**: y (hash/answer), z (latent reasoning)\n",
    "- **Deep supervision**: N_sup=16 steps, loss at each step\n",
    "- **Latent recursion**: z = net(x,y,z) √ó 6, then y = net(y,z)\n",
    "- **Deep recursion**: T=3 cycles (2 no-grad + 1 grad)\n",
    "\n",
    "### üîó References:\n",
    "- **DINOv3:** [arXiv:2508.10104](https://arxiv.org/abs/2508.10104)\n",
    "- **TRM:** [arXiv:2510.04871](https://arxiv.org/abs/2510.04871) \"Less is More: Recursive Reasoning with Tiny Networks\"\n",
    "- **Split Accumulation:** [ePrint 2020/1618](https://eprint.iacr.org/2020/1618)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a5805",
   "metadata": {
    "id": "027a5805"
   },
   "source": [
    "## 7. Export ONNX to Google Drive\n",
    "\n",
    "Save the trained model to Drive for use in the Go application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb7fc8",
   "metadata": {
    "id": "74cb7fc8"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT TRAINED TRM ONNX MODEL TO GOOGLE DRIVE\n",
    "# =============================================================================\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Source: the ONNX file exported earlier in this notebook\n",
    "source_onnx = Path(\"trm_hasher.onnx\")\n",
    "\n",
    "# Destination on Drive\n",
    "dest_dir = Path(\"/content/drive/MyDrive/dazzled/outputs/models\")\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "dest_onnx = dest_dir / \"trm_hasher.onnx\"\n",
    "\n",
    "# Check if ONNX already exists on Drive\n",
    "if dest_onnx.exists():\n",
    "    existing_size = dest_onnx.stat().st_size / 1024\n",
    "    print(f\"‚ö†Ô∏è  ONNX model already exists on Drive!\")\n",
    "    print(f\"   Path: {dest_onnx}\")\n",
    "    print(f\"   Size: {existing_size:.2f} KB\")\n",
    "    print(f\"\\n   Skipping copy to preserve existing model.\")\n",
    "    print(f\"   To overwrite, delete the file manually or set FORCE_OVERWRITE = True below.\")\n",
    "    \n",
    "    FORCE_OVERWRITE = False  # <-- Set to True to overwrite\n",
    "    \n",
    "    if FORCE_OVERWRITE and source_onnx.exists():\n",
    "        shutil.copy(source_onnx, dest_onnx)\n",
    "        print(f\"\\n   ‚úì Overwrote existing model (FORCE_OVERWRITE=True)\")\n",
    "\n",
    "elif source_onnx.exists():\n",
    "    shutil.copy(source_onnx, dest_onnx)\n",
    "    size_kb = dest_onnx.stat().st_size / 1024\n",
    "    \n",
    "    print(f\"‚úì Exported TRM ONNX model to Drive!\")\n",
    "    print(f\"  Path: {dest_onnx}\")\n",
    "    print(f\"  Size: {size_kb:.2f} KB\")\n",
    "    print(f\"\\nüìã Model Interface (for Go runtime):\")\n",
    "    print(f\"  Inputs:\")\n",
    "    print(f\"    - image: [B, 3, 224, 224] float32 (normalized ImageNet)\")\n",
    "    print(f\"    - y: [B, {EMBED_DIM}] float32 (answer state)\")\n",
    "    print(f\"    - z: [B, {EMBED_DIM}] float32 (latent state)\")\n",
    "    print(f\"    - x_cached: [B, {EMBED_DIM}] float32 (cached image encoding)\")\n",
    "    print(f\"  Outputs:\")\n",
    "    print(f\"    - x: [B, {EMBED_DIM}] float32 (image encoding, cache this)\")\n",
    "    print(f\"    - y_new: [B, {EMBED_DIM}] float32 (updated answer state)\")\n",
    "    print(f\"    - z_new: [B, {EMBED_DIM}] float32 (updated latent state)\")\n",
    "    print(f\"    - hash: [B, {HASH_DIM}] float32 (current hash)\")\n",
    "    print(f\"\\nüîÅ Usage: Call {N_SUP} times, passing outputs back as inputs\")\n",
    "else:\n",
    "    print(f\"‚ùå ONNX file not found at {source_onnx}\")\n",
    "    print(\"   Run the ONNX export cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c2763",
   "metadata": {},
   "source": [
    "# üéØ MODEL VALIDATION MILESTONES\n",
    "\n",
    "Before exporting to ONNX and implementing in Go, the model must pass **all four milestones**.\n",
    "These tests ensure the recursive student is stable, accurate, and portable.\n",
    "\n",
    "| Milestone | Test | Success Criteria |\n",
    "|-----------|------|------------------|\n",
    "| 1 | Recursive Drift | Emb‚ÇÅ ‚âà Emb‚ÇÖ (cosine sim > 0.99) |\n",
    "| 2 | Validation Loss | Plateaued for 3-5 epochs |\n",
    "| 3 | Preprocessing Parity | Go/Python use identical transforms |\n",
    "| 4 | ONNX Parity | PyTorch vs ONNX diff < 1e-5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d458ef",
   "metadata": {},
   "source": [
    "## Milestone 1: Recursive Drift Test (Stability)\n",
    "\n",
    "A recursive model feeds its own output back into itself. If the model is unstable,\n",
    "errors compound and the embedding \"drifts\" into garbage after 2-3 passes.\n",
    "\n",
    "**Test:** Run an image through the student 5 times recursively.\n",
    "**Pass Criteria:** Distance between Emb‚ÇÅ and Emb‚ÇÖ should be near-zero (cosine similarity > 0.99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6896df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MILESTONE 1: TRM RECURSIVE DRIFT TEST\n",
    "# =============================================================================\n",
    "# Test that the TRM produces stable embeddings across multiple inference runs.\n",
    "# If the model is well-trained, repeated inference should produce same hash.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Load trained TRM model\n",
    "CHECKPOINT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
    "ONNX_PATH = Path(\"/content/drive/MyDrive/dazzled/outputs/models/trm_hasher.onnx\")\n",
    "\n",
    "# Find checkpoints - prioritize epoch checkpoints over step checkpoints\n",
    "all_checkpoints = sorted(CHECKPOINT_DIR.glob(\"*.safetensors\"))\n",
    "epoch_ckpts = [c for c in all_checkpoints if \"epoch\" in c.name]\n",
    "step_ckpts = [c for c in all_checkpoints if \"step\" in c.name]\n",
    "\n",
    "if epoch_ckpts:\n",
    "    latest_ckpt = epoch_ckpts[-1]  # Use final epoch checkpoint\n",
    "    print(f\"‚úì Loading EPOCH checkpoint: {latest_ckpt.name}\")\n",
    "elif step_ckpts:\n",
    "    latest_ckpt = step_ckpts[-1]\n",
    "    print(f\"‚ö†Ô∏è  No epoch checkpoints found. Loading step: {latest_ckpt.name}\")\n",
    "elif all_checkpoints:\n",
    "    latest_ckpt = all_checkpoints[-1]\n",
    "    print(f\"‚ö†Ô∏è  Loading: {latest_ckpt.name}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No checkpoints found in {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Load TRM model\n",
    "import safetensors.torch\n",
    "import sys\n",
    "sys.path.insert(0, \"/content/DaZZLeD/ml-core\")\n",
    "from models.trm_hasher import TRMHasher\n",
    "\n",
    "# TRM Architecture parameters (must match training)\n",
    "EMBED_DIM = 256\n",
    "HASH_DIM = 96\n",
    "N_LAYERS = 2\n",
    "N_LATENT = 6\n",
    "T = 3\n",
    "N_SUP = 16  # Supervision steps for inference\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student = TRMHasher(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hash_dim=HASH_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_latent=N_LATENT,\n",
    "    T=T\n",
    ").to(device)\n",
    "safetensors.torch.load_model(student, str(latest_ckpt))\n",
    "student.eval()\n",
    "\n",
    "# Standard ImageNet normalization (MUST match Go implementation)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Load a test image\n",
    "DATA_ROOT = Path(\"/content/data\")\n",
    "test_images = list(DATA_ROOT.rglob(\"*.jpg\"))[:5]\n",
    "if not test_images:\n",
    "    raise FileNotFoundError(\"No test images found\")\n",
    "\n",
    "print(f\"\\n{'='*65}\")\n",
    "print(\"MILESTONE 1: TRM RECURSIVE DRIFT TEST\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "milestone1_passed = True\n",
    "for img_path in test_images:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for pass_num in range(5):\n",
    "            # Run TRM inference (N_SUP supervision steps)\n",
    "            hash_out = student.inference(x, n_sup=N_SUP)\n",
    "            embeddings.append(F.normalize(hash_out, dim=1).cpu())\n",
    "    \n",
    "    # Compare first and last embeddings\n",
    "    emb1 = embeddings[0]\n",
    "    emb5 = embeddings[4]\n",
    "    cosine_sim = F.cosine_similarity(emb1, emb5).item()\n",
    "    l2_dist = torch.norm(emb1 - emb5).item()\n",
    "    \n",
    "    status = \"‚úì PASS\" if cosine_sim > 0.99 else \"‚úó FAIL\"\n",
    "    if cosine_sim <= 0.99:\n",
    "        milestone1_passed = False\n",
    "    \n",
    "    print(f\"{status} {img_path.name[:30]:<30} | Cosine(Run1,Run5)={cosine_sim:.6f} | L2={l2_dist:.6f}\")\n",
    "\n",
    "print(f\"\\n{'MILESTONE 1: PASSED ‚úì' if milestone1_passed else 'MILESTONE 1: FAILED ‚úó'}\")\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc35473",
   "metadata": {},
   "source": [
    "## Milestone 2: Validation Loss Plateau + Visual Inspection\n",
    "\n",
    "Check that validation loss has plateaued and visually verify embeddings on \"hard\" images\n",
    "(blurry faces, text documents, edge cases) are close to teacher embeddings.\n",
    "\n",
    "**Test:** Compare student vs teacher embeddings on held-out validation set.\n",
    "**Pass Criteria:** Student-Teacher cosine similarity > 0.95 on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80385c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MILESTONE 2: TRM TEACHER-STUDENT ALIGNMENT (Relative Similarity Preservation)\n",
    "# =============================================================================\n",
    "# Since teacher (1024-dim) and student (96-dim) have different dimensions,\n",
    "# we check if student PRESERVES RELATIVE SIMILARITIES learned from teacher.\n",
    "# \n",
    "# Test: For pairs of images, does student agree with teacher on which pairs\n",
    "# are more/less similar? This is the true test of distillation quality.\n",
    "\n",
    "from transformers import AutoModel\n",
    "from itertools import combinations\n",
    "\n",
    "# Load teacher model\n",
    "print(\"Loading DINOv3 teacher model...\")\n",
    "teacher = AutoModel.from_pretrained(\n",
    "    \"facebook/dinov3-vitl16-pretrain-lvd1689m\",\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "teacher.eval()\n",
    "\n",
    "# Select validation images (mix of categories for diversity)\n",
    "val_images = []\n",
    "for subdir in [\"ffhq\", \"openimages\", \"mobileviews\"]:\n",
    "    subdir_path = DATA_ROOT / subdir\n",
    "    if subdir_path.exists():\n",
    "        imgs = list(subdir_path.glob(\"*.jpg\"))[:5]\n",
    "        val_images.extend(imgs)\n",
    "\n",
    "if len(val_images) < 6:\n",
    "    val_images = test_images\n",
    "\n",
    "print(f\"\\n{'='*65}\")\n",
    "print(\"MILESTONE 2: TRM TEACHER-STUDENT ALIGNMENT\")\n",
    "print(\"=\"*65)\n",
    "print(f\"Evaluating relative similarity preservation on {len(val_images)} images...\")\n",
    "print(\"(Checking if student agrees with teacher on which images are similar)\\n\")\n",
    "\n",
    "# Collect all embeddings\n",
    "teacher_embeddings = []\n",
    "student_embeddings = []\n",
    "image_names = []\n",
    "\n",
    "for img_path in val_images:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Teacher embedding (1024-dim)\n",
    "        teacher_out = teacher(x)\n",
    "        teacher_emb = F.normalize(teacher_out.last_hidden_state[:, 0], dim=1)\n",
    "        teacher_embeddings.append(teacher_emb.cpu())\n",
    "        \n",
    "        # TRM Student embedding (96-dim via deep supervision inference)\n",
    "        student_hash = student.inference(x, n_sup=N_SUP)\n",
    "        student_emb = F.normalize(student_hash, dim=1)\n",
    "        student_embeddings.append(student_emb.cpu())\n",
    "        \n",
    "        image_names.append(f\"{img_path.parent.name}/{img_path.name[:15]}\")\n",
    "\n",
    "# Stack embeddings\n",
    "teacher_embs = torch.cat(teacher_embeddings, dim=0)  # [N, 1024]\n",
    "student_embs = torch.cat(student_embeddings, dim=0)  # [N, 96]\n",
    "\n",
    "# Check for embedding collapse (all embeddings too similar)\n",
    "student_avg_sim = 0\n",
    "n_pairs = 0\n",
    "for i in range(len(student_embs)):\n",
    "    for j in range(i+1, len(student_embs)):\n",
    "        student_avg_sim += F.cosine_similarity(student_embs[i:i+1], student_embs[j:j+1]).item()\n",
    "        n_pairs += 1\n",
    "student_avg_sim /= max(n_pairs, 1)\n",
    "\n",
    "print(f\"üìä Embedding Diversity Check:\")\n",
    "print(f\"   Student avg pairwise similarity: {student_avg_sim:.4f}\")\n",
    "if student_avg_sim > 0.95:\n",
    "    print(f\"   ‚ö†Ô∏è WARNING: Possible embedding collapse! All outputs too similar.\")\n",
    "else:\n",
    "    print(f\"   ‚úì Good diversity in student embeddings\")\n",
    "\n",
    "# Compute pairwise similarities for both models\n",
    "n_images = len(val_images)\n",
    "teacher_sims = []\n",
    "student_sims = []\n",
    "pair_names = []\n",
    "\n",
    "for i, j in combinations(range(n_images), 2):\n",
    "    t_sim = F.cosine_similarity(teacher_embs[i:i+1], teacher_embs[j:j+1]).item()\n",
    "    s_sim = F.cosine_similarity(student_embs[i:i+1], student_embs[j:j+1]).item()\n",
    "    teacher_sims.append(t_sim)\n",
    "    student_sims.append(s_sim)\n",
    "    pair_names.append(f\"{image_names[i][:12]} ‚Üî {image_names[j][:12]}\")\n",
    "\n",
    "teacher_sims = np.array(teacher_sims)\n",
    "student_sims = np.array(student_sims)\n",
    "\n",
    "# Compute rank correlation (Spearman) - does student preserve similarity ordering?\n",
    "from scipy.stats import spearmanr\n",
    "correlation, p_value = spearmanr(teacher_sims, student_sims)\n",
    "\n",
    "# Also check if student correctly identifies \"similar\" vs \"different\" pairs\n",
    "# (using median as threshold)\n",
    "teacher_median = np.median(teacher_sims)\n",
    "student_median = np.median(student_sims)\n",
    "\n",
    "teacher_similar = teacher_sims > teacher_median\n",
    "student_similar = student_sims > student_median\n",
    "agreement_rate = np.mean(teacher_similar == student_similar)\n",
    "\n",
    "# Show sample pairs\n",
    "print(f\"\\n{'Pair':<35} {'Teacher':>10} {'Student':>10} {'Agree?':>8}\")\n",
    "print(\"-\"*65)\n",
    "for idx in range(min(10, len(pair_names))):\n",
    "    t_sim = teacher_sims[idx]\n",
    "    s_sim = student_sims[idx]\n",
    "    t_high = \"high\" if t_sim > teacher_median else \"low\"\n",
    "    s_high = \"high\" if s_sim > student_median else \"low\"\n",
    "    agree = \"‚úì\" if t_high == s_high else \"‚úó\"\n",
    "    print(f\"{pair_names[idx]:<35} {t_sim:>10.4f} {s_sim:>10.4f} {agree:>8}\")\n",
    "\n",
    "if len(pair_names) > 10:\n",
    "    print(f\"... ({len(pair_names) - 10} more pairs)\")\n",
    "\n",
    "print(\"-\"*65)\n",
    "print(f\"\\nüìä ALIGNMENT METRICS:\")\n",
    "print(f\"   Spearman Correlation: {correlation:.4f} (p={p_value:.2e})\")\n",
    "print(f\"   Agreement Rate:       {agreement_rate*100:.1f}%\")\n",
    "print(f\"   (Student agrees with teacher on similar/different classification)\")\n",
    "\n",
    "# Pass criteria: \n",
    "# - Spearman correlation > 0.5 (moderate positive correlation)\n",
    "# - Agreement rate > 70%\n",
    "# - No embedding collapse (avg pairwise sim < 0.95)\n",
    "milestone2_passed = correlation > 0.5 and agreement_rate > 0.70 and student_avg_sim < 0.95\n",
    "\n",
    "print(f\"\\n{'MILESTONE 2: PASSED ‚úì' if milestone2_passed else 'MILESTONE 2: FAILED ‚úó'}\")\n",
    "print(f\"   (requires: correlation > 0.5 AND agreement > 70% AND no collapse)\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# Cleanup teacher to free GPU memory\n",
    "del teacher\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065321d",
   "metadata": {},
   "source": [
    "## Milestone 3: Preprocessing Parity Check\n",
    "\n",
    "**Critical:** The Go implementation MUST use identical preprocessing to Python.\n",
    "This cell documents and verifies the exact preprocessing pipeline.\n",
    "\n",
    "| Parameter | Value | Go Implementation |\n",
    "|-----------|-------|-------------------|\n",
    "| Image Size | 224√ó224 | `imaging.Resize(224, 224, imaging.Lanczos)` |\n",
    "| Interpolation | Bicubic | `imaging.Lanczos` (closest match) |\n",
    "| Normalization | ImageNet | mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225] |\n",
    "| Channel Order | RGB | Standard (not BGR) |\n",
    "| Data Type | float32 | `float32` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e699ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MILESTONE 3: PREPROCESSING PARITY CHECK\n",
    "# =============================================================================\n",
    "# Document and verify the exact preprocessing pipeline for Go parity.\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"MILESTONE 3: PREPROCESSING PARITY SPECIFICATION\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# Lock these values - changing them requires retraining!\n",
    "PREPROCESSING_SPEC = {\n",
    "    \"image_size\": IMAGE_SIZE,\n",
    "    \"interpolation\": \"BICUBIC\",  # Go: imaging.Lanczos (closest match)\n",
    "    \"normalization\": {\n",
    "        \"mean\": IMAGENET_MEAN,\n",
    "        \"std\": IMAGENET_STD,\n",
    "    },\n",
    "    \"channel_order\": \"RGB\",  # Not BGR!\n",
    "    \"data_type\": \"float32\",\n",
    "    \"recursion_steps\": RECURSION_STEPS,\n",
    "    \"state_dim\": STATE_DIM,\n",
    "    \"hash_dim\": HASH_DIM,\n",
    "}\n",
    "\n",
    "print(\"\\nüìã LOCKED PREPROCESSING SPECIFICATION:\")\n",
    "print(\"-\"*65)\n",
    "print(f\"  Image Size:      {PREPROCESSING_SPEC['image_size']}√ó{PREPROCESSING_SPEC['image_size']}\")\n",
    "print(f\"  Interpolation:   {PREPROCESSING_SPEC['interpolation']}\")\n",
    "print(f\"  Mean:            {PREPROCESSING_SPEC['normalization']['mean']}\")\n",
    "print(f\"  Std:             {PREPROCESSING_SPEC['normalization']['std']}\")\n",
    "print(f\"  Channel Order:   {PREPROCESSING_SPEC['channel_order']}\")\n",
    "print(f\"  Data Type:       {PREPROCESSING_SPEC['data_type']}\")\n",
    "print(f\"  Recursion Steps: {PREPROCESSING_SPEC['recursion_steps']}\")\n",
    "print(f\"  State Dim:       {PREPROCESSING_SPEC['state_dim']}\")\n",
    "print(f\"  Hash Dim:        {PREPROCESSING_SPEC['hash_dim']}\")\n",
    "\n",
    "# Demonstrate preprocessing step-by-step\n",
    "print(\"\\nüìù STEP-BY-STEP PREPROCESSING (for Go implementation):\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "test_img = Image.open(test_images[0]).convert(\"RGB\")\n",
    "print(f\"1. Load image as RGB: {test_img.size} ‚Üí {test_img.mode}\")\n",
    "\n",
    "# Resize\n",
    "resized = test_img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.BICUBIC)\n",
    "print(f\"2. Resize to {IMAGE_SIZE}√ó{IMAGE_SIZE} using BICUBIC interpolation\")\n",
    "\n",
    "# To tensor (0-1 range)\n",
    "import numpy as np\n",
    "arr = np.array(resized).astype(np.float32) / 255.0\n",
    "print(f\"3. Convert to float32 and scale to [0, 1]: shape={arr.shape}, range=[{arr.min():.3f}, {arr.max():.3f}]\")\n",
    "\n",
    "# Normalize\n",
    "for c, (m, s) in enumerate(zip(IMAGENET_MEAN, IMAGENET_STD)):\n",
    "    arr[:, :, c] = (arr[:, :, c] - m) / s\n",
    "print(f\"4. Normalize with ImageNet mean/std: range=[{arr.min():.3f}, {arr.max():.3f}]\")\n",
    "\n",
    "# Transpose to NCHW\n",
    "arr = arr.transpose(2, 0, 1)  # HWC -> CHW\n",
    "arr = arr[np.newaxis, ...]   # Add batch dimension\n",
    "print(f\"5. Transpose HWC‚ÜíCHW and add batch: shape={arr.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Go implementation must produce identical tensor!\")\n",
    "print(\"\\n‚ö†Ô∏è  CRITICAL: Use imaging.Lanczos in Go (closest to BICUBIC)\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# Save spec to JSON for Go reference\n",
    "import json\n",
    "spec_path = Path(\"/content/drive/MyDrive/dazzled/outputs/preprocessing_spec.json\")\n",
    "spec_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(spec_path, \"w\") as f:\n",
    "    json.dump(PREPROCESSING_SPEC, f, indent=2)\n",
    "print(f\"\\nüíæ Saved specification to: {spec_path}\")\n",
    "\n",
    "milestone3_passed = True  # Manual check - specification documented\n",
    "print(f\"\\n{'MILESTONE 3: PASSED ‚úì' if milestone3_passed else 'MILESTONE 3: FAILED ‚úó'} (specification documented)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47abbdf",
   "metadata": {},
   "source": [
    "## Milestone 4: ONNX Parity Check\n",
    "\n",
    "The final and most critical test: verify that the exported ONNX model produces\n",
    "**identical outputs** to the PyTorch model.\n",
    "\n",
    "**Test:** Run the same image through PyTorch and ONNX, compare outputs.\n",
    "**Pass Criteria:** Maximum absolute difference < 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd223f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MILESTONE 4: ONNX PARITY CHECK\n",
    "# =============================================================================\n",
    "# Verify PyTorch and ONNX outputs are identical (to ~1e-5 precision).\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"MILESTONE 4: ONNX PARITY CHECK\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# Check if ONNX model exists\n",
    "if not ONNX_PATH.exists():\n",
    "    print(f\"‚ö†Ô∏è  ONNX model not found at {ONNX_PATH}\")\n",
    "    print(\"   Run the ONNX export cell first!\")\n",
    "    milestone4_passed = False\n",
    "else:\n",
    "    # Load ONNX model\n",
    "    print(f\"Loading ONNX model: {ONNX_PATH.name}\")\n",
    "    onnx_model = onnx.load(str(ONNX_PATH))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"‚úì ONNX model validation passed\")\n",
    "    \n",
    "    # Create ONNX runtime session\n",
    "    session = ort.InferenceSession(str(ONNX_PATH))\n",
    "    \n",
    "    # Test multiple images\n",
    "    parity_results = []\n",
    "    \n",
    "    for img_path in test_images[:5]:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0)\n",
    "        \n",
    "        # PyTorch inference\n",
    "        with torch.no_grad():\n",
    "            state_pt = torch.zeros(1, STATE_DIM, device=device)\n",
    "            for _ in range(RECURSION_STEPS):\n",
    "                state_pt, hash_pt = student(x.to(device), state_pt)\n",
    "            pytorch_output = hash_pt.cpu().numpy()\n",
    "        \n",
    "        # ONNX inference\n",
    "        x_np = x.numpy()\n",
    "        state_np = np.zeros((1, STATE_DIM), dtype=np.float32)\n",
    "        \n",
    "        for _ in range(RECURSION_STEPS):\n",
    "            onnx_outputs = session.run(None, {\n",
    "                \"image\": x_np,\n",
    "                \"prev_state\": state_np\n",
    "            })\n",
    "            state_np = onnx_outputs[0]\n",
    "        onnx_output = onnx_outputs[1]\n",
    "        \n",
    "        # Compare\n",
    "        max_diff = np.abs(pytorch_output - onnx_output).max()\n",
    "        mean_diff = np.abs(pytorch_output - onnx_output).mean()\n",
    "        \n",
    "        status = \"‚úì PASS\" if max_diff < 1e-4 else \"‚úó FAIL\"\n",
    "        parity_results.append((img_path.name[:30], max_diff, mean_diff, max_diff < 1e-4))\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'Image':<32} {'Max Diff':>12} {'Mean Diff':>12} {'Status':>8}\")\n",
    "    print(\"-\"*65)\n",
    "    for name, max_d, mean_d, passed in parity_results:\n",
    "        status = \"‚úì PASS\" if passed else \"‚úó FAIL\"\n",
    "        print(f\"{name:<32} {max_d:>12.2e} {mean_d:>12.2e} {status:>8}\")\n",
    "    \n",
    "    milestone4_passed = all(r[3] for r in parity_results)\n",
    "    print(\"-\"*65)\n",
    "    print(f\"\\n{'MILESTONE 4: PASSED ‚úì' if milestone4_passed else 'MILESTONE 4: FAILED ‚úó'} (threshold: max_diff < 1e-4)\")\n",
    "\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3410b5e",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ GO / NO-GO Decision\n",
    "\n",
    "Final checkpoint aggregating all milestone results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a46c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GO / NO-GO DECISION\n",
    "# =============================================================================\n",
    "# Final checkpoint: Are we ready to lock in the ONNX model?\n",
    "\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"           MODEL VALIDATION SUMMARY - GO/NO-GO DECISION\")\n",
    "print(\"=\"*65 + \"\\n\")\n",
    "\n",
    "milestones = [\n",
    "    (\"Milestone 1\", \"Recursive Drift Test\", \"milestone1_passed\" in dir() and milestone1_passed),\n",
    "    (\"Milestone 2\", \"Teacher Alignment\", \"milestone2_passed\" in dir() and milestone2_passed),\n",
    "    (\"Milestone 3\", \"Preprocessing Spec\", \"milestone3_passed\" in dir() and milestone3_passed),\n",
    "    (\"Milestone 4\", \"ONNX Parity Check\", \"milestone4_passed\" in dir() and milestone4_passed),\n",
    "]\n",
    "\n",
    "print(f\"{'Milestone':<14} {'Test':<25} {'Status':>10}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "all_passed = True\n",
    "for name, desc, passed in milestones:\n",
    "    status = \"‚úì PASSED\" if passed else \"‚úó FAILED\"\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "    print(f\"{name:<14} {desc:<25} {status:>10}\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "print()\n",
    "\n",
    "if all_passed:\n",
    "    print(\"‚ïî\" + \"‚ïê\"*63 + \"‚ïó\")\n",
    "    print(\"‚ïë\" + \" \"*20 + \"üöÄ GO FOR ONNX EXPORT üöÄ\" + \" \"*18 + \"‚ïë\")\n",
    "    print(\"‚ï†\" + \"‚ïê\"*63 + \"‚ï£\")\n",
    "    print(\"‚ïë  All milestones passed! You are ready to:                    ‚ïë\")\n",
    "    print(\"‚ïë                                                               ‚ïë\")\n",
    "    print(\"‚ïë  1. Download ONNX from Drive:                                 ‚ïë\")\n",
    "    print(\"‚ïë     /content/drive/MyDrive/dazzled/outputs/models/            ‚ïë\")\n",
    "    print(\"‚ïë                                                               ‚ïë\")\n",
    "    print(\"‚ïë  2. Copy to Go project:                                       ‚ïë\")\n",
    "    print(\"‚ïë     recursive_hasher.onnx ‚Üí bin/                              ‚ïë\")\n",
    "    print(\"‚ïë                                                               ‚ïë\")\n",
    "    print(\"‚ïë  3. Update internal/bridge/onnx_runtime.go                    ‚ïë\")\n",
    "    print(\"‚ïö\" + \"‚ïê\"*63 + \"‚ïù\")\n",
    "else:\n",
    "    print(\"‚ïî\" + \"‚ïê\"*63 + \"‚ïó\")\n",
    "    print(\"‚ïë\" + \" \"*22 + \"‚õî NO-GO - FIX ISSUES ‚õî\" + \" \"*17 + \"‚ïë\")\n",
    "    print(\"‚ï†\" + \"‚ïê\"*63 + \"‚ï£\")\n",
    "    print(\"‚ïë  Some milestones failed. Address the issues above before      ‚ïë\")\n",
    "    print(\"‚ïë  proceeding with ONNX export.                                 ‚ïë\")\n",
    "    print(\"‚ïö\" + \"‚ïê\"*63 + \"‚ïù\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*65)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
