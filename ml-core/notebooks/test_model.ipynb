{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6732215f",
      "metadata": {
        "id": "6732215f"
      },
      "source": [
        "# ü¶ñ DaZZLeD: Recursive Hasher Training Notebook\n",
        "\n",
        "**Goal:** Train a Tiny Recursive Model (TRM) using DINOv3 distillation for adversarially-robust perceptual hashing.\n",
        "\n",
        "## How to Use This Notebook\n",
        "\n",
        "1. **Run Cell 1** - Mount Google Drive\n",
        "2. **Run Cell 2** - Download training datasets (one-time, ~30 min)\n",
        "3. **Run Cell 3** - Clone repo & install dependencies\n",
        "4. **Run Cell 4** - Build manifest & train the model\n",
        "5. **Run remaining cells** - Test and export the trained model\n",
        "\n",
        "‚ö†Ô∏è **Before training:** Runtime ‚Üí Change runtime type ‚Üí **GPU (T4 free or A100)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ccd8ce8",
      "metadata": {
        "id": "7ccd8ce8"
      },
      "source": [
        "## 0. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5ef02f76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ef02f76",
        "outputId": "316d357e-8921-48c7-f0c9-7300d0e00965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úì Project root: /content/drive/MyDrive/dazzled\n",
            "‚úì Data root: /content/drive/MyDrive/dazzled/data\n",
            "‚úì Output root: /content/drive/MyDrive/dazzled/outputs\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (required for data storage)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directories\n",
        "from pathlib import Path\n",
        "\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
        "DATA_ROOT = DRIVE_ROOT / \"data\"\n",
        "OUTPUT_ROOT = DRIVE_ROOT / \"outputs\"\n",
        "\n",
        "# Create all needed directories\n",
        "for d in [DATA_ROOT / \"ffhq\", DATA_ROOT / \"openimages\", DATA_ROOT / \"text\",\n",
        "          OUTPUT_ROOT / \"checkpoints\", OUTPUT_ROOT / \"models\",\n",
        "          DRIVE_ROOT / \"manifests\"]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úì Project root: {DRIVE_ROOT}\")\n",
        "print(f\"‚úì Data root: {DATA_ROOT}\")\n",
        "print(f\"‚úì Output root: {OUTPUT_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfaee7b4",
      "metadata": {
        "id": "dfaee7b4"
      },
      "source": [
        "## 0.1 Download Training Datasets (Optimized for Speed)\n",
        "\n",
        "**Strategy:** Use local disk (`/content/data`) for training speed, then cache to Drive as a zip.\n",
        "\n",
        "- **First run:** Downloads data ‚Üí processes ‚Üí creates zip backup on Drive (~30 min)\n",
        "- **Future runs:** Extracts from Drive zip ‚Üí ready in ~2 min\n",
        "\n",
        "‚ö†Ô∏è Accessing individual files from Drive during training is **100x slower** than local disk!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13dfac76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13dfac76",
        "outputId": "cfd039fa-27c2-4793-f7b0-edf5e418a912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=================================================================\n",
            "üöÄ DOWNLOADING DATASETS\n",
            "=================================================================\n",
            "\n",
            "‚úì [1/3] FFHQ already exists (52,001 images). Skipping download.\n",
            "\n",
            "üì• [2/3] OpenImages via FiftyOne\n",
            "   Method: Official Google download (handles AWS S3 shards)\n",
            "   Target: 15k diverse real-world images\n",
            "   Downloading from AWS S3...\n",
            "Downloading split 'validation' to '/root/fiftyone/open-images-v7/validation' if necessary\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/open-images-v7/validation' if necessary\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 15000 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 15000 images\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [4.9m elapsed, 0s remaining, 50.6 files/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [4.9m elapsed, 0s remaining, 50.6 files/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset info written to '/root/fiftyone/open-images-v7/info.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/open-images-v7/info.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 'open-images-v7' split 'validation'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'open-images-v7' split 'validation'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  94% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-| 14173/15000 [11.4m elapsed, 40.2s remaining, 20.6 samples/s]   "
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# üì¶ DOWNLOAD & PREPARE DATASETS\n",
        "# =============================================================================\n",
        "# This cell downloads and prepares the training data.\n",
        "# Total: ~45k images from 3 sources:\n",
        "#   - FFHQ: Kaggle (face images for identity)\n",
        "#   - OpenImages: FiftyOne (diverse real-world objects)\n",
        "#   - MobileViews: HuggingFace parquet\n",
        "#\n",
        "# WHY THIS MIX?\n",
        "#   - FFHQ 40k: Faces require precise hashing (identity preservation)\n",
        "#   - OpenImages 2.5k: Broad category coverage (animals, vehicles, food)\n",
        "#   - MobileViews 2k: Edge cases for text/UI (600k available, 2k is enough)\n",
        "# =============================================================================\n",
        "\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Config\n",
        "DATA_ROOT = Path(\"/content/data\")\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
        "DRIVE_ARCHIVE = DRIVE_ROOT / \"data-cache/training-images.zip\"\n",
        "\n",
        "EXPECTED_COUNTS = {\n",
        "    \"ffhq\": 40000,          # Full dataset\n",
        "    \"openimages\": 2500,     # Subset\n",
        "    \"mobileviews\": 1500,    # Target: 2k\n",
        "}\n",
        "\n",
        "def validate_dataset(data_root: Path, expected: dict) -> tuple[bool, dict]:\n",
        "    \"\"\"Validate all datasets have enough images.\"\"\"\n",
        "    results = {}\n",
        "    for name, exp_count in expected.items():\n",
        "        path = data_root / name\n",
        "        actual = len(list(path.glob(\"*.jpg\"))) if path.exists() else 0\n",
        "        results[name] = {\"count\": actual, \"expected\": exp_count, \"valid\": actual >= exp_count * 0.95}\n",
        "    return all(r[\"valid\"] for r in results.values()), results\n",
        "\n",
        "# Check what we already have\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Option 1: Restore from Drive cache (fastest)\n",
        "if DRIVE_ARCHIVE.exists():\n",
        "    print(\"üîÑ Restoring from Drive cache...\")\n",
        "    shutil.unpack_archive(DRIVE_ARCHIVE, DATA_ROOT)\n",
        "    all_valid, validation = validate_dataset(DATA_ROOT, EXPECTED_COUNTS)\n",
        "    if all_valid:\n",
        "        print(\"‚úì All datasets restored from cache!\")\n",
        "        for name, info in validation.items():\n",
        "            print(f\"  {name}: {info['count']:,} images\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Cache incomplete, will download missing data\")\n",
        "        need_download = {name: not info[\"valid\"] for name, info in validation.items()}\n",
        "else:\n",
        "    print(\"üì• No cache found, downloading all datasets...\")\n",
        "    need_download = {\"ffhq\": True, \"openimages\": True, \"mobileviews\": True}\n",
        "\n",
        "# Option 2: Download fresh data\n",
        "if any(need_download.values()):\n",
        "    print(\"\\n\" + \"=\"*65)\n",
        "    print(\"DOWNLOADING DATASETS\")\n",
        "    print(\"=\"*65)\n",
        "\n",
        "    import torchvision.transforms as transforms\n",
        "    from PIL import Image\n",
        "    import io\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 1. FFHQ via Kaggle\n",
        "    # -------------------------------------------------------------------------\n",
        "    if need_download[\"ffhq\"]:\n",
        "        ffhq_dir = DATA_ROOT / \"ffhq\"\n",
        "        ffhq_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(\"\\nüì• [1/3] FFHQ via Kaggle\")\n",
        "        print(\"   Target: 40k high-quality face images\")\n",
        "\n",
        "        # Setup Kaggle credentials from Colab secrets\n",
        "        from google.colab import userdata\n",
        "        import os\n",
        "\n",
        "        try:\n",
        "            os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "            os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Kaggle credentials not found in Colab Secrets!\")\n",
        "            print(\"   To set up:\")\n",
        "            print(\"   1. Go to kaggle.com ‚Üí Your Profile ‚Üí Account ‚Üí API ‚Üí Create New Token\")\n",
        "            print(\"   2. Add to Colab Secrets (üîë icon on left sidebar):\")\n",
        "            print(\"      - KAGGLE_USERNAME = your_username\")\n",
        "            print(\"      - KAGGLE_KEY = your_api_key\")\n",
        "            raise RuntimeError(\"Kaggle credentials required\") from e\n",
        "\n",
        "        print(\"   Downloading from Kaggle...\")\n",
        "        !kaggle datasets download -d denislukovnikov/ffhq256-images-only -p /content/ffhq_temp --unzip -q\n",
        "\n",
        "        # Move and verify\n",
        "        for src in Path(\"/content/ffhq_temp\").rglob(\"*.png\"):\n",
        "            src.rename(ffhq_dir / src.name)\n",
        "        shutil.rmtree(\"/content/ffhq_temp\", ignore_errors=True)\n",
        "\n",
        "        # Convert to jpg for consistency\n",
        "        print(\"   Converting to JPEG...\")\n",
        "        from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "        def convert_to_jpg(png_path):\n",
        "            jpg_path = png_path.with_suffix('.jpg')\n",
        "            try:\n",
        "                Image.open(png_path).convert('RGB').save(jpg_path, 'JPEG', quality=95)\n",
        "                png_path.unlink()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        pngs = list(ffhq_dir.glob(\"*.png\"))\n",
        "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "            list(executor.map(convert_to_jpg, pngs))\n",
        "\n",
        "        print(f\"   ‚úì FFHQ: {len(list(ffhq_dir.glob('*.jpg'))):,} images\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 2. OpenImages via FiftyOne\n",
        "    # -------------------------------------------------------------------------\n",
        "    if need_download[\"openimages\"]:\n",
        "        openimages_dir = DATA_ROOT / \"openimages\"\n",
        "        openimages_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(\"\\nüì• [2/3] OpenImages via FiftyOne\")\n",
        "        print(\"   Target: 2.5k diverse real-world images\")\n",
        "\n",
        "        !pip install -q fiftyone\n",
        "\n",
        "        import fiftyone as fo\n",
        "        import fiftyone.zoo as foz\n",
        "\n",
        "        dataset = foz.load_zoo_dataset(\n",
        "            \"open-images-v7\",\n",
        "            split=\"train\",\n",
        "            max_samples=2500,\n",
        "            shuffle=True,\n",
        "            seed=42,\n",
        "            dataset_name=\"openimages_train\"\n",
        "        )\n",
        "\n",
        "        resize_tfm = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)\n",
        "        ])\n",
        "\n",
        "        for idx, sample in enumerate(dataset):\n",
        "            try:\n",
        "                img = Image.open(sample.filepath).convert(\"RGB\")\n",
        "                resize_tfm(img).save(openimages_dir / f\"openimg_{idx:05d}.jpg\", quality=90)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        fo.delete_dataset(dataset.name)\n",
        "        print(f\"   ‚úì OpenImages: {len(list(openimages_dir.glob('*.jpg'))):,} images\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 3. MobileViews (parquet - MEMORY-EFFICIENT STREAMING)\n",
        "    # -------------------------------------------------------------------------\n",
        "    if need_download[\"mobileviews\"]:\n",
        "        mobileviews_dir = DATA_ROOT / \"mobileviews\"\n",
        "        mobileviews_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(\"\\nüì• [3/3] MobileViews via HuggingFace Parquet\")\n",
        "        print(\"   Target: 2k mobile UI screenshots (edge cases for text/UI)\")\n",
        "\n",
        "        !pip install -q huggingface_hub pyarrow\n",
        "\n",
        "        from huggingface_hub import hf_hub_download, login\n",
        "        from google.colab import userdata\n",
        "        import pyarrow.parquet as pq\n",
        "\n",
        "        try:\n",
        "            hf_token = userdata.get('HF_TOKEN')\n",
        "            if hf_token:\n",
        "                login(hf_token)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(\"   Downloading parquet...\")\n",
        "        parquet_path = hf_hub_download(\n",
        "            repo_id=\"mllmTeam/MobileViews\",\n",
        "            filename=\"MobileViews_Screenshots_ViewHierarchies/Parquets/MobileViews_0-150000.parquet\",\n",
        "            repo_type=\"dataset\",\n",
        "            local_dir=\"/content/mobileviews_cache\"\n",
        "        )\n",
        "\n",
        "        print(\"   Extracting screenshots (streaming to avoid OOM)...\")\n",
        "        \n",
        "        # Use ParquetFile for memory-efficient streaming instead of read_table()\n",
        "        # read_table() loads ALL 150k images into RAM at once (~50GB+)\n",
        "        # iter_batches() streams row-by-row, using only ~100MB RAM\n",
        "        parquet_file = pq.ParquetFile(parquet_path)\n",
        "        total_rows = parquet_file.metadata.num_rows\n",
        "        \n",
        "        mv_resize = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)\n",
        "        ])\n",
        "\n",
        "        from tqdm import tqdm\n",
        "        num_samples = 2000\n",
        "        step = max(1, total_rows // num_samples)\n",
        "        \n",
        "        # Stream through the file, extracting every Nth row\n",
        "        saved_count = 0\n",
        "        current_row = 0\n",
        "        \n",
        "        for batch in tqdm(parquet_file.iter_batches(batch_size=1000, columns=[\"image_content\"]),\n",
        "                          desc=\"   Streaming\", total=(total_rows // 1000) + 1):\n",
        "            for i in range(len(batch)):\n",
        "                if current_row % step == 0 and saved_count < num_samples:\n",
        "                    try:\n",
        "                        img_bytes = batch.column(\"image_content\")[i].as_py()\n",
        "                        img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
        "                        mv_resize(img).save(mobileviews_dir / f\"mobileview_{saved_count:05d}.jpg\", quality=90)\n",
        "                        saved_count += 1\n",
        "                    except:\n",
        "                        pass\n",
        "                current_row += 1\n",
        "                \n",
        "            # Early exit once we have enough samples\n",
        "            if saved_count >= num_samples:\n",
        "                break\n",
        "\n",
        "        shutil.rmtree(\"/content/mobileviews_cache\", ignore_errors=True)\n",
        "        print(f\"   ‚úì MobileViews: {len(list(mobileviews_dir.glob('*.jpg'))):,} images\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # SAVE BACKUP\n",
        "    # -------------------------------------------------------------------------\n",
        "    all_valid, validation = validate_dataset(DATA_ROOT, EXPECTED_COUNTS)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*65)\n",
        "    print(\"üìã VALIDATION\")\n",
        "    print(\"=\"*65)\n",
        "    for name, info in validation.items():\n",
        "        status = \"‚úì\" if info[\"valid\"] else \"‚úó\"\n",
        "        print(f\"{status} {name:<15} {info['count']:>6,} / {info['expected']:,}\")\n",
        "\n",
        "    if all_valid:\n",
        "        print(f\"\\nüíæ Creating backup: {DRIVE_ARCHIVE}\")\n",
        "        DRIVE_ARCHIVE.parent.mkdir(parents=True, exist_ok=True)\n",
        "        if DRIVE_ARCHIVE.exists():\n",
        "            DRIVE_ARCHIVE.unlink()\n",
        "        shutil.make_archive(str(DRIVE_ARCHIVE.with_suffix('')), 'zip', DATA_ROOT)\n",
        "        archive_size = DRIVE_ARCHIVE.stat().st_size / (1024**3)\n",
        "        print(f\"‚úì Backup complete! ({archive_size:.2f} GB)\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Some datasets incomplete - not saving cache.\")\n",
        "\n",
        "# =============================================================================\n",
        "# SUMMARY\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"üìä DATASET SUMMARY\")\n",
        "print(\"=\"*65)\n",
        "print(f\"{'Dataset':<20} {'Role':<25} {'Count':>10}\")\n",
        "print(\"-\"*65)\n",
        "for subdir, role in [(\"ffhq\", \"People (faces)\"),\n",
        "                      (\"openimages\", \"Life (real-world)\"),\n",
        "                      (\"mobileviews\", \"Edge Case (mobile UI)\")]:\n",
        "    path = DATA_ROOT / subdir\n",
        "    count = len(list(path.glob(\"*.jpg\"))) if path.exists() else 0\n",
        "    print(f\"{subdir:<20} {role:<25} {count:>10,}\")\n",
        "total = sum(1 for _ in DATA_ROOT.rglob(\"*.jpg\"))\n",
        "print(\"-\"*65)\n",
        "print(f\"{'TOTAL':<20} {'':<25} {total:>10,}\")\n",
        "print(\"=\"*65)\n",
        "print(f\"\\nüìÅ Data: {DATA_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4c8053",
      "metadata": {
        "id": "1c4c8053"
      },
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ade7e62",
      "metadata": {
        "id": "9ade7e62"
      },
      "outputs": [],
      "source": [
        "# Clone the repo (only needed in Colab)\n",
        "import os\n",
        "if not os.path.exists('DaZZLeD'):\n",
        "    !git clone https://github.com/D13ya/DaZZLeD.git\n",
        "    %cd DaZZLeD/ml-core\n",
        "else:\n",
        "    %cd DaZZLeD/ml-core\n",
        "\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99f363c2",
      "metadata": {
        "id": "99f363c2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "from models.recursive_student import RecursiveHasher\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b3d9a99",
      "metadata": {
        "id": "4b3d9a99"
      },
      "source": [
        "## 1.1 Build Manifest & Train Model\n",
        "\n",
        "‚ö†Ô∏è **Switch to GPU first:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or A100)\n",
        "\n",
        "This cell:\n",
        "1. Builds a manifest of all training images\n",
        "2. Trains the RecursiveHasher using DINOv3 distillation\n",
        "3. Saves checkpoints to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e674d4df",
      "metadata": {
        "id": "e674d4df"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BUILD MANIFEST FROM LOCAL DATA\n",
        "# =============================================================================\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Point to local fast storage\n",
        "DATA_ROOT = Path(\"/content/data\")\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
        "\n",
        "# Find all training images\n",
        "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "paths = [str(p) for p in DATA_ROOT.rglob(\"*\") if p.suffix.lower() in exts]\n",
        "\n",
        "print(f\"Found {len(paths):,} training images in {DATA_ROOT}\")\n",
        "\n",
        "if len(paths) < 100:\n",
        "    print(\"‚ö†Ô∏è  Not enough images! Run the download cell first.\")\n",
        "else:\n",
        "    # Write manifest to Drive so it persists, but content points to /content/data\n",
        "    manifest_path = DRIVE_ROOT / \"manifests/train.txt\"\n",
        "    manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(manifest_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(paths))\n",
        "\n",
        "    print(f\"‚úì Manifest written: {manifest_path}\")\n",
        "    print(f\"  (Points to {len(paths)} local files for high-speed training)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7291d049",
      "metadata": {
        "id": "7291d049"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN THE MODEL (Choose one option)\n",
        "# =============================================================================\n",
        "\n",
        "# üîß TRAINING OPTIONS - Uncomment ONE of the following:\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# OPTION A: Quick Test (5-10 min on T4) - Just to verify everything works\n",
        "# -----------------------------------------------------------------------------\n",
        "# !python training/train.py \\\n",
        "#     --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
        "#     --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
        "#     --epochs 1 \\\n",
        "#     --batch-size 16 \\\n",
        "#     --recursion-steps 8 \\\n",
        "#     --max-steps 100 \\\n",
        "#     --amp \\\n",
        "#     --log-interval 10 \\\n",
        "#     --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# OPTION B: Full Training (High RAM Optimized)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Optimized for 50GB+ RAM:\n",
        "# --cache-ram: Preloads all images into RAM (fastest IO, uses ~10GB RAM)\n",
        "# --workers 8: Maximizes CPU usage for augmentation\n",
        "# -----------------------------------------------------------------------------\n",
        "!python training/train.py \\\n",
        "    --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
        "    --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
        "    --epochs 5 \\\n",
        "    --batch-size 64 \\\n",
        "    --recursion-steps 16 \\\n",
        "    --grad-accum 2 \\\n",
        "    --lr 1e-4 \\\n",
        "    --amp \\\n",
        "    --allow-tf32 \\\n",
        "    --channels-last \\\n",
        "    --cudnn-benchmark \\\n",
        "    --workers 8 \\\n",
        "    --cache-ram \\\n",
        "    --log-interval 50 \\\n",
        "    --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints \\\n",
        "    --checkpoint-every 500\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# OPTION C: Resume from Checkpoint (if session disconnected)\n",
        "# -----------------------------------------------------------------------------\n",
        "# !python training/train.py \\\n",
        "#     --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
        "#     --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
        "#     --resume /content/drive/MyDrive/dazzled/outputs/checkpoints/student_epoch_3.safetensors \\\n",
        "#     --epochs 5 \\\n",
        "#     --batch-size 64 \\\n",
        "#     --recursion-steps 16 \\\n",
        "#     --amp \\\n",
        "#     --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5102d066",
      "metadata": {
        "id": "5102d066"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LIST CHECKPOINTS & LOAD TRAINED WEIGHTS\n",
        "# =============================================================================\n",
        "\n",
        "from pathlib import Path\n",
        "import safetensors.torch\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
        "\n",
        "# List available checkpoints\n",
        "checkpoints = sorted(CKPT_DIR.glob(\"*.safetensors\"))\n",
        "print(f\"Found {len(checkpoints)} checkpoints:\")\n",
        "for ckpt in checkpoints:\n",
        "    size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
        "    print(f\"  {ckpt.name} ({size_mb:.2f} MB)\")\n",
        "\n",
        "# Load the latest checkpoint into model\n",
        "if checkpoints:\n",
        "    latest_ckpt = checkpoints[-1]\n",
        "    print(f\"\\nüì• Loading: {latest_ckpt.name}\")\n",
        "\n",
        "    # Model should already be defined from earlier cell\n",
        "    # If not, uncomment:\n",
        "    # from models.recursive_student import RecursiveHasher\n",
        "    # model = RecursiveHasher(state_dim=128, hash_dim=96)\n",
        "\n",
        "    safetensors.torch.load_model(model, str(latest_ckpt))\n",
        "    model.eval()\n",
        "    print(\"‚úì Trained weights loaded!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No checkpoints found. Run training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acbba1d4",
      "metadata": {
        "id": "acbba1d4"
      },
      "source": [
        "## 2. Model Architecture Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f3c710",
      "metadata": {
        "id": "30f3c710"
      },
      "outputs": [],
      "source": [
        "# Initialize model with default params\n",
        "STATE_DIM = 128\n",
        "HASH_DIM = 96\n",
        "RECURSION_STEPS = 16\n",
        "\n",
        "model = RecursiveHasher(state_dim=STATE_DIM, hash_dim=HASH_DIM)\n",
        "model.eval()\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f6d21e",
      "metadata": {
        "id": "49f6d21e"
      },
      "outputs": [],
      "source": [
        "# Test forward pass with dummy input\n",
        "batch_size = 4\n",
        "image_size = 224\n",
        "\n",
        "dummy_img = torch.randn(batch_size, 3, image_size, image_size)\n",
        "dummy_state = torch.zeros(batch_size, STATE_DIM)\n",
        "\n",
        "with torch.no_grad():\n",
        "    next_state, hash_out = model(dummy_img, dummy_state)\n",
        "\n",
        "print(f\"Input image shape: {dummy_img.shape}\")\n",
        "print(f\"Input state shape: {dummy_state.shape}\")\n",
        "print(f\"Output state shape: {next_state.shape}\")\n",
        "print(f\"Output hash shape: {hash_out.shape}\")\n",
        "print(f\"Hash L2 norm (should be ~1.0): {torch.norm(hash_out, dim=1).mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871b0ff3",
      "metadata": {
        "id": "871b0ff3"
      },
      "source": [
        "## 3. Recursive Inference Test\n",
        "\n",
        "The key innovation is running the model recursively 16 times, refining the hash at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc68871",
      "metadata": {
        "id": "dbc68871"
      },
      "outputs": [],
      "source": [
        "def recursive_inference(model, image, steps=16):\n",
        "    \"\"\"Run recursive inference for the specified number of steps.\"\"\"\n",
        "    batch_size = image.size(0)\n",
        "    state = torch.zeros(batch_size, STATE_DIM, device=image.device)\n",
        "\n",
        "    hashes = []\n",
        "    with torch.no_grad():\n",
        "        for step in range(steps):\n",
        "            state, hash_out = model(image, state)\n",
        "            hashes.append(hash_out.clone())\n",
        "\n",
        "    return hashes\n",
        "\n",
        "# Run recursive inference\n",
        "hashes = recursive_inference(model, dummy_img, steps=RECURSION_STEPS)\n",
        "\n",
        "print(f\"Generated {len(hashes)} hash vectors\")\n",
        "print(f\"Final hash shape: {hashes[-1].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7063c3",
      "metadata": {
        "id": "7a7063c3"
      },
      "outputs": [],
      "source": [
        "# Analyze hash stability across recursive steps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute cosine similarity between consecutive steps\n",
        "similarities = []\n",
        "for i in range(1, len(hashes)):\n",
        "    sim = F.cosine_similarity(hashes[i], hashes[i-1], dim=1).mean().item()\n",
        "    similarities.append(sim)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(hashes)), similarities, 'b-o')\n",
        "plt.xlabel('Recursion Step')\n",
        "plt.ylabel('Cosine Similarity with Previous')\n",
        "plt.title('Hash Convergence Over Recursion')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.grid(True)\n",
        "\n",
        "# Compute similarity to final hash\n",
        "final_hash = hashes[-1]\n",
        "similarities_to_final = []\n",
        "for h in hashes:\n",
        "    sim = F.cosine_similarity(h, final_hash, dim=1).mean().item()\n",
        "    similarities_to_final.append(sim)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(hashes)), similarities_to_final, 'r-o')\n",
        "plt.xlabel('Recursion Step')\n",
        "plt.ylabel('Cosine Similarity to Final Hash')\n",
        "plt.title('Convergence to Final Hash')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f87e5f8",
      "metadata": {
        "id": "3f87e5f8"
      },
      "source": [
        "## 4. Adversarial Robustness Test\n",
        "\n",
        "Test if small perturbations to input cause large changes in hash (they shouldn't after recursive refinement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa469be7",
      "metadata": {
        "id": "aa469be7"
      },
      "outputs": [],
      "source": [
        "def test_perturbation_robustness(model, image, epsilon_range, steps=16):\n",
        "    \"\"\"Test hash stability under input perturbations.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Get baseline hash\n",
        "    baseline_hashes = recursive_inference(model, image, steps)\n",
        "    baseline_final = baseline_hashes[-1]\n",
        "\n",
        "    for epsilon in epsilon_range:\n",
        "        # Add random noise\n",
        "        noise = torch.randn_like(image) * epsilon\n",
        "        perturbed = image + noise\n",
        "\n",
        "        # Get perturbed hash\n",
        "        perturbed_hashes = recursive_inference(model, perturbed, steps)\n",
        "        perturbed_final = perturbed_hashes[-1]\n",
        "\n",
        "        # Compute similarity\n",
        "        sim = F.cosine_similarity(baseline_final, perturbed_final, dim=1).mean().item()\n",
        "        results.append((epsilon, sim))\n",
        "        print(f\"Epsilon={epsilon:.4f}: Cosine Similarity={sim:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test with various noise levels\n",
        "epsilons = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
        "test_img = torch.randn(1, 3, 224, 224)\n",
        "results = test_perturbation_robustness(model, test_img, epsilons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7c2f8d",
      "metadata": {
        "id": "1c7c2f8d"
      },
      "outputs": [],
      "source": [
        "# Plot robustness results\n",
        "epsilons, sims = zip(*results)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epsilons, sims, 'g-o', linewidth=2, markersize=8)\n",
        "plt.axhline(y=0.9, color='r', linestyle='--', label='0.9 threshold')\n",
        "plt.xlabel('Noise Level (epsilon)', fontsize=12)\n",
        "plt.ylabel('Cosine Similarity to Original', fontsize=12)\n",
        "plt.title('Hash Robustness to Input Perturbations', fontsize=14)\n",
        "plt.xscale('log')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e33729",
      "metadata": {
        "id": "36e33729"
      },
      "source": [
        "## 5. ONNX Export Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe78324",
      "metadata": {
        "id": "dbe78324"
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# Export to ONNX\n",
        "onnx_path = \"test_model.onnx\"\n",
        "\n",
        "model.eval()\n",
        "dummy_img = torch.randn(1, 3, 224, 224)\n",
        "dummy_state = torch.zeros(1, STATE_DIM)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    (dummy_img, dummy_state),\n",
        "    onnx_path,\n",
        "    input_names=[\"image\", \"prev_state\"],\n",
        "    output_names=[\"next_state\", \"hash\"],\n",
        "    opset_version=14,\n",
        "    dynamic_axes={\n",
        "        \"image\": {0: \"batch\"},\n",
        "        \"prev_state\": {0: \"batch\"},\n",
        "        \"next_state\": {0: \"batch\"},\n",
        "        \"hash\": {0: \"batch\"},\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"Exported ONNX model to {onnx_path}\")\n",
        "print(f\"File size: {os.path.getsize(onnx_path) / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe910628",
      "metadata": {
        "id": "fe910628"
      },
      "outputs": [],
      "source": [
        "# Validate ONNX model\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"ONNX model validation passed!\")\n",
        "\n",
        "# Test ONNX runtime inference\n",
        "session = ort.InferenceSession(onnx_path)\n",
        "\n",
        "# Run inference\n",
        "test_img = np.random.default_rng(42).standard_normal((1, 3, 224, 224)).astype(np.float32)\n",
        "test_state = np.zeros((1, STATE_DIM), dtype=np.float32)\n",
        "\n",
        "outputs = session.run(None, {\"image\": test_img, \"prev_state\": test_state})\n",
        "next_state_onnx, hash_onnx = outputs\n",
        "\n",
        "print(f\"ONNX output state shape: {next_state_onnx.shape}\")\n",
        "print(f\"ONNX output hash shape: {hash_onnx.shape}\")\n",
        "print(f\"ONNX hash L2 norm: {np.linalg.norm(hash_onnx, axis=1).mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa9ac143",
      "metadata": {
        "id": "fa9ac143"
      },
      "outputs": [],
      "source": [
        "# Compare PyTorch vs ONNX outputs\n",
        "with torch.no_grad():\n",
        "    pt_state, pt_hash = model(torch.from_numpy(test_img), torch.from_numpy(test_state))\n",
        "\n",
        "pt_hash_np = pt_hash.numpy()\n",
        "pt_state_np = pt_state.numpy()\n",
        "\n",
        "hash_diff = np.abs(pt_hash_np - hash_onnx).max()\n",
        "state_diff = np.abs(pt_state_np - next_state_onnx).max()\n",
        "\n",
        "print(f\"Max hash difference (PyTorch vs ONNX): {hash_diff:.8f}\")\n",
        "print(f\"Max state difference (PyTorch vs ONNX): {state_diff:.8f}\")\n",
        "\n",
        "if hash_diff < 1e-5 and state_diff < 1e-5:\n",
        "    print(\"‚úÖ ONNX export matches PyTorch output!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Warning: Numerical differences detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8934775c",
      "metadata": {
        "id": "8934775c"
      },
      "source": [
        "## 6. Latency Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48f27eef",
      "metadata": {
        "id": "48f27eef"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def benchmark_inference(model, device, num_runs=100, warmup=10):\n",
        "    \"\"\"Benchmark recursive inference latency.\"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_img = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        _ = recursive_inference(model, test_img, steps=16)\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Benchmark\n",
        "    times = []\n",
        "    for _ in range(num_runs):\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        start = time.perf_counter()\n",
        "\n",
        "        _ = recursive_inference(model, test_img, steps=16)\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        times.append(time.perf_counter() - start)\n",
        "\n",
        "    return times\n",
        "\n",
        "# Benchmark on CPU\n",
        "cpu_times = benchmark_inference(model, torch.device('cpu'), num_runs=50)\n",
        "print(f\"CPU Latency: {np.mean(cpu_times)*1000:.2f} ¬± {np.std(cpu_times)*1000:.2f} ms\")\n",
        "\n",
        "# Benchmark on GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    gpu_times = benchmark_inference(model, torch.device('cuda'), num_runs=100)\n",
        "    print(f\"GPU Latency: {np.mean(gpu_times)*1000:.2f} ¬± {np.std(gpu_times)*1000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd5027b0",
      "metadata": {
        "id": "bd5027b0"
      },
      "source": [
        "## 8. Summary\n",
        "\n",
        "### ‚úÖ This notebook provides a complete workflow:\n",
        "\n",
        "| Step | Cell | Description |\n",
        "|------|------|-------------|\n",
        "| 0 | Mount Drive | Connect Google Drive for data/checkpoints |\n",
        "| 0.1 | Download Data | Download FFHQ, COCO, text images directly to Drive |\n",
        "| 1 | Setup | Clone repo & install dependencies |\n",
        "| 1.1 | Train | Build manifest & train with DINOv3 distillation |\n",
        "| 2-3 | Architecture | Verify model structure & forward pass |\n",
        "| 4 | Recursion | Test recursive inference convergence |\n",
        "| 5 | Robustness | Perturbation stability testing |\n",
        "| 6 | ONNX | Export & validate ONNX model |\n",
        "| 7 | Save | Copy ONNX to Drive for Go runtime |\n",
        "\n",
        "### üì¶ Output artifacts (on Google Drive):\n",
        "\n",
        "```\n",
        "/content/drive/MyDrive/dazzled/outputs/\n",
        "‚îú‚îÄ‚îÄ checkpoints/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ student_epoch_5.safetensors   # Trained weights\n",
        "‚îî‚îÄ‚îÄ models/\n",
        "    ‚îî‚îÄ‚îÄ recursive_hasher.onnx          # ONNX for Go runtime\n",
        "```\n",
        "\n",
        "### üîó References:\n",
        "- **DINOv3:** [arXiv:2508.10104](https://arxiv.org/abs/2508.10104)\n",
        "- **TRM:** [arXiv:2510.04871](https://arxiv.org/abs/2510.04871)\n",
        "- **Split Accumulation:** [ePrint 2020/1618](https://eprint.iacr.org/2020/1618)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027a5805",
      "metadata": {
        "id": "027a5805"
      },
      "source": [
        "## 7. Export ONNX to Google Drive\n",
        "\n",
        "Save the trained model to Drive for use in the Go application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74cb7fc8",
      "metadata": {
        "id": "74cb7fc8"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXPORT TRAINED ONNX MODEL TO GOOGLE DRIVE\n",
        "# =============================================================================\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Source: the ONNX file exported earlier in this notebook\n",
        "source_onnx = Path(\"test_model.onnx\")\n",
        "\n",
        "# Destination on Drive\n",
        "dest_dir = Path(\"/content/drive/MyDrive/dazzled/outputs/models\")\n",
        "dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "dest_onnx = dest_dir / \"recursive_hasher.onnx\"\n",
        "\n",
        "if source_onnx.exists():\n",
        "    shutil.copy(source_onnx, dest_onnx)\n",
        "    size_mb = dest_onnx.stat().st_size / (1024 * 1024)\n",
        "    print(f\"‚úì ONNX model saved to Drive: {dest_onnx}\")\n",
        "    print(f\"  Size: {size_mb:.2f} MB\")\n",
        "\n",
        "    # Also save the latest checkpoint\n",
        "    ckpt_dir = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
        "    checkpoints = sorted(ckpt_dir.glob(\"*.safetensors\"))\n",
        "    if checkpoints:\n",
        "        print(\"\\nüì¶ Available artifacts on Drive:\")\n",
        "        print(f\"  Model: {dest_onnx}\")\n",
        "        print(f\"  Checkpoint: {checkpoints[-1]}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  ONNX file not found. Run the ONNX export cell first (Cell 23-25).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b9c2763",
      "metadata": {},
      "source": [
        "# üéØ MODEL VALIDATION MILESTONES\n",
        "\n",
        "Before exporting to ONNX and implementing in Go, the model must pass **all four milestones**.\n",
        "These tests ensure the recursive student is stable, accurate, and portable.\n",
        "\n",
        "| Milestone | Test | Success Criteria |\n",
        "|-----------|------|------------------|\n",
        "| 1 | Recursive Drift | Emb‚ÇÅ ‚âà Emb‚ÇÖ (cosine sim > 0.99) |\n",
        "| 2 | Validation Loss | Plateaued for 3-5 epochs |\n",
        "| 3 | Preprocessing Parity | Go/Python use identical transforms |\n",
        "| 4 | ONNX Parity | PyTorch vs ONNX diff < 1e-5 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d458ef",
      "metadata": {},
      "source": [
        "## Milestone 1: Recursive Drift Test (Stability)\n",
        "\n",
        "A recursive model feeds its own output back into itself. If the model is unstable,\n",
        "errors compound and the embedding \"drifts\" into garbage after 2-3 passes.\n",
        "\n",
        "**Test:** Run an image through the student 5 times recursively.\n",
        "**Pass Criteria:** Distance between Emb‚ÇÅ and Emb‚ÇÖ should be near-zero (cosine similarity > 0.99)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6896df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MILESTONE 1: RECURSIVE DRIFT TEST\n",
        "# =============================================================================\n",
        "# Test that the recursive student produces stable embeddings across multiple passes.\n",
        "# If the model is well-trained, feeding the output back should not cause drift.\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# Load trained student model\n",
        "CHECKPOINT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
        "ONNX_PATH = Path(\"/content/drive/MyDrive/dazzled/outputs/student.onnx\")\n",
        "\n",
        "# Find latest checkpoint\n",
        "checkpoints = sorted(CHECKPOINT_DIR.glob(\"*.safetensors\"))\n",
        "if not checkpoints:\n",
        "    raise FileNotFoundError(f\"No checkpoints found in {CHECKPOINT_DIR}\")\n",
        "\n",
        "latest_ckpt = checkpoints[-1]\n",
        "print(f\"Loading checkpoint: {latest_ckpt.name}\")\n",
        "\n",
        "# Load model\n",
        "import safetensors.torch\n",
        "import sys\n",
        "sys.path.append(\"/content\")\n",
        "from models.recursive_student import RecursiveHasher\n",
        "\n",
        "STATE_DIM = 128\n",
        "HASH_DIM = 96\n",
        "IMAGE_SIZE = 224\n",
        "RECURSION_STEPS = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "student = RecursiveHasher(state_dim=STATE_DIM, hash_dim=HASH_DIM).to(device)\n",
        "safetensors.torch.load_model(student, str(latest_ckpt))\n",
        "student.eval()\n",
        "\n",
        "# Standard ImageNet normalization (MUST match Go implementation)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "# Load a test image\n",
        "DATA_ROOT = Path(\"/content/data\")\n",
        "test_images = list(DATA_ROOT.rglob(\"*.jpg\"))[:5]\n",
        "if not test_images:\n",
        "    raise FileNotFoundError(\"No test images found\")\n",
        "\n",
        "print(f\"\\n{'='*65}\")\n",
        "print(\"MILESTONE 1: RECURSIVE DRIFT TEST\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "milestone1_passed = True\n",
        "for img_path in test_images:\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    x = transform(img).unsqueeze(0).to(device)\n",
        "    \n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        state = torch.zeros(1, STATE_DIM, device=device)\n",
        "        for pass_num in range(5):\n",
        "            # Run through student\n",
        "            for _ in range(RECURSION_STEPS):\n",
        "                state, student_hash = student(x, state)\n",
        "            embeddings.append(F.normalize(student_hash, dim=1).cpu())\n",
        "    \n",
        "    # Compare first and last embeddings\n",
        "    emb1 = embeddings[0]\n",
        "    emb5 = embeddings[4]\n",
        "    cosine_sim = F.cosine_similarity(emb1, emb5).item()\n",
        "    l2_dist = torch.norm(emb1 - emb5).item()\n",
        "    \n",
        "    status = \"‚úì PASS\" if cosine_sim > 0.99 else \"‚úó FAIL\"\n",
        "    if cosine_sim <= 0.99:\n",
        "        milestone1_passed = False\n",
        "    \n",
        "    print(f\"{status} {img_path.name[:30]:<30} | Cosine(Emb1,Emb5)={cosine_sim:.6f} | L2={l2_dist:.6f}\")\n",
        "\n",
        "print(f\"\\n{'MILESTONE 1: PASSED ‚úì' if milestone1_passed else 'MILESTONE 1: FAILED ‚úó'}\")\n",
        "print(\"=\"*65)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc35473",
      "metadata": {},
      "source": [
        "## Milestone 2: Validation Loss Plateau + Visual Inspection\n",
        "\n",
        "Check that validation loss has plateaued and visually verify embeddings on \"hard\" images\n",
        "(blurry faces, text documents, edge cases) are close to teacher embeddings.\n",
        "\n",
        "**Test:** Compare student vs teacher embeddings on held-out validation set.\n",
        "**Pass Criteria:** Student-Teacher cosine similarity > 0.95 on average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b80385c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MILESTONE 2: VALIDATION LOSS & TEACHER-STUDENT ALIGNMENT\n",
        "# =============================================================================\n",
        "# Verify the student embeddings closely match the teacher (DINOv3) embeddings.\n",
        "\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Load teacher model\n",
        "print(\"Loading DINOv3 teacher model...\")\n",
        "teacher = AutoModel.from_pretrained(\n",
        "    \"facebook/dinov3-vitl16-pretrain-lvd1689m\",\n",
        "    trust_remote_code=True\n",
        ").to(device)\n",
        "teacher.eval()\n",
        "\n",
        "# Select validation images (mix of easy and hard cases)\n",
        "val_images = []\n",
        "for subdir in [\"ffhq\", \"openimages\", \"mobileviews\"]:\n",
        "    subdir_path = DATA_ROOT / subdir\n",
        "    if subdir_path.exists():\n",
        "        imgs = list(subdir_path.glob(\"*.jpg\"))[:5]\n",
        "        val_images.extend(imgs)\n",
        "\n",
        "if len(val_images) < 5:\n",
        "    val_images = test_images\n",
        "\n",
        "print(f\"\\n{'='*65}\")\n",
        "print(\"MILESTONE 2: TEACHER-STUDENT ALIGNMENT\")\n",
        "print(\"=\"*65)\n",
        "print(f\"Evaluating on {len(val_images)} validation images...\")\n",
        "\n",
        "similarities = []\n",
        "milestone2_details = []\n",
        "\n",
        "for img_path in val_images:\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    x = transform(img).unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Teacher embedding\n",
        "        teacher_out = teacher(x)\n",
        "        teacher_emb = F.normalize(teacher_out.last_hidden_state[:, 0], dim=1)\n",
        "        \n",
        "        # Student embedding\n",
        "        state = torch.zeros(1, STATE_DIM, device=device)\n",
        "        for _ in range(RECURSION_STEPS):\n",
        "            state, student_hash = student(x, state)\n",
        "        student_emb = F.normalize(student_hash, dim=1)\n",
        "        \n",
        "        # Compare\n",
        "        cosine_sim = F.cosine_similarity(teacher_emb, student_emb).item()\n",
        "        similarities.append(cosine_sim)\n",
        "        \n",
        "        status = \"‚úì\" if cosine_sim > 0.90 else \"‚ö†\" if cosine_sim > 0.80 else \"‚úó\"\n",
        "        milestone2_details.append((img_path.parent.name, img_path.name[:25], cosine_sim, status))\n",
        "\n",
        "# Print results\n",
        "print(f\"\\n{'Category':<15} {'Image':<28} {'Similarity':>12} {'Status'}\")\n",
        "print(\"-\"*65)\n",
        "for cat, name, sim, status in milestone2_details:\n",
        "    print(f\"{cat:<15} {name:<28} {sim:>12.4f} {status:>6}\")\n",
        "\n",
        "avg_sim = np.mean(similarities)\n",
        "min_sim = np.min(similarities)\n",
        "milestone2_passed = avg_sim > 0.95\n",
        "\n",
        "print(\"-\"*65)\n",
        "print(f\"{'AVERAGE':<44} {avg_sim:>12.4f}\")\n",
        "print(f\"{'MINIMUM':<44} {min_sim:>12.4f}\")\n",
        "print(f\"\\n{'MILESTONE 2: PASSED ‚úì' if milestone2_passed else 'MILESTONE 2: FAILED ‚úó'} (threshold: avg > 0.95)\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Cleanup teacher to free GPU memory\n",
        "del teacher\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3065321d",
      "metadata": {},
      "source": [
        "## Milestone 3: Preprocessing Parity Check\n",
        "\n",
        "**Critical:** The Go implementation MUST use identical preprocessing to Python.\n",
        "This cell documents and verifies the exact preprocessing pipeline.\n",
        "\n",
        "| Parameter | Value | Go Implementation |\n",
        "|-----------|-------|-------------------|\n",
        "| Image Size | 224√ó224 | `imaging.Resize(224, 224, imaging.Lanczos)` |\n",
        "| Interpolation | Bicubic | `imaging.Lanczos` (closest match) |\n",
        "| Normalization | ImageNet | mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225] |\n",
        "| Channel Order | RGB | Standard (not BGR) |\n",
        "| Data Type | float32 | `float32` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e699ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MILESTONE 3: PREPROCESSING PARITY CHECK\n",
        "# =============================================================================\n",
        "# Document and verify the exact preprocessing pipeline for Go parity.\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"MILESTONE 3: PREPROCESSING PARITY SPECIFICATION\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Lock these values - changing them requires retraining!\n",
        "PREPROCESSING_SPEC = {\n",
        "    \"image_size\": IMAGE_SIZE,\n",
        "    \"interpolation\": \"BICUBIC\",  # Go: imaging.Lanczos (closest match)\n",
        "    \"normalization\": {\n",
        "        \"mean\": IMAGENET_MEAN,\n",
        "        \"std\": IMAGENET_STD,\n",
        "    },\n",
        "    \"channel_order\": \"RGB\",  # Not BGR!\n",
        "    \"data_type\": \"float32\",\n",
        "    \"recursion_steps\": RECURSION_STEPS,\n",
        "    \"state_dim\": STATE_DIM,\n",
        "    \"hash_dim\": HASH_DIM,\n",
        "}\n",
        "\n",
        "print(\"\\nüìã LOCKED PREPROCESSING SPECIFICATION:\")\n",
        "print(\"-\"*65)\n",
        "print(f\"  Image Size:      {PREPROCESSING_SPEC['image_size']}√ó{PREPROCESSING_SPEC['image_size']}\")\n",
        "print(f\"  Interpolation:   {PREPROCESSING_SPEC['interpolation']}\")\n",
        "print(f\"  Mean:            {PREPROCESSING_SPEC['normalization']['mean']}\")\n",
        "print(f\"  Std:             {PREPROCESSING_SPEC['normalization']['std']}\")\n",
        "print(f\"  Channel Order:   {PREPROCESSING_SPEC['channel_order']}\")\n",
        "print(f\"  Data Type:       {PREPROCESSING_SPEC['data_type']}\")\n",
        "print(f\"  Recursion Steps: {PREPROCESSING_SPEC['recursion_steps']}\")\n",
        "print(f\"  State Dim:       {PREPROCESSING_SPEC['state_dim']}\")\n",
        "print(f\"  Hash Dim:        {PREPROCESSING_SPEC['hash_dim']}\")\n",
        "\n",
        "# Demonstrate preprocessing step-by-step\n",
        "print(\"\\nüìù STEP-BY-STEP PREPROCESSING (for Go implementation):\")\n",
        "print(\"-\"*65)\n",
        "\n",
        "test_img = Image.open(test_images[0]).convert(\"RGB\")\n",
        "print(f\"1. Load image as RGB: {test_img.size} ‚Üí {test_img.mode}\")\n",
        "\n",
        "# Resize\n",
        "resized = test_img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.BICUBIC)\n",
        "print(f\"2. Resize to {IMAGE_SIZE}√ó{IMAGE_SIZE} using BICUBIC interpolation\")\n",
        "\n",
        "# To tensor (0-1 range)\n",
        "import numpy as np\n",
        "arr = np.array(resized).astype(np.float32) / 255.0\n",
        "print(f\"3. Convert to float32 and scale to [0, 1]: shape={arr.shape}, range=[{arr.min():.3f}, {arr.max():.3f}]\")\n",
        "\n",
        "# Normalize\n",
        "for c, (m, s) in enumerate(zip(IMAGENET_MEAN, IMAGENET_STD)):\n",
        "    arr[:, :, c] = (arr[:, :, c] - m) / s\n",
        "print(f\"4. Normalize with ImageNet mean/std: range=[{arr.min():.3f}, {arr.max():.3f}]\")\n",
        "\n",
        "# Transpose to NCHW\n",
        "arr = arr.transpose(2, 0, 1)  # HWC -> CHW\n",
        "arr = arr[np.newaxis, ...]   # Add batch dimension\n",
        "print(f\"5. Transpose HWC‚ÜíCHW and add batch: shape={arr.shape}\")\n",
        "\n",
        "print(\"\\n‚úÖ Go implementation must produce identical tensor!\")\n",
        "print(\"\\n‚ö†Ô∏è  CRITICAL: Use imaging.Lanczos in Go (closest to BICUBIC)\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Save spec to JSON for Go reference\n",
        "import json\n",
        "spec_path = Path(\"/content/drive/MyDrive/dazzled/outputs/preprocessing_spec.json\")\n",
        "spec_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(spec_path, \"w\") as f:\n",
        "    json.dump(PREPROCESSING_SPEC, f, indent=2)\n",
        "print(f\"\\nüíæ Saved specification to: {spec_path}\")\n",
        "\n",
        "milestone3_passed = True  # Manual check - specification documented\n",
        "print(f\"\\n{'MILESTONE 3: PASSED ‚úì' if milestone3_passed else 'MILESTONE 3: FAILED ‚úó'} (specification documented)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47abbdf",
      "metadata": {},
      "source": [
        "## Milestone 4: ONNX Parity Check\n",
        "\n",
        "The final and most critical test: verify that the exported ONNX model produces\n",
        "**identical outputs** to the PyTorch model.\n",
        "\n",
        "**Test:** Run the same image through PyTorch and ONNX, compare outputs.\n",
        "**Pass Criteria:** Maximum absolute difference < 1e-5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd223f46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MILESTONE 4: ONNX PARITY CHECK\n",
        "# =============================================================================\n",
        "# Verify PyTorch and ONNX outputs are identical (to ~1e-5 precision).\n",
        "\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"MILESTONE 4: ONNX PARITY CHECK\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Check if ONNX model exists\n",
        "if not ONNX_PATH.exists():\n",
        "    print(f\"‚ö†Ô∏è  ONNX model not found at {ONNX_PATH}\")\n",
        "    print(\"   Run the ONNX export cell first!\")\n",
        "    milestone4_passed = False\n",
        "else:\n",
        "    # Load ONNX model\n",
        "    print(f\"Loading ONNX model: {ONNX_PATH.name}\")\n",
        "    onnx_model = onnx.load(str(ONNX_PATH))\n",
        "    onnx.checker.check_model(onnx_model)\n",
        "    print(\"‚úì ONNX model validation passed\")\n",
        "    \n",
        "    # Create ONNX runtime session\n",
        "    session = ort.InferenceSession(str(ONNX_PATH))\n",
        "    \n",
        "    # Test multiple images\n",
        "    parity_results = []\n",
        "    \n",
        "    for img_path in test_images[:5]:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        x = transform(img).unsqueeze(0)\n",
        "        \n",
        "        # PyTorch inference\n",
        "        with torch.no_grad():\n",
        "            state_pt = torch.zeros(1, STATE_DIM, device=device)\n",
        "            for _ in range(RECURSION_STEPS):\n",
        "                state_pt, hash_pt = student(x.to(device), state_pt)\n",
        "            pytorch_output = hash_pt.cpu().numpy()\n",
        "        \n",
        "        # ONNX inference\n",
        "        x_np = x.numpy()\n",
        "        state_np = np.zeros((1, STATE_DIM), dtype=np.float32)\n",
        "        \n",
        "        for _ in range(RECURSION_STEPS):\n",
        "            onnx_outputs = session.run(None, {\n",
        "                \"image\": x_np,\n",
        "                \"prev_state\": state_np\n",
        "            })\n",
        "            state_np = onnx_outputs[0]\n",
        "        onnx_output = onnx_outputs[1]\n",
        "        \n",
        "        # Compare\n",
        "        max_diff = np.abs(pytorch_output - onnx_output).max()\n",
        "        mean_diff = np.abs(pytorch_output - onnx_output).mean()\n",
        "        \n",
        "        status = \"‚úì PASS\" if max_diff < 1e-4 else \"‚úó FAIL\"\n",
        "        parity_results.append((img_path.name[:30], max_diff, mean_diff, max_diff < 1e-4))\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{'Image':<32} {'Max Diff':>12} {'Mean Diff':>12} {'Status':>8}\")\n",
        "    print(\"-\"*65)\n",
        "    for name, max_d, mean_d, passed in parity_results:\n",
        "        status = \"‚úì PASS\" if passed else \"‚úó FAIL\"\n",
        "        print(f\"{name:<32} {max_d:>12.2e} {mean_d:>12.2e} {status:>8}\")\n",
        "    \n",
        "    milestone4_passed = all(r[3] for r in parity_results)\n",
        "    print(\"-\"*65)\n",
        "    print(f\"\\n{'MILESTONE 4: PASSED ‚úì' if milestone4_passed else 'MILESTONE 4: FAILED ‚úó'} (threshold: max_diff < 1e-4)\")\n",
        "\n",
        "print(\"=\"*65)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3410b5e",
      "metadata": {},
      "source": [
        "---\n",
        "## üöÄ GO / NO-GO Decision\n",
        "\n",
        "Final checkpoint aggregating all milestone results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a46c43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GO / NO-GO DECISION\n",
        "# =============================================================================\n",
        "# Final checkpoint: Are we ready to lock in the ONNX model?\n",
        "\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"           MODEL VALIDATION SUMMARY - GO/NO-GO DECISION\")\n",
        "print(\"=\"*65 + \"\\n\")\n",
        "\n",
        "milestones = [\n",
        "    (\"Milestone 1\", \"Recursive Drift Test\", \"drift_passed\" in dir() and drift_passed),\n",
        "    (\"Milestone 2\", \"Teacher Alignment\", \"milestone2_passed\" in dir() and milestone2_passed),\n",
        "    (\"Milestone 3\", \"Preprocessing Spec\", \"milestone3_passed\" in dir() and milestone3_passed),\n",
        "    (\"Milestone 4\", \"ONNX Parity Check\", \"milestone4_passed\" in dir() and milestone4_passed),\n",
        "]\n",
        "\n",
        "print(f\"{'Milestone':<14} {'Test':<25} {'Status':>10}\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "all_passed = True\n",
        "for name, desc, passed in milestones:\n",
        "    status = \"‚úì PASSED\" if passed else \"‚úó FAILED\"\n",
        "    if not passed:\n",
        "        all_passed = False\n",
        "    print(f\"{name:<14} {desc:<25} {status:>10}\")\n",
        "\n",
        "print(\"-\"*50)\n",
        "print()\n",
        "\n",
        "if all_passed:\n",
        "    print(\"‚ïî\" + \"‚ïê\"*63 + \"‚ïó\")\n",
        "    print(\"‚ïë\" + \" \"*20 + \"üöÄ GO FOR ONNX EXPORT üöÄ\" + \" \"*18 + \"‚ïë\")\n",
        "    print(\"‚ï†\" + \"‚ïê\"*63 + \"‚ï£\")\n",
        "    print(\"‚ïë  All milestones passed! You are ready to:                    ‚ïë\")\n",
        "    print(\"‚ïë                                                               ‚ïë\")\n",
        "    print(\"‚ïë  1. Export final ONNX model:                                  ‚ïë\")\n",
        "    print(\"‚ïë     python ml-core/training/export_onnx.py                    ‚ïë\")\n",
        "    print(\"‚ïë                                                               ‚ïë\")\n",
        "    print(\"‚ïë  2. Copy to Go project:                                       ‚ïë\")\n",
        "    print(\"‚ïë     cp outputs/recursive_hasher.onnx bin/                     ‚ïë\")\n",
        "    print(\"‚ïë                                                               ‚ïë\")\n",
        "    print(\"‚ïë  3. Update internal/bridge/onnx_runtime.go                    ‚ïë\")\n",
        "    print(\"‚ïö\" + \"‚ïê\"*63 + \"‚ïù\")\n",
        "else:\n",
        "    print(\"‚ïî\" + \"‚ïê\"*63 + \"‚ïó\")\n",
        "    print(\"‚ïë\" + \" \"*22 + \"‚õî NO-GO - FIX ISSUES ‚õî\" + \" \"*17 + \"‚ïë\")\n",
        "    print(\"‚ï†\" + \"‚ïê\"*63 + \"‚ï£\")\n",
        "    print(\"‚ïë  Some milestones failed. Address the issues above before      ‚ïë\")\n",
        "    print(\"‚ïë  proceeding with ONNX export.                                 ‚ïë\")\n",
        "    print(\"‚ïö\" + \"‚ïê\"*63 + \"‚ïù\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*65)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
