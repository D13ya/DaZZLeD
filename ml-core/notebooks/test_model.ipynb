{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6732215f",
   "metadata": {},
   "source": [
    "# ü¶ñ DaZZLeD: Recursive Hasher Training Notebook\n",
    "\n",
    "**Goal:** Train a Tiny Recursive Model (TRM) using DINOv3 distillation for adversarially-robust perceptual hashing.\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run Cell 1** - Mount Google Drive\n",
    "2. **Run Cell 2** - Download training datasets (one-time, ~30 min)\n",
    "3. **Run Cell 3** - Clone repo & install dependencies\n",
    "4. **Run Cell 4** - Build manifest & train the model\n",
    "5. **Run remaining cells** - Test and export the trained model\n",
    "\n",
    "‚ö†Ô∏è **Before training:** Runtime ‚Üí Change runtime type ‚Üí **GPU (T4 free or A100)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd8ce8",
   "metadata": {},
   "source": [
    "## 0. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef02f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (required for data storage)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
    "DATA_ROOT = DRIVE_ROOT / \"data\"\n",
    "OUTPUT_ROOT = DRIVE_ROOT / \"outputs\"\n",
    "\n",
    "# Create all needed directories\n",
    "for d in [DATA_ROOT / \"ffhq\", DATA_ROOT / \"openimages\", DATA_ROOT / \"text\",\n",
    "          OUTPUT_ROOT / \"checkpoints\", OUTPUT_ROOT / \"models\",\n",
    "          DRIVE_ROOT / \"manifests\"]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Project root: {DRIVE_ROOT}\")\n",
    "print(f\"‚úì Data root: {DATA_ROOT}\")\n",
    "print(f\"‚úì Output root: {OUTPUT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaee7b4",
   "metadata": {},
   "source": [
    "## 0.1 Download Training Datasets (Optimized for Speed)\n",
    "\n",
    "**Strategy:** Use local disk (`/content/data`) for training speed, then cache to Drive as a zip.\n",
    "\n",
    "- **First run:** Downloads data ‚Üí processes ‚Üí creates zip backup on Drive (~30 min)\n",
    "- **Future runs:** Extracts from Drive zip ‚Üí ready in ~2 min\n",
    "\n",
    "‚ö†Ô∏è Accessing individual files from Drive during training is **100x slower** than local disk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOWNLOAD & PREPARE DATASET (Optimized for Colab I/O)\n",
    "#\n",
    "# STANDARD APPROACHES:\n",
    "#   - FFHQ: Kaggle bulk download ‚Üí ALL 70k images (more data = better model)\n",
    "#   - OpenImages: FiftyOne (official Google method)\n",
    "#   - MobileViews: HuggingFace parquet\n",
    "#\n",
    "# WHY THESE COUNTS:\n",
    "#   - FFHQ 70k: Full dataset, 1024x1024 faces ‚Üí resize to 224\n",
    "#   - OpenImages 15k: Diverse real-world (validation split has 41k, we sample 15k)\n",
    "#   - MobileViews 2k: Edge cases for text/UI (600k available, 2k is enough)\n",
    "#   - Total: ~87k images for distillation training\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import io\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# =============================================================================\n",
    "# ‚öôÔ∏è CONFIGURATION\n",
    "# =============================================================================\n",
    "FORCE_REDOWNLOAD = False  # Set True to bypass cache\n",
    "\n",
    "# Expected counts - USING FULL FFHQ (70k)\n",
    "EXPECTED_COUNTS = {\n",
    "    \"ffhq\": 60000,        # Target: 70k, allow ~85% success (some corrupt)\n",
    "    \"openimages\": 12000,  # Target: 15k\n",
    "    \"mobileviews\": 1500,  # Target: 2k\n",
    "}\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path(\"/content/data\")\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "DRIVE_ARCHIVE = Path(\"/content/drive/MyDrive/dazzled/dazzled_dataset_v5.zip\")  # v5 = full FFHQ\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATION HELPER\n",
    "# =============================================================================\n",
    "def validate_dataset(data_root, expected_counts):\n",
    "    results = {}\n",
    "    all_valid = True\n",
    "    for name, min_count in expected_counts.items():\n",
    "        path = data_root / name\n",
    "        count = len(list(path.glob(\"*.jpg\"))) + len(list(path.glob(\"*.png\"))) if path.exists() else 0\n",
    "        valid = count >= min_count\n",
    "        results[name] = {\"count\": count, \"expected\": min_count, \"valid\": valid}\n",
    "        if not valid:\n",
    "            all_valid = False\n",
    "    return all_valid, results\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK CACHE\n",
    "# =============================================================================\n",
    "need_download = {\"ffhq\": True, \"openimages\": True, \"mobileviews\": True}\n",
    "\n",
    "if FORCE_REDOWNLOAD:\n",
    "    print(\"üîÑ FORCE_REDOWNLOAD=True - Starting fresh...\")\n",
    "    shutil.rmtree(DATA_ROOT, ignore_errors=True)\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "elif DRIVE_ARCHIVE.exists():\n",
    "    print(f\"üì¶ Found cache: {DRIVE_ARCHIVE}\")\n",
    "    shutil.unpack_archive(DRIVE_ARCHIVE, DATA_ROOT)\n",
    "    all_valid, validation = validate_dataset(DATA_ROOT, EXPECTED_COUNTS)\n",
    "    print(\"\\n   Validation:\")\n",
    "    for name, info in validation.items():\n",
    "        status = \"‚úì\" if info[\"valid\"] else \"‚úó\"\n",
    "        print(f\"   {status} {name}: {info['count']:,} / {info['expected']:,}\")\n",
    "        need_download[name] = not info[\"valid\"]\n",
    "    if all_valid:\n",
    "        print(\"\\n‚úì All datasets ready!\")\n",
    "\n",
    "# =============================================================================\n",
    "# DOWNLOAD MISSING DATASETS\n",
    "# =============================================================================\n",
    "if any(need_download.values()):\n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "    print(\"üöÄ DOWNLOADING DATASETS\")\n",
    "    print(\"=\"*65)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. FFHQ via Kaggle - FULL 70k DATASET\n",
    "    #    Why Kaggle: Single 89GB zip, fast CDN, no rate limits\n",
    "    #    Why 70k: More data = better distillation. We resize to 224x224.\n",
    "    # -------------------------------------------------------------------------\n",
    "    if need_download[\"ffhq\"]:\n",
    "        ffhq_dir = DATA_ROOT / \"ffhq\"\n",
    "        ffhq_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"\\nüì• [1/3] FFHQ via Kaggle - FULL 70k DATASET\")\n",
    "        print(\"   Source: arnaud58/flickrfaceshq-dataset-ffhq (89GB ‚Üí 70k √ó 1024√ó1024)\")\n",
    "        print(\"   Output: 70k √ó 224√ó224 (~3GB after resize)\")\n",
    "        print(\"   Time estimate: ~15 min download + ~10 min resize\")\n",
    "        \n",
    "        # Setup Kaggle credentials\n",
    "        from google.colab import userdata\n",
    "        kaggle_ok = False\n",
    "        try:\n",
    "            kaggle_username = userdata.get('KAGGLE_USERNAME')\n",
    "            kaggle_key = userdata.get('KAGGLE_KEY')\n",
    "            \n",
    "            os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "            with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "                f.write(f'{{\"username\":\"{kaggle_username}\",\"key\":\"{kaggle_key}\"}}')\n",
    "            os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "            print(\"   ‚úì Kaggle credentials configured\")\n",
    "            kaggle_ok = True\n",
    "        except Exception as e:\n",
    "            print(f\"\\n   ‚ùå Kaggle credentials not found!\")\n",
    "            print(f\"   To set up:\")\n",
    "            print(f\"   1. Go to kaggle.com ‚Üí Your Profile ‚Üí Account ‚Üí API ‚Üí Create New Token\")\n",
    "            print(f\"   2. Add to Colab Secrets (üîë icon on left sidebar):\")\n",
    "            print(f\"      - KAGGLE_USERNAME = your_username\")\n",
    "            print(f\"      - KAGGLE_KEY = your_api_key\")\n",
    "        \n",
    "        if kaggle_ok:\n",
    "            print(\"\\n   Downloading from Kaggle CDN...\")\n",
    "            !pip install -q kaggle\n",
    "            !kaggle datasets download -d arnaud58/flickrfaceshq-dataset-ffhq -p /content/ffhq_download --unzip\n",
    "            \n",
    "            # Process ALL images (not sampling)\n",
    "            print(\"\\n   Resizing all images to 224√ó224...\")\n",
    "            from tqdm import tqdm\n",
    "            resize = transforms.Resize((224, 224))\n",
    "            \n",
    "            ffhq_source = Path(\"/content/ffhq_download\")\n",
    "            all_images = sorted(list(ffhq_source.rglob(\"*.png\")) + list(ffhq_source.rglob(\"*.jpg\")))\n",
    "            print(f\"   Found {len(all_images):,} images\")\n",
    "            \n",
    "            for i, img_path in enumerate(tqdm(all_images, desc=\"   Resizing\")):\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert(\"RGB\")\n",
    "                    resize(img).save(ffhq_dir / f\"ffhq_{i:05d}.jpg\", quality=90)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Cleanup (important - 89GB!)\n",
    "            print(\"   Cleaning up original 1024√ó1024 images...\")\n",
    "            shutil.rmtree(\"/content/ffhq_download\", ignore_errors=True)\n",
    "            print(f\"   ‚úì FFHQ: {len(list(ffhq_dir.glob('*.jpg'))):,} images\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. OpenImages via FiftyOne (official method, bug workaround)\n",
    "    # -------------------------------------------------------------------------\n",
    "    if need_download[\"openimages\"]:\n",
    "        openimages_dir = DATA_ROOT / \"openimages\"\n",
    "        openimages_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"\\nüì• [2/3] OpenImages via FiftyOne\")\n",
    "        print(\"   Method: Official Google download (handles AWS S3 shards)\")\n",
    "        print(\"   Target: 15k diverse real-world images\")\n",
    "        \n",
    "        !pip install -q fiftyone\n",
    "        \n",
    "        import fiftyone as fo\n",
    "        import fiftyone.zoo as foz\n",
    "        \n",
    "        print(\"   Downloading from AWS S3...\")\n",
    "        # Workaround: do NOT set dataset_dir (FiftyOne bug)\n",
    "        dataset = foz.load_zoo_dataset(\n",
    "            \"open-images-v7\",\n",
    "            split=\"validation\",\n",
    "            max_samples=15000,\n",
    "            shuffle=True,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        print(\"   Copying images to openimages_dir...\")\n",
    "        from tqdm import tqdm\n",
    "        oi_resize = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224)\n",
    "        ])\n",
    "        \n",
    "        for i, sample in enumerate(tqdm(dataset, desc=\"   Resizing\")):\n",
    "            try:\n",
    "                img = Image.open(sample.filepath).convert(\"RGB\")\n",
    "                oi_resize(img).save(openimages_dir / f\"openimages_{i:05d}.jpg\", quality=90)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        fo.delete_dataset(dataset.name)\n",
    "        print(f\"   ‚úì OpenImages: {len(list(openimages_dir.glob('*.jpg'))):,} images\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. MobileViews (parquet - already optimal)\n",
    "    # -------------------------------------------------------------------------\n",
    "    if need_download[\"mobileviews\"]:\n",
    "        mobileviews_dir = DATA_ROOT / \"mobileviews\"\n",
    "        mobileviews_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"\\nüì• [3/3] MobileViews via HuggingFace Parquet\")\n",
    "        print(\"   Target: 2k mobile UI screenshots (edge cases for text/UI)\")\n",
    "        \n",
    "        !pip install -q huggingface_hub pyarrow\n",
    "        \n",
    "        from huggingface_hub import hf_hub_download, login\n",
    "        from google.colab import userdata\n",
    "        import pyarrow.parquet as pq\n",
    "        \n",
    "        try:\n",
    "            hf_token = userdata.get('HF_TOKEN')\n",
    "            if hf_token:\n",
    "                login(hf_token)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"   Downloading parquet...\")\n",
    "        parquet_path = hf_hub_download(\n",
    "            repo_id=\"mllmTeam/MobileViews\",\n",
    "            filename=\"MobileViews_Screenshots_ViewHierarchies/Parquets/MobileViews_0-150000.parquet\",\n",
    "            repo_type=\"dataset\",\n",
    "            local_dir=\"/content/mobileviews_cache\"\n",
    "        )\n",
    "        \n",
    "        print(\"   Extracting screenshots...\")\n",
    "        table = pq.read_table(parquet_path, columns=[\"image_content\"])\n",
    "        \n",
    "        mv_resize = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224)\n",
    "        ])\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        num_samples = 2000\n",
    "        step = max(1, len(table) // num_samples)\n",
    "        \n",
    "        for idx, i in enumerate(tqdm(range(0, len(table), step), desc=\"   Processing\", total=num_samples)):\n",
    "            if idx >= num_samples:\n",
    "                break\n",
    "            try:\n",
    "                img_bytes = table.column(\"image_content\")[i].as_py()\n",
    "                img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "                mv_resize(img).save(mobileviews_dir / f\"mobileview_{idx:05d}.jpg\", quality=90)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        shutil.rmtree(\"/content/mobileviews_cache\", ignore_errors=True)\n",
    "        print(f\"   ‚úì MobileViews: {len(list(mobileviews_dir.glob('*.jpg'))):,} images\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE BACKUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    all_valid, validation = validate_dataset(DATA_ROOT, EXPECTED_COUNTS)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "    print(\"üìã VALIDATION\")\n",
    "    print(\"=\"*65)\n",
    "    for name, info in validation.items():\n",
    "        status = \"‚úì\" if info[\"valid\"] else \"‚úó\"\n",
    "        print(f\"{status} {name:<15} {info['count']:>6,} / {info['expected']:,}\")\n",
    "    \n",
    "    if all_valid:\n",
    "        print(f\"\\nüíæ Creating backup: {DRIVE_ARCHIVE}\")\n",
    "        DRIVE_ARCHIVE.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if DRIVE_ARCHIVE.exists():\n",
    "            DRIVE_ARCHIVE.unlink()\n",
    "        shutil.make_archive(str(DRIVE_ARCHIVE.with_suffix('')), 'zip', DATA_ROOT)\n",
    "        archive_size = DRIVE_ARCHIVE.stat().st_size / (1024**3)\n",
    "        print(f\"‚úì Backup complete! ({archive_size:.2f} GB)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Some datasets incomplete - not saving cache.\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"üìä DATASET SUMMARY\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'Dataset':<20} {'Role':<25} {'Count':>10}\")\n",
    "print(\"-\"*65)\n",
    "for subdir, role in [(\"ffhq\", \"People (faces)\"),\n",
    "                      (\"openimages\", \"Life (real-world)\"),\n",
    "                      (\"mobileviews\", \"Edge Case (mobile UI)\")]:\n",
    "    path = DATA_ROOT / subdir\n",
    "    count = len(list(path.glob(\"*.jpg\"))) if path.exists() else 0\n",
    "    print(f\"{subdir:<20} {role:<25} {count:>10,}\")\n",
    "total = sum(1 for _ in DATA_ROOT.rglob(\"*.jpg\"))\n",
    "print(\"-\"*65)\n",
    "print(f\"{'TOTAL':<20} {'':<25} {total:>10,}\")\n",
    "print(\"=\"*65)\n",
    "print(f\"\\nüìÅ Data: {DATA_ROOT}\")\n",
    "print(f\"üíæ Cache: {DRIVE_ARCHIVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c8053",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo (only needed in Colab)\n",
    "import os\n",
    "if not os.path.exists('DaZZLeD'):\n",
    "    !git clone https://github.com/D13ya/DaZZLeD.git\n",
    "    %cd DaZZLeD/ml-core\n",
    "else:\n",
    "    %cd DaZZLeD/ml-core\n",
    "\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f363c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from models.recursive_student import RecursiveHasher\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d9a99",
   "metadata": {},
   "source": [
    "## 1.1 Build Manifest & Train Model\n",
    "\n",
    "‚ö†Ô∏è **Switch to GPU first:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or A100)\n",
    "\n",
    "This cell:\n",
    "1. Builds a manifest of all training images\n",
    "2. Trains the RecursiveHasher using DINOv3 distillation\n",
    "3. Saves checkpoints to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILD MANIFEST FROM LOCAL DATA\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Point to local fast storage\n",
    "DATA_ROOT = Path(\"/content/data\")\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
    "\n",
    "# Find all training images\n",
    "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "paths = [str(p) for p in DATA_ROOT.rglob(\"*\") if p.suffix.lower() in exts]\n",
    "\n",
    "print(f\"Found {len(paths):,} training images in {DATA_ROOT}\")\n",
    "\n",
    "if len(paths) < 100:\n",
    "    print(\"‚ö†Ô∏è  Not enough images! Run the download cell first.\")\n",
    "else:\n",
    "    # Write manifest to Drive so it persists, but content points to /content/data\n",
    "    manifest_path = DRIVE_ROOT / \"manifests/train.txt\"\n",
    "    manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(manifest_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(paths))\n",
    "\n",
    "    print(f\"‚úì Manifest written: {manifest_path}\")\n",
    "    print(f\"  (Points to {len(paths)} local files for high-speed training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN THE MODEL (Choose one option)\n",
    "# =============================================================================\n",
    "\n",
    "# üîß TRAINING OPTIONS - Uncomment ONE of the following:\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTION A: Quick Test (5-10 min on T4) - Just to verify everything works\n",
    "# -----------------------------------------------------------------------------\n",
    "# !python training/train.py \\\n",
    "#     --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
    "#     --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
    "#     --epochs 1 \\\n",
    "#     --batch-size 16 \\\n",
    "#     --recursion-steps 8 \\\n",
    "#     --max-steps 100 \\\n",
    "#     --amp \\\n",
    "#     --log-interval 10 \\\n",
    "#     --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTION B: Full Training (High RAM Optimized)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Optimized for 50GB+ RAM:\n",
    "# --cache-ram: Preloads all images into RAM (fastest IO, uses ~10GB RAM)\n",
    "# --workers 8: Maximizes CPU usage for augmentation\n",
    "# -----------------------------------------------------------------------------\n",
    "!python training/train.py \\\n",
    "    --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
    "    --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
    "    --epochs 5 \\\n",
    "    --batch-size 64 \\\n",
    "    --recursion-steps 16 \\\n",
    "    --grad-accum 2 \\\n",
    "    --lr 1e-4 \\\n",
    "    --amp \\\n",
    "    --allow-tf32 \\\n",
    "    --channels-last \\\n",
    "    --cudnn-benchmark \\\n",
    "    --workers 8 \\\n",
    "    --cache-ram \\\n",
    "    --log-interval 50 \\\n",
    "    --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints \\\n",
    "    --checkpoint-every 500\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OPTION C: Resume from Checkpoint (if session disconnected)\n",
    "# -----------------------------------------------------------------------------\n",
    "# !python training/train.py \\\n",
    "#     --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
    "#     --teacher facebook/dinov3-vitl16-pretrain-lvd1689m \\\n",
    "#     --resume /content/drive/MyDrive/dazzled/outputs/checkpoints/student_epoch_3.safetensors \\\n",
    "#     --epochs 5 \\\n",
    "#     --batch-size 64 \\\n",
    "#     --recursion-steps 16 \\\n",
    "#     --amp \\\n",
    "#     --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIST CHECKPOINTS & LOAD TRAINED WEIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import safetensors.torch\n",
    "\n",
    "CKPT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
    "\n",
    "# List available checkpoints\n",
    "checkpoints = sorted(CKPT_DIR.glob(\"*.safetensors\"))\n",
    "print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "for ckpt in checkpoints:\n",
    "    size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {ckpt.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# Load the latest checkpoint into model\n",
    "if checkpoints:\n",
    "    latest_ckpt = checkpoints[-1]\n",
    "    print(f\"\\nüì• Loading: {latest_ckpt.name}\")\n",
    "    \n",
    "    # Model should already be defined from earlier cell\n",
    "    # If not, uncomment:\n",
    "    # from models.recursive_student import RecursiveHasher\n",
    "    # model = RecursiveHasher(state_dim=128, hash_dim=96)\n",
    "    \n",
    "    safetensors.torch.load_model(model, str(latest_ckpt))\n",
    "    model.eval()\n",
    "    print(\"‚úì Trained weights loaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No checkpoints found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbba1d4",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with default params\n",
    "STATE_DIM = 128\n",
    "HASH_DIM = 96\n",
    "RECURSION_STEPS = 16\n",
    "\n",
    "model = RecursiveHasher(state_dim=STATE_DIM, hash_dim=HASH_DIM)\n",
    "model.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with dummy input\n",
    "batch_size = 4\n",
    "image_size = 224\n",
    "\n",
    "dummy_img = torch.randn(batch_size, 3, image_size, image_size)\n",
    "dummy_state = torch.zeros(batch_size, STATE_DIM)\n",
    "\n",
    "with torch.no_grad():\n",
    "    next_state, hash_out = model(dummy_img, dummy_state)\n",
    "\n",
    "print(f\"Input image shape: {dummy_img.shape}\")\n",
    "print(f\"Input state shape: {dummy_state.shape}\")\n",
    "print(f\"Output state shape: {next_state.shape}\")\n",
    "print(f\"Output hash shape: {hash_out.shape}\")\n",
    "print(f\"Hash L2 norm (should be ~1.0): {torch.norm(hash_out, dim=1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b0ff3",
   "metadata": {},
   "source": [
    "## 3. Recursive Inference Test\n",
    "\n",
    "The key innovation is running the model recursively 16 times, refining the hash at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc68871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_inference(model, image, steps=16):\n",
    "    \"\"\"Run recursive inference for the specified number of steps.\"\"\"\n",
    "    batch_size = image.size(0)\n",
    "    state = torch.zeros(batch_size, STATE_DIM, device=image.device)\n",
    "    \n",
    "    hashes = []\n",
    "    with torch.no_grad():\n",
    "        for step in range(steps):\n",
    "            state, hash_out = model(image, state)\n",
    "            hashes.append(hash_out.clone())\n",
    "    \n",
    "    return hashes\n",
    "\n",
    "# Run recursive inference\n",
    "hashes = recursive_inference(model, dummy_img, steps=RECURSION_STEPS)\n",
    "\n",
    "print(f\"Generated {len(hashes)} hash vectors\")\n",
    "print(f\"Final hash shape: {hashes[-1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7063c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hash stability across recursive steps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute cosine similarity between consecutive steps\n",
    "similarities = []\n",
    "for i in range(1, len(hashes)):\n",
    "    sim = F.cosine_similarity(hashes[i], hashes[i-1], dim=1).mean().item()\n",
    "    similarities.append(sim)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(hashes)), similarities, 'b-o')\n",
    "plt.xlabel('Recursion Step')\n",
    "plt.ylabel('Cosine Similarity with Previous')\n",
    "plt.title('Hash Convergence Over Recursion')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True)\n",
    "\n",
    "# Compute similarity to final hash\n",
    "final_hash = hashes[-1]\n",
    "similarities_to_final = []\n",
    "for h in hashes:\n",
    "    sim = F.cosine_similarity(h, final_hash, dim=1).mean().item()\n",
    "    similarities_to_final.append(sim)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(hashes)), similarities_to_final, 'r-o')\n",
    "plt.xlabel('Recursion Step')\n",
    "plt.ylabel('Cosine Similarity to Final Hash')\n",
    "plt.title('Convergence to Final Hash')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87e5f8",
   "metadata": {},
   "source": [
    "## 4. Adversarial Robustness Test\n",
    "\n",
    "Test if small perturbations to input cause large changes in hash (they shouldn't after recursive refinement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa469be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perturbation_robustness(model, image, epsilon_range, steps=16):\n",
    "    \"\"\"Test hash stability under input perturbations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get baseline hash\n",
    "    baseline_hashes = recursive_inference(model, image, steps)\n",
    "    baseline_final = baseline_hashes[-1]\n",
    "    \n",
    "    for epsilon in epsilon_range:\n",
    "        # Add random noise\n",
    "        noise = torch.randn_like(image) * epsilon\n",
    "        perturbed = image + noise\n",
    "        \n",
    "        # Get perturbed hash\n",
    "        perturbed_hashes = recursive_inference(model, perturbed, steps)\n",
    "        perturbed_final = perturbed_hashes[-1]\n",
    "        \n",
    "        # Compute similarity\n",
    "        sim = F.cosine_similarity(baseline_final, perturbed_final, dim=1).mean().item()\n",
    "        results.append((epsilon, sim))\n",
    "        print(f\"Epsilon={epsilon:.4f}: Cosine Similarity={sim:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with various noise levels\n",
    "epsilons = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "test_img = torch.randn(1, 3, 224, 224)\n",
    "results = test_perturbation_robustness(model, test_img, epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot robustness results\n",
    "epsilons, sims = zip(*results)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epsilons, sims, 'g-o', linewidth=2, markersize=8)\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='0.9 threshold')\n",
    "plt.xlabel('Noise Level (epsilon)', fontsize=12)\n",
    "plt.ylabel('Cosine Similarity to Original', fontsize=12)\n",
    "plt.title('Hash Robustness to Input Perturbations', fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e33729",
   "metadata": {},
   "source": [
    "## 5. ONNX Export Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe78324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"test_model.onnx\"\n",
    "\n",
    "model.eval()\n",
    "dummy_img = torch.randn(1, 3, 224, 224)\n",
    "dummy_state = torch.zeros(1, STATE_DIM)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_img, dummy_state),\n",
    "    onnx_path,\n",
    "    input_names=[\"image\", \"prev_state\"],\n",
    "    output_names=[\"next_state\", \"hash\"],\n",
    "    opset_version=14,\n",
    "    dynamic_axes={\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"prev_state\": {0: \"batch\"},\n",
    "        \"next_state\": {0: \"batch\"},\n",
    "        \"hash\": {0: \"batch\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Exported ONNX model to {onnx_path}\")\n",
    "print(f\"File size: {os.path.getsize(onnx_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe910628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate ONNX model\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model validation passed!\")\n",
    "\n",
    "# Test ONNX runtime inference\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Run inference\n",
    "test_img = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
    "test_state = np.zeros((1, STATE_DIM), dtype=np.float32)\n",
    "\n",
    "outputs = session.run(None, {\"image\": test_img, \"prev_state\": test_state})\n",
    "next_state_onnx, hash_onnx = outputs\n",
    "\n",
    "print(f\"ONNX output state shape: {next_state_onnx.shape}\")\n",
    "print(f\"ONNX output hash shape: {hash_onnx.shape}\")\n",
    "print(f\"ONNX hash L2 norm: {np.linalg.norm(hash_onnx, axis=1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ac143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PyTorch vs ONNX outputs\n",
    "with torch.no_grad():\n",
    "    pt_state, pt_hash = model(torch.from_numpy(test_img), torch.from_numpy(test_state))\n",
    "\n",
    "pt_hash_np = pt_hash.numpy()\n",
    "pt_state_np = pt_state.numpy()\n",
    "\n",
    "hash_diff = np.abs(pt_hash_np - hash_onnx).max()\n",
    "state_diff = np.abs(pt_state_np - next_state_onnx).max()\n",
    "\n",
    "print(f\"Max hash difference (PyTorch vs ONNX): {hash_diff:.8f}\")\n",
    "print(f\"Max state difference (PyTorch vs ONNX): {state_diff:.8f}\")\n",
    "\n",
    "if hash_diff < 1e-5 and state_diff < 1e-5:\n",
    "    print(\"‚úÖ ONNX export matches PyTorch output!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Numerical differences detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934775c",
   "metadata": {},
   "source": [
    "## 6. Latency Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f27eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_inference(model, device, num_runs=100, warmup=10):\n",
    "    \"\"\"Benchmark recursive inference latency.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_img = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = recursive_inference(model, test_img, steps=16)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        _ = recursive_inference(model, test_img, steps=16)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Benchmark on CPU\n",
    "cpu_times = benchmark_inference(model, torch.device('cpu'), num_runs=50)\n",
    "print(f\"CPU Latency: {np.mean(cpu_times)*1000:.2f} ¬± {np.std(cpu_times)*1000:.2f} ms\")\n",
    "\n",
    "# Benchmark on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    gpu_times = benchmark_inference(model, torch.device('cuda'), num_runs=100)\n",
    "    print(f\"GPU Latency: {np.mean(gpu_times)*1000:.2f} ¬± {np.std(gpu_times)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5027b0",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### ‚úÖ This notebook provides a complete workflow:\n",
    "\n",
    "| Step | Cell | Description |\n",
    "|------|------|-------------|\n",
    "| 0 | Mount Drive | Connect Google Drive for data/checkpoints |\n",
    "| 0.1 | Download Data | Download FFHQ, COCO, text images directly to Drive |\n",
    "| 1 | Setup | Clone repo & install dependencies |\n",
    "| 1.1 | Train | Build manifest & train with DINOv3 distillation |\n",
    "| 2-3 | Architecture | Verify model structure & forward pass |\n",
    "| 4 | Recursion | Test recursive inference convergence |\n",
    "| 5 | Robustness | Perturbation stability testing |\n",
    "| 6 | ONNX | Export & validate ONNX model |\n",
    "| 7 | Save | Copy ONNX to Drive for Go runtime |\n",
    "\n",
    "### üì¶ Output artifacts (on Google Drive):\n",
    "\n",
    "```\n",
    "/content/drive/MyDrive/dazzled/outputs/\n",
    "‚îú‚îÄ‚îÄ checkpoints/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ student_epoch_5.safetensors   # Trained weights\n",
    "‚îî‚îÄ‚îÄ models/\n",
    "    ‚îî‚îÄ‚îÄ recursive_hasher.onnx          # ONNX for Go runtime\n",
    "```\n",
    "\n",
    "### üîó References:\n",
    "- **DINOv3:** [arXiv:2508.10104](https://arxiv.org/abs/2508.10104)\n",
    "- **TRM:** [arXiv:2510.04871](https://arxiv.org/abs/2510.04871)\n",
    "- **Split Accumulation:** [ePrint 2020/1618](https://eprint.iacr.org/2020/1618)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a5805",
   "metadata": {},
   "source": [
    "## 7. Export ONNX to Google Drive\n",
    "\n",
    "Save the trained model to Drive for use in the Go application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT TRAINED ONNX MODEL TO GOOGLE DRIVE\n",
    "# =============================================================================\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Source: the ONNX file exported earlier in this notebook\n",
    "source_onnx = Path(\"test_model.onnx\")\n",
    "\n",
    "# Destination on Drive\n",
    "dest_dir = Path(\"/content/drive/MyDrive/dazzled/outputs/models\")\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "dest_onnx = dest_dir / \"recursive_hasher.onnx\"\n",
    "\n",
    "if source_onnx.exists():\n",
    "    shutil.copy(source_onnx, dest_onnx)\n",
    "    size_mb = dest_onnx.stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úì ONNX model saved to Drive: {dest_onnx}\")\n",
    "    print(f\"  Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Also save the latest checkpoint\n",
    "    ckpt_dir = Path(\"/content/drive/MyDrive/dazzled/outputs/checkpoints\")\n",
    "    checkpoints = sorted(ckpt_dir.glob(\"*.safetensors\"))\n",
    "    if checkpoints:\n",
    "        print(f\"\\nüì¶ Available artifacts on Drive:\")\n",
    "        print(f\"  Model: {dest_onnx}\")\n",
    "        print(f\"  Checkpoint: {checkpoints[-1]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ONNX file not found. Run the ONNX export cell first (Cell 23-25).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
