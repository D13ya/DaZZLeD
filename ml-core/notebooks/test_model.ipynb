{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DaZZLeD Hash Center Training Notebook (ResNet + Counterfactual VAE)\n",
        "\n",
        "**Goal:** Train the ResNet Hash Center model from `resnet.tex` with counterfactual VAE, CF\u2011SimCLR, DHD, PGD, and TTC checks.\n",
        "\n",
        "**Runtime:** Set Colab to GPU before running training cells.\n",
        "\n",
        "**Note:** If you do not have VAE weights yet, you must train them first (Step 3). If you want a quick run without VAE, set `--counterfactual-mode aug` in Step 4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Mount Google Drive (required for data storage)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directories\n",
        "from pathlib import Path\n",
        "\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
        "DATA_ROOT = DRIVE_ROOT / \"data\"\n",
        "OUTPUT_ROOT = DRIVE_ROOT / \"outputs\"\n",
        "\n",
        "# Create all needed directories\n",
        "for d in [\n",
        "    DATA_ROOT / \"ffhq\",\n",
        "    DATA_ROOT / \"openimages\",\n",
        "    DATA_ROOT / \"text\",\n",
        "    OUTPUT_ROOT / \"checkpoints\",\n",
        "    OUTPUT_ROOT / \"models\",\n",
        "    DRIVE_ROOT / \"manifests\",\n",
        "]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"OK: Project root: {DRIVE_ROOT}\")\n",
        "print(f\"OK: Data root: {DATA_ROOT}\")\n",
        "print(f\"OK: Output root: {OUTPUT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Credentials check (Colab secrets)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    required = [\"KAGGLE_USERNAME\", \"KAGGLE_KEY\", \"HF_TOKEN\"]\n",
        "    for key in required:\n",
        "        val = userdata.get(key)\n",
        "        print(f\"{key}: {'OK' if val else 'MISSING'}\")\n",
        "except Exception as e:\n",
        "    print(\"Credentials check skipped:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists('DaZZLeD'):\n",
        "    !git clone https://github.com/D13ya/DaZZLeD.git\n",
        "    %cd DaZZLeD/ml-core\n",
        "else:\n",
        "    %cd DaZZLeD/ml-core\n",
        "\n",
        "!pip install -q -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Restore Dataset (Drive Zip)\n",
        "\n",
        "If you keep a dataset zip on Drive, extract it once to local disk (`/content/data`) for faster I/O.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# DOWNLOAD & PREPARE DATASETS\n",
        "# FFHQ (Kaggle), OpenImages (FiftyOne), MobileViews (HF)\n",
        "# Restores from Drive cache if available; otherwise downloads and builds cache.\n",
        "\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path(\"/content/data\")\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
        "DRIVE_ARCHIVE = DRIVE_ROOT / \"data-cache/training-images.zip\"\n",
        "\n",
        "# CHECK: Drive mounted\n",
        "if not Path(\"/content/drive/MyDrive\").exists():\n",
        "    raise RuntimeError(\n",
        "        \"Google Drive is NOT mounted. Run the mount cell first, then re-run this cell.\"\n",
        "    )\n",
        "\n",
        "EXPECTED_COUNTS = {\n",
        "    \"ffhq\": 40000,\n",
        "    \"openimages\": 2500,\n",
        "    \"mobileviews\": 2000,\n",
        "}\n",
        "\n",
        "def validate_dataset(data_root: Path, expected: dict):\n",
        "    results = {}\n",
        "    for name, exp_count in expected.items():\n",
        "        path = data_root / name\n",
        "        actual = len(list(path.glob(\"*.jpg\"))) if path.exists() else 0\n",
        "        results[name] = {\"count\": actual, \"expected\": exp_count, \"valid\": actual >= exp_count * 0.95}\n",
        "    return all(r[\"valid\"] for r in results.values()), results\n",
        "\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "need_download = {\"ffhq\": False, \"openimages\": False, \"mobileviews\": False}\n",
        "skip_downloads = False\n",
        "\n",
        "# Option 1: Restore from Drive cache\n",
        "if DRIVE_ARCHIVE.exists():\n",
        "    print(\"Found cached dataset on Drive:\", DRIVE_ARCHIVE)\n",
        "    shutil.unpack_archive(DRIVE_ARCHIVE, DATA_ROOT)\n",
        "    all_valid, validation = validate_dataset(DATA_ROOT, EXPECTED_COUNTS)\n",
        "    if all_valid:\n",
        "        print(\"All datasets restored from cache successfully.\")\n",
        "        for name, info in validation.items():\n",
        "            print(f\"  OK {name}: {info['count']:,} images\")\n",
        "        print(\"Skipping downloads - data is ready.\")\n",
        "        skip_downloads = True\n",
        "    else:\n",
        "        print(\"Cache incomplete; will download missing data:\")\n",
        "        for name, info in validation.items():\n",
        "            if not info[\"valid\"]:\n",
        "                need_download[name] = True\n",
        "                print(f\"  MISSING {name}: {info['count']:,}/{info['expected']:,}\")\n",
        "            else:\n",
        "                print(f\"  OK {name}: {info['count']:,} images\")\n",
        "else:\n",
        "    print(\"No cache found on Drive; downloading datasets.\")\n",
        "    need_download = {\"ffhq\": True, \"openimages\": True, \"mobileviews\": True}\n",
        "\n",
        "# Option 2: Download fresh data\n",
        "if not skip_downloads and any(need_download.values()):\n",
        "    print(\"\")\n",
        "    print(\"=\" * 65)\n",
        "    print(\"DOWNLOADING DATASETS\")\n",
        "    print(\"=\" * 65)\n",
        "\n",
        "    import torchvision.transforms as transforms\n",
        "    from PIL import Image\n",
        "    import io\n",
        "\n",
        "    # 1) FFHQ via Kaggle\n",
        "    if need_download[\"ffhq\"]:\n",
        "        ffhq_dir = DATA_ROOT / \"ffhq\"\n",
        "        ffhq_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"[1/3] FFHQ via Kaggle\")\n",
        "        print(\"Target: 40k face images\")\n",
        "\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            import os\n",
        "            os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "            os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "            print(\"Kaggle credentials loaded from Colab secrets\")\n",
        "        except Exception as e:\n",
        "            print(\"Could not load Kaggle secrets:\", e)\n",
        "            print(\"Add KAGGLE_USERNAME and KAGGLE_KEY to Colab secrets\")\n",
        "\n",
        "        subprocess.run([\n",
        "            \"kaggle\", \"datasets\", \"download\", \"-d\", \"arnaud58/flickrfaceshq-dataset-ffhq\",\n",
        "            \"-p\", str(ffhq_dir), \"--unzip\"\n",
        "        ], check=True)\n",
        "\n",
        "        # Flatten directory structure\n",
        "        for nested in ffhq_dir.rglob(\"*\"):\n",
        "            if nested.is_file() and nested.suffix.lower() in {\".jpg\", \".png\"}:\n",
        "                target = ffhq_dir / nested.name\n",
        "                if not target.exists():\n",
        "                    shutil.move(str(nested), str(target))\n",
        "\n",
        "        for d in ffhq_dir.iterdir():\n",
        "            if d.is_dir():\n",
        "                shutil.rmtree(d)\n",
        "\n",
        "        count = len(list(ffhq_dir.glob(\"*.jpg\"))) + len(list(ffhq_dir.glob(\"*.png\")))\n",
        "        print(f\"FFHQ: {count:,} images\")\n",
        "\n",
        "    # 2) OpenImages via FiftyOne\n",
        "    if need_download[\"openimages\"]:\n",
        "        oi_dir = DATA_ROOT / \"openimages\"\n",
        "        oi_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"[2/3] OpenImages via FiftyOne\")\n",
        "        print(\"Target: 2.5k diverse images\")\n",
        "\n",
        "        try:\n",
        "            import fiftyone as fo\n",
        "            import fiftyone.zoo as foz\n",
        "\n",
        "            dataset = foz.load_zoo_dataset(\n",
        "                \"open-images-v7\",\n",
        "                split=\"validation\",\n",
        "                max_samples=2500,\n",
        "                shuffle=True,\n",
        "                seed=42,\n",
        "            )\n",
        "\n",
        "            for sample in dataset:\n",
        "                src = Path(sample.filepath)\n",
        "                dst = oi_dir / src.name\n",
        "                if src.exists() and not dst.exists():\n",
        "                    shutil.copy2(src, dst)\n",
        "\n",
        "            count = len(list(oi_dir.glob(\"*\")))\n",
        "            print(f\"OpenImages: {count:,} images\")\n",
        "\n",
        "            fo.delete_dataset(dataset.name)\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"FiftyOne not installed. Installing...\")\n",
        "            subprocess.run([\"pip\", \"install\", \"fiftyone\"], check=True)\n",
        "            print(\"Re-run this cell after installation\")\n",
        "\n",
        "    # Optional: Hugging Face token (for gated datasets)\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        from huggingface_hub import login\n",
        "        hf_token = userdata.get(\"HF_TOKEN\")\n",
        "        if hf_token:\n",
        "            login(hf_token)\n",
        "            print(\"HF token loaded\")\n",
        "        else:\n",
        "            print(\"HF_TOKEN not found in Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(\"HF login skipped:\", e)\n",
        "\n",
        "    # 3) MobileViews via HuggingFace\n",
        "    if need_download[\"mobileviews\"]:\n",
        "        mv_dir = DATA_ROOT / \"mobileviews\"\n",
        "        mv_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"[3/3] MobileViews via HuggingFace\")\n",
        "        print(\"Target: 2k mobile UI screenshots\")\n",
        "\n",
        "        try:\n",
        "            from datasets import load_dataset\n",
        "\n",
        "            ds = load_dataset(\n",
        "                \"mllmTeam/MobileViews\",\n",
        "                split=\"train\",\n",
        "                streaming=True,\n",
        "            )\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.Lambda(lambda x: x.convert(\"RGB\")),\n",
        "            ])\n",
        "\n",
        "            count = 0\n",
        "            target = 2000\n",
        "            for sample in ds:\n",
        "                if count >= target:\n",
        "                    break\n",
        "                try:\n",
        "                    img = sample.get(\"image\")\n",
        "                    if img is not None:\n",
        "                        img = transform(img)\n",
        "                        img.save(mv_dir / f\"mv_{count:05d}.jpg\", \"JPEG\", quality=85)\n",
        "                        count += 1\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "            print(f\"MobileViews: {count:,} images\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"datasets not installed. Installing...\")\n",
        "            subprocess.run([\"pip\", \"install\", \"datasets\"], check=True)\n",
        "            print(\"Re-run this cell after installation\")\n",
        "\n",
        "    # Save cache to Drive\n",
        "    print(\"\")\n",
        "    print(\"=\" * 65)\n",
        "    print(\"SAVING CACHE TO DRIVE\")\n",
        "    print(\"=\" * 65)\n",
        "\n",
        "    DRIVE_ARCHIVE.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"Creating archive:\", DRIVE_ARCHIVE)\n",
        "    shutil.make_archive(\n",
        "        str(DRIVE_ARCHIVE.with_suffix(\"\")),\n",
        "        \"zip\",\n",
        "        DATA_ROOT,\n",
        "    )\n",
        "\n",
        "    archive_size = DRIVE_ARCHIVE.stat().st_size / (1024 ** 3)\n",
        "    print(f\"Cache saved ({archive_size:.2f} GB)\")\n",
        "\n",
        "# Final validation\n",
        "print(\"\")\n",
        "print(\"=\" * 65)\n",
        "print(\"FINAL DATASET VALIDATION\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "all_valid, validation = validate_dataset(DATA_ROOT, EXPECTED_COUNTS)\n",
        "total_images = sum(info[\"count\"] for info in validation.values())\n",
        "\n",
        "for name, info in validation.items():\n",
        "    status = \"OK\" if info[\"valid\"] else \"BAD\"\n",
        "    print(f\"{status} {name}: {info['count']:,} / {info['expected']:,} images\")\n",
        "\n",
        "print(f\"Total: {total_images:,} images\")\n",
        "\n",
        "if all_valid:\n",
        "    print(\"All datasets ready for training.\")\n",
        "else:\n",
        "    print(\"Some datasets are incomplete. Re-run this cell to download more.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "\n",
        "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
        "\n",
        "if \"DATA_ROOT\" not in globals():\n",
        "    DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
        "    LOCAL_DATA = Path(\"/content/data\")\n",
        "    DRIVE_DATA = DRIVE_ROOT / \"data\"\n",
        "    def has_images(root: Path) -> bool:\n",
        "        if not root.exists():\n",
        "            return False\n",
        "        return any(p.is_file() and p.suffix.lower() in exts for p in root.rglob(\"*\"))\n",
        "    DATA_ROOT = LOCAL_DATA if has_images(LOCAL_DATA) else DRIVE_DATA\n",
        "\n",
        "print(f\"Validating DATA_ROOT: {DATA_ROOT}\")\n",
        "\n",
        "paths = [p for p in DATA_ROOT.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
        "print(f\"Total images found: {len(paths)}\")\n",
        "\n",
        "ext_counts = Counter(p.suffix.lower() for p in paths)\n",
        "print(\"Extension counts:\", dict(ext_counts))\n",
        "\n",
        "sample = paths[:200]\n",
        "bad = []\n",
        "for p in sample:\n",
        "    try:\n",
        "        with Image.open(p) as img:\n",
        "            img.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        bad.append((p, str(e)))\n",
        "\n",
        "if bad:\n",
        "    print(f\"Corrupt/unreadable samples: {len(bad)} (showing up to 5)\")\n",
        "    for p, err in bad[:5]:\n",
        "        print(f\"  - {p}: {err}\")\n",
        "else:\n",
        "    print(\"Sample check: all images readable and convertible to RGB.\")\n",
        "\n",
        "if len(paths) == 0:\n",
        "    raise ValueError(\"No images found for training. Check extraction paths or zip contents.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Build Manifest (Optional)\n\nIf you already have a manifest at `/content/drive/MyDrive/dazzled/manifests/train.txt`, you can skip this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/dazzled\")\n",
        "LOCAL_DATA = Path(\"/content/data\")\n",
        "DRIVE_DATA = DRIVE_ROOT / \"data\"\n",
        "MANIFEST = DRIVE_ROOT / \"manifests/train.txt\"\n",
        "MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
        "LABEL_REGEX = re.compile(r\"^((?:ffhq|openimages|openimg|mobileviews?)_\\d+|\\d+)\")\n",
        "\n",
        "def has_images(root: Path) -> bool:\n",
        "    if not root.exists():\n",
        "        return False\n",
        "    return any(p.is_file() and p.suffix.lower() in exts for p in root.rglob(\"*\"))\n",
        "\n",
        "DATA_ROOT = LOCAL_DATA if has_images(LOCAL_DATA) else DRIVE_DATA\n",
        "print(f\"Using DATA_ROOT: {DATA_ROOT}\")\n",
        "\n",
        "lines = []\n",
        "for p in DATA_ROOT.rglob(\"*\"):\n",
        "    if not p.is_file() or p.suffix.lower() not in exts:\n",
        "        continue\n",
        "    match = LABEL_REGEX.search(p.name)\n",
        "    label = match.group(1) if match else p.stem\n",
        "    lines.append(f\"{p} {label}\")\n",
        "\n",
        "MANIFEST.write_text(\"\\n\".join(lines))\n",
        "print(f\"Wrote {len(lines)} lines to {MANIFEST} (per-image labels)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5. Sanity Checks (Labels + Domains)\n",
        "\n",
        "Run this once after the manifest is created to verify labels/domains before any training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "MANIFEST = Path(\"/content/drive/MyDrive/dazzled/manifests/train.txt\")\n",
        "LABEL_REGEX = re.compile(r\"^((?:ffhq|openimages|openimg|mobileview)_\\d+|\\d+)\")\n",
        "DOMAIN_REGEX = re.compile(r\"(?:^|/)(ffhq|openimages|openimg|mobileviews?)(?:/|_)\")\n",
        "\n",
        "if not MANIFEST.exists():\n",
        "    raise FileNotFoundError(f\"Manifest not found: {MANIFEST}\")\n",
        "\n",
        "base = MANIFEST.resolve().parent\n",
        "lines = [line.strip() for line in MANIFEST.read_text().splitlines() if line.strip() and not line.strip().startswith('#')]\n",
        "total = len(lines)\n",
        "\n",
        "labels = []\n",
        "domains = []\n",
        "missing = []\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.split()\n",
        "    path = Path(parts[0])\n",
        "    if not path.is_absolute():\n",
        "        path = (base / path).resolve()\n",
        "\n",
        "    label = parts[1] if len(parts) > 1 else None\n",
        "    if label is None:\n",
        "        match = LABEL_REGEX.search(path.name)\n",
        "        if match:\n",
        "            label = match.group(1)\n",
        "\n",
        "    domain = None\n",
        "    match = DOMAIN_REGEX.search(str(path))\n",
        "    if match:\n",
        "        domain = match.group(1)\n",
        "\n",
        "    labels.append(label)\n",
        "    domains.append(domain)\n",
        "    if not path.exists():\n",
        "        missing.append(str(path))\n",
        "\n",
        "label_known = [str(l) for l in labels if l is not None]\n",
        "domain_known = [str(d) for d in domains if d is not None]\n",
        "\n",
        "label_unique = len(set(label_known))\n",
        "label_unlabeled = total - len(label_known)\n",
        "label_pct = (label_unlabeled / total * 100.0) if total else 0.0\n",
        "\n",
        "domain_unique = len(set(domain_known))\n",
        "domain_unlabeled = total - len(domain_known)\n",
        "domain_pct = (domain_unlabeled / total * 100.0) if total else 0.0\n",
        "\n",
        "print(f\"Label stats: {label_unique} unique, {label_unlabeled}/{total} unlabeled ({label_pct:.1f}%).\")\n",
        "if label_known:\n",
        "    top = Counter(label_known).most_common(5)\n",
        "    top_str = \", \".join(f\"{k}:{v}\" for k, v in top)\n",
        "    print(f\"Top labels: {top_str}\")\n",
        "\n",
        "print(f\"Domain stats: {domain_unique} unique, {domain_unlabeled}/{total} unlabeled ({domain_pct:.1f}%).\")\n",
        "if domain_known:\n",
        "    top = Counter(domain_known).most_common(5)\n",
        "    top_str = \", \".join(f\"{k}:{v}\" for k, v in top)\n",
        "    print(f\"Top domains: {top_str}\")\n",
        "\n",
        "if missing:\n",
        "    print(f\"Missing files: {len(missing)} (showing up to 5)\")\n",
        "    for p in missing[:5]:\n",
        "        print(f\"  - {p}\")\n",
        "\n",
        "if label_unique < 2:\n",
        "    raise ValueError(\"CRITICAL: fewer than 2 unique labels found. Check regex/manifest.\")\n",
        "if domain_unique < 2:\n",
        "    print(\"WARNING: fewer than 2 unique domains found; VAE training will fail.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Counterfactual VAE (Save Weights)\n",
        "\n",
        "This produces the `--counterfactual-weights` file used by HashNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/DaZZLeD/ml-core\")\n",
        "\n",
        "!PYTHONPATH=/content/DaZZLeD/ml-core python training/train_counterfactual_vae.py \\\n",
        "  --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
        "  --epochs 10 \\\n",
        "  --batch-size 96 \\\n",
        "  --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/cf_vae \\\n",
        "  --domain-mode regex \\\n",
        "  --domain-regex \"(?:^|/)(ffhq|openimages|openimg|mobileviews?)(?:/|_)\" \\\n",
        "  --workers 4 \\\n",
        "  --prefetch-factor 2 \\\n",
        "  --pin-memory \\\n",
        "  --amp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train HashNet (ResNet + Hash Centers + CF/DHD/PGD)\n",
        "\n",
        "This uses the VAE weights from Step 3 and writes checkpoints to Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/DaZZLeD/ml-core\")\n",
        "\n",
        "!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \\\n",
        "PYTHONPATH=/content/DaZZLeD/ml-core \\\n",
        "python training/train_hashnet.py \\\n",
        "  --data-list /content/drive/MyDrive/dazzled/manifests/train.txt \\\n",
        "  --epochs 10 \\\n",
        "  --batch-size 64 \\\n",
        "  --center-mode random \\\n",
        "  --extra-negatives 1024 \\\n",
        "  --center-neg-k 0 \\\n",
        "  --counterfactual-mode vae \\\n",
        "  --counterfactual-weights /content/drive/MyDrive/dazzled/outputs/cf_vae/cf_vae_final.safetensors \\\n",
        "  --checkpoint-dir /content/drive/MyDrive/dazzled/outputs/hashnet \\\n",
        "  --domain-mode regex \\\n",
        "  --domain-regex '(?:^|/)(ffhq|openimages|openimg|mobileviews?)(?:/|_)' \\\n",
        "  --workers 2 \\\n",
        "  --prefetch-factor 1 \\\n",
        "  --pin-memory \\\n",
        "  --amp \\\n",
        "  --channels-last \\\n",
        "  --allow-tf32 \\\n",
        "  --cudnn-benchmark \\\n",
        "  --lr 3e-4 \\\n",
        "  --warmup-steps 500 \\\n",
        "  --center-weight 10 \\\n",
        "  --distinct-weight 0.5 \\\n",
        "  --quant-weight 0.1 \\\n",
        "  --cf-weight 0.1 \\\n",
        "  --dhd-weight 0.1 \\\n",
        "  --adv-weight 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. List Checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/hashnet\")\n",
        "ckpts = sorted(CKPT_DIR.glob(\"*.safetensors\"))\n",
        "print(f\"Found {len(ckpts)} checkpoints\")\n",
        "for ckpt in ckpts:\n",
        "    print(ckpt.name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. TTC Inference (Production-Style)\n",
        "\n",
        "Run the standalone TTC inference script on a sample image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/DaZZLeD/ml-core\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/dazzled/outputs/hashnet\")\n",
        "IMAGE_PATH = \"/content/drive/MyDrive/dazzled/data/ffhq/224/00000.jpg\"  # TODO: set a real path\n",
        "\n",
        "ckpts = sorted(CKPT_DIR.glob(\"*.safetensors\"))\n",
        "if not ckpts:\n",
        "    raise FileNotFoundError(f\"No checkpoints in {CKPT_DIR}\")\n",
        "\n",
        "checkpoint = str(ckpts[-1])\n",
        "print(f\"Using checkpoint: {checkpoint}\")\n",
        "\n",
        "!python inference.py   --image \"{IMAGE_PATH}\"   --checkpoint \"{checkpoint}\"   --backbone resnet50   --hash-dim 128   --proj-dim 512   --ttc-views 8   --stability-threshold 0.9   --hamming-threshold 10\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}